#!/usr/bin/env python3
"""
LINEARæ•°æ®é›†ç‰©ç†çº¦æŸTimeGANé‡é‡‡æ ·
ä¸“é—¨ä¼˜åŒ–ç±»åˆ«0 (Beta_Persei) å’Œç±»åˆ«1 (Delta_Scuti) çš„åŒºåˆ†æ€§
è§£å†³è¯¯åˆ†ç±»é—®é¢˜ï¼šç±»åˆ«1â†’3,4ç±»ï¼Œç±»åˆ«0â†’å…¶ä»–ç±»çš„æ··æ·†
"""

import os
import sys
import pickle
import numpy as np
import torch
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/root/autodl-tmp/lnsde-contiformer')

from utils.resampling import HybridResampler

def configure_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    import matplotlib.font_manager as fm
    
    # æ·»åŠ å­—ä½“åˆ°matplotlibç®¡ç†å™¨
    fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc')
    fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')
    
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    return True

def analyze_confusion_classes():
    """åˆ†æç±»åˆ«0å’Œ1ä¸å…¶ä»–ç±»åˆ«çš„ç‰¹å¾å·®å¼‚"""
    print("ğŸ” æ·±åº¦åˆ†æLINEARç±»åˆ«0å’Œ1çš„æ··æ·†é—®é¢˜...")
    
    data_path = '/root/autodl-fs/lnsde-contiformer/data/LINEAR_original.pkl'
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    class_samples = {}
    for item in data:
        label = item['label']
        if label not in class_samples:
            class_samples[label] = []
        class_samples[label].append(item)
    
    # è¯¦ç»†ç‰¹å¾åˆ†æ
    class_features = {}
    for label, samples in class_samples.items():
        class_name = samples[0]['class_name']
        print(f"\nğŸ“Š ç±»åˆ«{label} ({class_name}) - {len(samples)}æ ·æœ¬:")
        
        periods = []
        mag_ranges = []
        mag_means = []
        error_means = []
        sequence_lengths = []
        mag_stds = []
        
        for sample in samples:
            mask = sample['mask'].astype(bool)
            if np.sum(mask) < 5:  # è‡³å°‘éœ€è¦5ä¸ªæœ‰æ•ˆç‚¹
                continue
                
            times = sample['time'][mask]
            mags = sample['mag'][mask]
            errors = sample['errmag'][mask]
            
            periods.append(sample['period'])
            mag_ranges.append(mags.max() - mags.min())
            mag_means.append(mags.mean())
            mag_stds.append(mags.std())
            error_means.append(errors.mean())
            sequence_lengths.append(len(times))
        
        # ç»Ÿè®¡ç‰¹å¾
        features = {
            'period_mean': np.mean(periods),
            'period_std': np.std(periods),
            'mag_range_mean': np.mean(mag_ranges),
            'mag_range_std': np.std(mag_ranges),
            'mag_mean': np.mean(mag_means),
            'mag_std_mean': np.mean(mag_stds),
            'error_mean': np.mean(error_means),
            'seq_len_mean': np.mean(sequence_lengths),
            'count': len(samples)
        }
        
        class_features[label] = features
        
        print(f"  å‘¨æœŸ: {features['period_mean']:.3f} Â± {features['period_std']:.3f}")
        print(f"  æ˜Ÿç­‰å˜åŒ–: {features['mag_range_mean']:.3f} Â± {features['mag_range_std']:.3f}")
        print(f"  æ˜Ÿç­‰å‡å€¼: {features['mag_mean']:.3f}")
        print(f"  æ˜Ÿç­‰æ•£åº¦: {features['mag_std_mean']:.3f}")
        print(f"  è¯¯å·®æ°´å¹³: {features['error_mean']:.4f}")
        print(f"  åºåˆ—é•¿åº¦: {features['seq_len_mean']:.1f}")
    
    # åˆ†ææ··æ·†çŸ©é˜µ - æ‰¾å‡ºç±»åˆ«0å’Œ1æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«
    print(f"\nğŸ¯ é‡ç‚¹ç±»åˆ«å¯¹æ¯”åˆ†æ:")
    target_classes = [0, 1]  # Beta_Persei, Delta_Scuti
    confusing_classes = [2, 3, 4]  # å®¹æ˜“æ··æ·†çš„ç±»åˆ«
    
    print(f"ç›®æ ‡ä¼˜åŒ–:")
    for cls in target_classes:
        features = class_features[cls]
        name = class_samples[cls][0]['class_name']
        print(f"  ç±»åˆ«{cls} ({name}): å‘¨æœŸ{features['period_mean']:.3f}, å˜åŒ–{features['mag_range_mean']:.3f}")
    
    print(f"å®¹æ˜“æ··æ·†çš„ç±»åˆ«:")
    for cls in confusing_classes:
        features = class_features[cls]
        name = class_samples[cls][0]['class_name']
        print(f"  ç±»åˆ«{cls} ({name}): å‘¨æœŸ{features['period_mean']:.3f}, å˜åŒ–{features['mag_range_mean']:.3f}")
    
    return class_features, class_samples

def apply_enhanced_physics_timegan_resampling():
    """åº”ç”¨å¢å¼ºçš„ç‰©ç†çº¦æŸTimeGANé‡é‡‡æ ·ï¼Œä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1"""
    print(f"\nğŸš€ å¼€å§‹LINEARç‰©ç†çº¦æŸTimeGANé‡é‡‡æ ·...")
    
    # åŠ è½½åŸå§‹æ•°æ®
    data_path = '/root/autodl-fs/lnsde-contiformer/data/LINEAR_original.pkl'
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"åŸå§‹æ•°æ®: {len(data)}æ ·æœ¬")
    
    # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ - ç»Ÿä¸€åºåˆ—é•¿åº¦ä»¥æ”¯æŒTimeGAN
    X, y, times_list, masks_list, periods_list = [], [], [], [], []
    max_length = 512  # å›ºå®šæœ€å¤§é•¿åº¦
    
    for item in data:
        # æå–ç‰¹å¾
        mask = item['mask'].astype(bool)
        times = item['time'][mask]
        mags = item['mag'][mask]
        errors = item['errmag'][mask]
        
        if len(times) < 10:  # è¿‡æ»¤å¤ªçŸ­çš„åºåˆ—
            continue
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        seq_len = min(len(times), max_length)
        
        # åˆ›å»ºå›ºå®šé•¿åº¦çš„ç‰¹å¾çŸ©é˜µ
        features = np.zeros((max_length, 3), dtype=np.float32)  # [time, mag, error]
        features[:seq_len, 0] = times[:seq_len]
        features[:seq_len, 1] = mags[:seq_len] 
        features[:seq_len, 2] = errors[:seq_len]
        
        # åˆ›å»ºå¯¹åº”çš„mask
        feature_mask = np.zeros(max_length, dtype=bool)
        feature_mask[:seq_len] = True
        
        X.append(features)
        y.append(item['label'])
        times_list.append(times[:seq_len])
        masks_list.append(feature_mask)
        periods_list.append(item['period'])
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥æ”¯æŒTimeGAN
    X = np.array(X, dtype=np.float32)  # å›ºå®šå½¢çŠ¶ (N, 512, 3)
    y = np.array(y, dtype=np.int64)
    print(f"æœ‰æ•ˆæ ·æœ¬: {len(X)}ä¸ª")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {Counter(y.tolist())}")
    
    # è®¾è®¡é’ˆå¯¹ç±»åˆ«0å’Œ1çš„é‡é‡‡æ ·ç­–ç•¥
    # ç±»åˆ«0 (Beta_Persei): 291 â†’ 500 (+209)
    # ç±»åˆ«1 (Delta_Scuti): 70 â†’ 400 (+330)  é‡ç‚¹å¢å¼º
    
    target_strategy = {
        0: 500,   # Beta_Persei é€‚åº¦å¢å¼º
        1: 400,   # Delta_Scuti å¤§å¹…å¢å¼º (5.7å€)
        2: 2234,  # RR_Lyrae_FM ä¿æŒä¸å˜
        3: 749,   # RR_Lyrae_FO ä¿æŒä¸å˜  
        4: 1860   # W_Ursae_Maj ä¿æŒä¸å˜
    }
    
    print(f"\nğŸ¯ é‡é‡‡æ ·ç›®æ ‡:")
    current_counts = Counter(y)
    for cls, target_count in target_strategy.items():
        current = current_counts.get(cls, 0)
        increase = target_count - current
        print(f"  ç±»åˆ«{cls}: {current} â†’ {target_count} (+{increase})")
    
    # é…ç½®å¢å¼ºçš„HybridResampler
    resampler = HybridResampler(
        synthesis_mode='physics_timegan',
        # é’ˆå¯¹LINEARç±»åˆ«0å’Œ1ä¼˜åŒ–çš„ç‰©ç†çº¦æŸæƒé‡
        physics_weight=0.35,  # å¢å¼ºç‰©ç†çº¦æŸ
        noise_level=0.03,  # é™ä½å™ªå£°æ°´å¹³ä¿æŒæ•°æ®è´¨é‡
        smote_k_neighbors=8,  # å¢åŠ é‚»å±…æ•°æé«˜åˆæˆè´¨é‡
        apply_enn=False,  # å…³é—­ENNæ¸…ç†ï¼Œä¿æŒTimeGANç”Ÿæˆçš„æ ·æœ¬
        sampling_strategy=target_strategy,
        random_state=42
    )
    
    print(f"\nğŸ“ˆ å¼€å§‹é‡é‡‡æ ·è®­ç»ƒ...")
    print(f"ç‰©ç†çº¦æŸé…ç½®:")
    print(f"  ç‰©ç†æƒé‡: {resampler.physics_weight}")
    print(f"  ä½¿ç”¨ç‰©ç†çº¦æŸTimeGANæ¨¡å¼ä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1")
    
    # æ‰§è¡Œé‡é‡‡æ · - physics_timeganæ¨¡å¼è¿”å›4ä¸ªå€¼
    X_resampled, y_resampled, times_resampled, masks_resampled = resampler.fit_resample(X, y)
    
    print(f"\nâœ… é‡é‡‡æ ·å®Œæˆ!")
    print(f"é‡é‡‡æ ·åæ ·æœ¬æ•°: {len(X_resampled)}")
    print(f"é‡é‡‡æ ·åç±»åˆ«åˆ†å¸ƒ: {Counter(y_resampled)}")
    
    # è®¡ç®—å¢å¼ºæ•ˆæœ
    original_counts = Counter(y)
    resampled_counts = Counter(y_resampled)
    
    print(f"\nğŸ“Š å¢å¼ºæ•ˆæœç»Ÿè®¡:")
    total_original = len(y)
    total_resampled = len(y_resampled)
    
    for cls in sorted(set(y)):
        orig_count = original_counts[cls]
        resamp_count = resampled_counts[cls]
        increase = resamp_count - orig_count
        increase_rate = increase / orig_count if orig_count > 0 else 0
        
        print(f"  ç±»åˆ«{cls}: {orig_count} â†’ {resamp_count} (+{increase}, +{increase_rate:.1%})")
    
    print(f"æ€»æ ·æœ¬: {total_original} â†’ {total_resampled} (+{total_resampled - total_original})")
    
    # è®¡ç®—ç±»åˆ«å¹³è¡¡æ”¹å–„
    def gini_coefficient(counts):
        """è®¡ç®—åŸºå°¼ç³»æ•°è¡¡é‡ä¸å¹³è¡¡ç¨‹åº¦"""
        counts = np.array(list(counts.values()))
        n = len(counts)
        mean_count = np.mean(counts)
        return np.sum(np.abs(counts - mean_count)) / (2 * n * mean_count)
    
    original_imbalance = gini_coefficient(original_counts)
    resampled_imbalance = gini_coefficient(resampled_counts)
    
    print(f"\nç±»åˆ«å¹³è¡¡æ”¹å–„:")
    print(f"  åŸå§‹ä¸å¹³è¡¡åº¦: {original_imbalance:.3f}")
    print(f"  é‡é‡‡æ ·åä¸å¹³è¡¡åº¦: {resampled_imbalance:.3f}")
    print(f"  æ”¹å–„æ¯”ä¾‹: {(original_imbalance - resampled_imbalance) / original_imbalance:.1%}")
    
    return X_resampled, y_resampled, times_list, masks_list, periods_list

def convert_to_standard_format(X_resampled, y_resampled, original_data):
    """å°†é‡é‡‡æ ·æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†pklæ ¼å¼"""
    print(f"\nğŸ”„ è½¬æ¢ä¸ºæ ‡å‡†æ•°æ®æ ¼å¼...")
    
    # åˆ›å»ºç±»åˆ«åç§°æ˜ å°„
    class_name_mapping = {}
    for item in original_data:
        class_name_mapping[item['label']] = item['class_name']
    
    print(f"ç±»åˆ«æ˜ å°„: {class_name_mapping}")
    
    # è½¬æ¢é‡é‡‡æ ·æ•°æ®
    converted_data = []
    
    for i, (features, label) in enumerate(zip(X_resampled, y_resampled)):
        # features: [seq_len, 3] - [time, mag, error]
        times = features[:, 0].astype(np.float32)
        mags = features[:, 1].astype(np.float32) 
        errors = features[:, 2].astype(np.float32)
        
        seq_len = len(times)
        
        # åˆ›å»ºmask - æ‰€æœ‰ç‚¹éƒ½æ˜¯æœ‰æ•ˆçš„
        mask = np.ones(seq_len, dtype=bool)
        
        # è®¡ç®—å‘¨æœŸ - ä½¿ç”¨æ—¶é—´åºåˆ—çš„ç®€å•ä¼°è®¡
        time_span = times.max() - times.min()
        estimated_period = time_span / max(1, seq_len // 10)  # ç²—ç•¥ä¼°è®¡
        
        # æ„å»ºæ ‡å‡†æ ¼å¼çš„æ•°æ®é¡¹
        data_item = {
            'time': times,
            'mag': mags,
            'errmag': errors,
            'mask': mask,
            'period': np.float32(estimated_period),
            'label': int(label),
            'class_name': class_name_mapping.get(label, f'Class_{label}'),
            'file_id': f'timegan_generated_{i}',
            'original_length': seq_len,
            'valid_points': seq_len,
            'coverage': 1.0
        }
        
        converted_data.append(data_item)
    
    print(f"âœ… è½¬æ¢å®Œæˆ: {len(converted_data)}æ ·æœ¬")
    return converted_data

def save_linear_timegan_data(converted_data):
    """ä¿å­˜LINEAR TimeGANé‡é‡‡æ ·æ•°æ®"""
    print(f"\nğŸ’¾ ä¿å­˜LINEAR TimeGANé‡é‡‡æ ·æ•°æ®...")
    
    output_path = '/root/autodl-fs/lnsde-contiformer/data/LINEAR_resample_timegan.pkl'
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'wb') as f:
        pickle.dump(converted_data, f)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    
    # éªŒè¯ä¿å­˜çš„æ•°æ®
    with open(output_path, 'rb') as f:
        verified_data = pickle.load(f)
    
    print(f"éªŒè¯: åŠ è½½äº†{len(verified_data)}æ ·æœ¬")
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    labels = [item['label'] for item in verified_data]
    class_counts = Counter(labels)
    print(f"æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ: {dict(class_counts)}")
    
    return output_path

def create_comparison_visualization(original_data, converted_data):
    """åˆ›å»ºåŸå§‹æ•°æ®ä¸TimeGANæ•°æ®çš„å¯¹æ¯”å¯è§†åŒ–"""
    print(f"\nğŸ“Š åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–...")
    
    configure_chinese_font()
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('LINEARæ•°æ®é›†: åŸå§‹ vs TimeGANå¢å¼ºæ•°æ®å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    # æå–æ•°æ®
    orig_labels = [item['label'] for item in original_data]
    timegan_labels = [item['label'] for item in converted_data]
    
    orig_counts = Counter(orig_labels)
    timegan_counts = Counter(timegan_labels)
    
    # 1. ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”
    classes = sorted(set(orig_labels))
    class_names = {0: 'Beta_Persei', 1: 'Delta_Scuti', 2: 'RR_Lyrae_FM', 
                   3: 'RR_Lyrae_FO', 4: 'W_Ursae_Maj'}
    
    x_pos = np.arange(len(classes))
    orig_values = [orig_counts[cls] for cls in classes]
    timegan_values = [timegan_counts[cls] for cls in classes]
    
    axes[0, 0].bar(x_pos - 0.2, orig_values, 0.4, label='åŸå§‹æ•°æ®', alpha=0.7)
    axes[0, 0].bar(x_pos + 0.2, timegan_values, 0.4, label='TimeGANå¢å¼º', alpha=0.7)
    axes[0, 0].set_xlabel('ç±»åˆ«')
    axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')
    axes[0, 0].set_title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')
    axes[0, 0].set_xticks(x_pos)
    axes[0, 0].set_xticklabels([f'{cls}\n{class_names.get(cls, "Unknown")}' for cls in classes], rotation=45)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç±»åˆ«0å’Œ1çš„å¢å¼ºæ•ˆæœ
    target_classes = [0, 1]
    target_names = ['Beta_Persei\n(ç±»åˆ«0)', 'Delta_Scuti\n(ç±»åˆ«1)']
    target_orig = [orig_counts[cls] for cls in target_classes]
    target_timegan = [timegan_counts[cls] for cls in target_classes]
    target_increase = [timegan_counts[cls] - orig_counts[cls] for cls in target_classes]
    
    x_pos = np.arange(len(target_classes))
    axes[0, 1].bar(x_pos, target_orig, 0.6, label='åŸå§‹', alpha=0.7)
    axes[0, 1].bar(x_pos, target_increase, 0.6, bottom=target_orig, label='TimeGANæ–°å¢', alpha=0.7)
    axes[0, 1].set_xlabel('ç›®æ ‡ç±»åˆ«')
    axes[0, 1].set_ylabel('æ ·æœ¬æ•°é‡')
    axes[0, 1].set_title('é‡ç‚¹ç±»åˆ«å¢å¼ºæ•ˆæœ')
    axes[0, 1].set_xticks(x_pos)
    axes[0, 1].set_xticklabels(target_names)
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (orig, total) in enumerate(zip(target_orig, target_timegan)):
        axes[0, 1].text(i, total + 10, f'{total}', ha='center', fontweight='bold')
        increase_rate = (total - orig) / orig
        axes[0, 1].text(i, orig/2, f'+{increase_rate:.0%}', ha='center', color='white', fontweight='bold')
    
    # 3. ä¸å¹³è¡¡åº¦æ”¹å–„
    def gini_coefficient(counts):
        counts = np.array(list(counts.values()))
        n = len(counts)
        mean_count = np.mean(counts)
        return np.sum(np.abs(counts - mean_count)) / (2 * n * mean_count)
    
    orig_gini = gini_coefficient(orig_counts)
    timegan_gini = gini_coefficient(timegan_counts)
    
    metrics = ['ç±»åˆ«ä¸å¹³è¡¡åº¦\n(Giniç³»æ•°)']
    orig_metrics = [orig_gini]
    timegan_metrics = [timegan_gini]
    
    x_pos = np.arange(len(metrics))
    axes[0, 2].bar(x_pos - 0.2, orig_metrics, 0.4, label='åŸå§‹æ•°æ®', alpha=0.7)
    axes[0, 2].bar(x_pos + 0.2, timegan_metrics, 0.4, label='TimeGANå¢å¼º', alpha=0.7)
    axes[0, 2].set_xlabel('è¯„ä¼°æŒ‡æ ‡')
    axes[0, 2].set_ylabel('æ•°å€¼')
    axes[0, 2].set_title('æ•°æ®å¹³è¡¡æ€§æ”¹å–„')
    axes[0, 2].set_xticks(x_pos)
    axes[0, 2].set_xticklabels(metrics)
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    improvement = (orig_gini - timegan_gini) / orig_gini
    axes[0, 2].text(0, max(orig_gini, timegan_gini) * 1.1, f'æ”¹å–„: {improvement:.1%}', 
                    ha='center', fontweight='bold', color='green')
    
    # 4-6. ç±»åˆ«0å’Œ1çš„å…‰å˜æ›²çº¿æ ·æœ¬å¯¹æ¯”
    def plot_sample_curves(ax, data, class_label, title_suffix):
        samples = [item for item in data if item['label'] == class_label]
        if not samples:
            ax.text(0.5, 0.5, 'æ— æ ·æœ¬', ha='center', va='center', transform=ax.transAxes)
            return
            
        # éšæœºé€‰æ‹©å‡ ä¸ªæ ·æœ¬ç»˜åˆ¶
        np.random.seed(42)
        sample_indices = np.random.choice(len(samples), min(5, len(samples)), replace=False)
        
        for i, idx in enumerate(sample_indices):
            sample = samples[idx]
            mask = sample['mask'].astype(bool) if 'mask' in sample else np.ones(len(sample['time']), dtype=bool)
            times = sample['time'][mask]
            mags = sample['mag'][mask]
            
            # å½’ä¸€åŒ–æ—¶é—´åˆ°0-1
            if len(times) > 1:
                times_norm = (times - times.min()) / (times.max() - times.min())
                ax.plot(times_norm, mags, alpha=0.7, linewidth=1)
        
        ax.set_xlabel('å½’ä¸€åŒ–æ—¶é—´')
        ax.set_ylabel('æ˜Ÿç­‰')
        ax.set_title(f'{title_suffix}')
        ax.grid(True, alpha=0.3)
        ax.invert_yaxis()  # æ˜Ÿç­‰è¶Šå°è¶Šäº®
    
    # ç±»åˆ«0å¯¹æ¯”
    plot_sample_curves(axes[1, 0], original_data, 0, 'Beta_Persei - åŸå§‹æ•°æ®')
    plot_sample_curves(axes[1, 1], converted_data, 0, 'Beta_Persei - TimeGANç”Ÿæˆ')
    
    # ç±»åˆ«1å¯¹æ¯”  
    plot_sample_curves(axes[1, 2], [item for item in converted_data if item['label'] == 1], 1, 'Delta_Scuti - TimeGANç”Ÿæˆ')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    pic_dir = '/root/autodl-tmp/lnsde-contiformer/results/pics/LINEAR'
    os.makedirs(pic_dir, exist_ok=True)
    pic_path = os.path.join(pic_dir, 'timegan_comparison.png')
    plt.savefig(pic_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ å¯¹æ¯”å¯è§†åŒ–å·²ä¿å­˜è‡³: {pic_path}")
    return pic_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LINEARç‰©ç†çº¦æŸTimeGANé‡é‡‡æ · - ä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1")
    print("=" * 60)
    
    # 1. åˆ†ææ··æ·†é—®é¢˜
    class_features, class_samples = analyze_confusion_classes()
    
    # 2. åº”ç”¨ç‰©ç†çº¦æŸTimeGAN
    X_resampled, y_resampled, times_list, masks_list, periods_list = apply_enhanced_physics_timegan_resampling()
    
    # 3. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    original_data = []
    with open('/root/autodl-fs/lnsde-contiformer/data/LINEAR_original.pkl', 'rb') as f:
        original_data = pickle.load(f)
    
    converted_data = convert_to_standard_format(X_resampled, y_resampled, original_data)
    
    # 4. ä¿å­˜æ•°æ®
    output_path = save_linear_timegan_data(converted_data)
    
    # 5. åˆ›å»ºå¯è§†åŒ–
    pic_path = create_comparison_visualization(original_data, converted_data)
    
    print(f"\nğŸ‰ LINEAR TimeGANé‡é‡‡æ ·å®Œæˆ!")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {output_path}")
    print(f"ğŸ“Š å¯è§†åŒ–: {pic_path}")
    print(f"\nğŸ¯ ä¸»è¦æ”¹è¿›:")
    print(f"  â€¢ ç±»åˆ«0 (Beta_Persei): 291 â†’ 500æ ·æœ¬ (+71%)")
    print(f"  â€¢ ç±»åˆ«1 (Delta_Scuti): 70 â†’ 400æ ·æœ¬ (+471%)")
    print(f"  â€¢ å¢å¼ºç‰©ç†çº¦æŸä»¥å‡å°‘ä¸ç±»åˆ«3,4çš„æ··æ·†")
    print(f"  â€¢ åŠ å¼ºå»å™ªèƒ½åŠ›æé«˜åˆ†ç±»å‡†ç¡®æ€§")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"python main.py --dataset 2 --use_resampling --resampled_data_path {output_path}")

if __name__ == "__main__":
    main()