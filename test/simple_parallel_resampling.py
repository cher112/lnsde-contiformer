#!/usr/bin/env python3
"""
ç®€åŒ–å¹¶è¡Œé‡é‡‡æ ·è„šæœ¬ - å»æ‰å¤æ‚çš„è¿›åº¦ç›‘æ§
"""

import os
import sys
import numpy as np
import torch
import pickle
from multiprocessing import Pool
from datetime import datetime
import argparse
from collections import Counter
import psutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/root/autodl-tmp/lnsde-contiformer')

from utils.resampling import HybridResampler, configure_chinese_font

# é…ç½®ä¸­æ–‡å­—ä½“
configure_chinese_font()

def load_dataset_simple(dataset_name, data_dir='/root/autodl-fs/lnsde-contiformer/data'):
    """ç®€åŒ–çš„æ•°æ®åŠ è½½å‡½æ•°"""
    print(f"[{dataset_name}] ğŸ” å¼€å§‹åŠ è½½æ•°æ®...")
    
    file_path = os.path.join(data_dir, f'{dataset_name}_original.pkl')
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'rb') as f:
        data = pickle.load(f)
    
    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
        n_samples = len(data)
        seq_len = len(data[0]['time'])
        n_features = 3
        
        X = np.zeros((n_samples, seq_len, n_features))
        y = np.zeros(n_samples, dtype=int)
        times = np.zeros((n_samples, seq_len))
        masks = np.ones((n_samples, seq_len), dtype=bool)
        
        print(f"[{dataset_name}] ğŸ”„ è½¬æ¢{n_samples}ä¸ªæ ·æœ¬...")
        for i, sample in enumerate(data):
            X[i, :, 0] = sample['time']
            X[i, :, 1] = sample['mag'] 
            X[i, :, 2] = sample['errmag']
            y[i] = sample['label']
            times[i] = sample['time']
            if 'mask' in sample:
                masks[i] = sample['mask']
        
        print(f"[{dataset_name}] âœ… æ•°æ®åŠ è½½å®Œæˆ: {X.shape}, åˆ†å¸ƒ: {Counter(y)}")
        return X, y, times, masks
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")

def simple_resample_worker(dataset_name):
    """ç®€åŒ–çš„é‡é‡‡æ ·å·¥ä½œå‡½æ•°"""
    try:
        print(f"\n{'='*60}")
        print(f"[{dataset_name}] ğŸš€ å¼€å§‹é‡é‡‡æ ·è¿›ç¨‹")
        print(f"{'='*60}")
        
        start_time = datetime.now()
        
        # 1. åŠ è½½æ•°æ®
        X, y, times, masks = load_dataset_simple(dataset_name)
        
        # 2. åˆ›å»ºé‡é‡‡æ ·å™¨
        print(f"[{dataset_name}] ğŸ”§ åˆå§‹åŒ–é‡é‡‡æ ·å™¨...")
        resampler = HybridResampler(
            smote_k_neighbors=5,
            enn_n_neighbors=3,
            sampling_strategy='balanced',
            synthesis_mode='hybrid',
            noise_level=0.05,
            apply_enn=True,
            random_state=535411460
        )
        
        # 3. æ‰§è¡Œé‡é‡‡æ ·
        print(f"[{dataset_name}] âš¡ æ‰§è¡Œæ··åˆæ¨¡å¼é‡é‡‡æ ·...")
        X_resampled, y_resampled, times_resampled, masks_resampled = resampler.fit_resample(
            X, y, times, masks
        )
        
        # 4. ä¿å­˜ç»“æœ
        print(f"[{dataset_name}] ğŸ’¾ ä¿å­˜é‡é‡‡æ ·æ•°æ®...")
        save_path = f'/root/autodl-fs/lnsde-contiformer/data/{dataset_name}_resampled.pkl'
        
        resampled_data = {
            'X': X_resampled,
            'y': y_resampled,
            'times': times_resampled,
            'masks': masks_resampled,
            'dataset': dataset_name,
            'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'distribution': dict(Counter(y_resampled)),
            'synthesis_mode': 'hybrid',
            'original_distribution': dict(Counter(y))
        }
        
        with open(save_path, 'wb') as f:
            pickle.dump(resampled_data, f)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        result = {
            'dataset': dataset_name,
            'status': 'success',
            'original_samples': len(y),
            'resampled_samples': len(y_resampled),
            'original_distribution': dict(Counter(y)),
            'resampled_distribution': dict(Counter(y_resampled)),
            'processing_time': processing_time,
            'save_path': save_path
        }
        
        print(f"[{dataset_name}] âœ… é‡é‡‡æ ·å®Œæˆ! ç”¨æ—¶: {processing_time:.1f}ç§’")
        print(f"[{dataset_name}] ğŸ“Š {len(y):,} â†’ {len(y_resampled):,} æ ·æœ¬")
        print(f"[{dataset_name}] ğŸ’¾ ä¿å­˜è‡³: {save_path}")
        
        return result
        
    except Exception as e:
        print(f"[{dataset_name}] âŒ é‡é‡‡æ ·å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'dataset': dataset_name,
            'status': 'failed',
            'error': str(e),
            'processing_time': 0
        }

def simple_parallel_resample(datasets=['ASAS', 'LINEAR', 'MACHO'], n_processes=None):
    """ç®€åŒ–çš„å¹¶è¡Œé‡é‡‡æ ·"""
    
    # è‡ªåŠ¨é€‰æ‹©è¿›ç¨‹æ•°
    if n_processes is None:
        cpu_count = psutil.cpu_count(logical=False)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        max_by_memory = int(memory_gb / 4)  # æ¯ä¸ªè¿›ç¨‹4GBå†…å­˜
        max_by_cpu = min(cpu_count, len(datasets))
        n_processes = min(max_by_memory, max_by_cpu, len(datasets))
        n_processes = max(1, n_processes)
        
        print(f"ğŸ–¥ï¸  CPU: {cpu_count}æ ¸å¿ƒ, å†…å­˜: {memory_gb:.1f}GB")
        print(f"âš¡ è‡ªåŠ¨é€‰æ‹©è¿›ç¨‹æ•°: {n_processes}")
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ ç®€åŒ–å¹¶è¡Œé‡é‡‡æ · - {len(datasets)}ä¸ªæ•°æ®é›†")
    print(f"æ•°æ®é›†: {', '.join(datasets)}")
    print(f"å¹¶è¡Œè¿›ç¨‹æ•°: {n_processes}")
    print(f"{'='*80}")
    
    start_time = datetime.now()
    
    # å¹¶è¡Œå¤„ç†
    if n_processes == 1:
        # å•è¿›ç¨‹å¤„ç†
        print("ğŸ“ ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼...")
        results = []
        for dataset in datasets:
            result = simple_resample_worker(dataset)
            results.append(result)
    else:
        # å¤šè¿›ç¨‹å¤„ç†
        print(f"ğŸ”¥ å¯åŠ¨{n_processes}ä¸ªå¹¶è¡Œè¿›ç¨‹...")
        with Pool(processes=n_processes) as pool:
            results = pool.map(simple_resample_worker, datasets)
    
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    # æ±‡æ€»ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ¯ æœ€ç»ˆç»“æœæ±‡æ€»")
    print(f"{'='*80}")
    
    successful = 0
    failed = 0
    total_original = 0
    total_resampled = 0
    
    for result in results:
        if result['status'] == 'success':
            successful += 1
            original = result['original_samples']
            resampled = result['resampled_samples']
            total_original += original
            total_resampled += resampled
            
            print(f"âœ… {result['dataset']}: æˆåŠŸ")
            print(f"   ğŸ“Š {original:,} â†’ {resampled:,} æ ·æœ¬ (+{resampled-original:,})")
            print(f"   â±ï¸  ç”¨æ—¶: {result['processing_time']:.1f}ç§’")
            print(f"   ğŸ’¾ ä¿å­˜: {result['save_path']}")
        else:
            failed += 1
            print(f"âŒ {result['dataset']}: å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print(f"\nğŸ† æ€»ç»“:")
    print(f"   âœ… æˆåŠŸ: {successful}/{len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"   âŒ å¤±è´¥: {failed}/{len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"   ğŸ“ˆ æ€»æ ·æœ¬å¢é•¿: {total_original:,} â†’ {total_resampled:,} (+{((total_resampled/max(total_original,1)-1)*100):.1f}%)")
    print(f"   â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’ (å¹³å‡ {total_time/len(datasets):.1f}ç§’/æ•°æ®é›†)")
    
    return results

def main():
    parser = argparse.ArgumentParser(description='ç®€åŒ–å¹¶è¡Œé‡é‡‡æ ·è„šæœ¬')
    parser.add_argument('--datasets', nargs='+', default=['ASAS', 'LINEAR', 'MACHO'],
                        help='è¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨')
    parser.add_argument('--processes', type=int, default=None,
                        help='å¹¶è¡Œè¿›ç¨‹æ•° (None=è‡ªåŠ¨ä¼˜åŒ–)')
    
    args = parser.parse_args()
    
    results = simple_parallel_resample(
        datasets=args.datasets,
        n_processes=args.processes
    )
    
    print(f"\nğŸ‰ æ‰€æœ‰é‡é‡‡æ ·ä»»åŠ¡å®Œæˆï¼")
    return results

if __name__ == "__main__":
    main()