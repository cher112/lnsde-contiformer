#!/usr/bin/env python3
"""
è°ƒè¯•F1å’ŒRecallè®¡ç®—é—®é¢˜
"""

import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_recall_fscore_support

def test_metrics_calculation():
    """æµ‹è¯•æŒ‡æ ‡è®¡ç®—çš„æ­£ç¡®æ€§"""
    print("=" * 60)
    print("F1å’ŒRecallè®¡ç®—è°ƒè¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_classes = 7  # MACHOæ•°æ®é›†æœ‰7ä¸ªç±»åˆ«
    
    # ç”Ÿæˆéšæœºæ ‡ç­¾å’Œé¢„æµ‹
    true_labels = np.random.randint(0, n_classes, n_samples)
    
    # åˆ›å»ºä¸åŒå‡†ç¡®ç‡çš„é¢„æµ‹ç»“æœè¿›è¡Œæµ‹è¯•
    accuracies_to_test = [0.5, 0.7, 0.8, 0.9]
    
    for target_acc in accuracies_to_test:
        print(f"\næµ‹è¯•ç›®æ ‡å‡†ç¡®ç‡: {target_acc:.1%}")
        print("-" * 40)
        
        # ç”ŸæˆæŒ‡å®šå‡†ç¡®ç‡çš„é¢„æµ‹
        predictions = true_labels.copy()
        n_wrong = int(n_samples * (1 - target_acc))
        wrong_indices = np.random.choice(n_samples, n_wrong, replace=False)
        
        for idx in wrong_indices:
            # éšæœºé€‰æ‹©é”™è¯¯çš„ç±»åˆ«
            wrong_class = np.random.randint(0, n_classes)
            while wrong_class == true_labels[idx]:
                wrong_class = np.random.randint(0, n_classes)
            predictions[idx] = wrong_class
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        acc = accuracy_score(true_labels, predictions)
        macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)
        weighted_f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)
        macro_recall = recall_score(true_labels, predictions, average='macro', zero_division=0)
        weighted_recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)
        
        print(f"å®é™…å‡†ç¡®ç‡:    {acc:.4f} ({acc*100:.2f}%)")
        print(f"å®å¹³å‡F1:      {macro_f1:.4f} ({macro_f1*100:.2f}%)")
        print(f"åŠ æƒF1:        {weighted_f1:.4f} ({weighted_f1*100:.2f}%)")
        print(f"å®å¹³å‡Recall:  {macro_recall:.4f} ({macro_recall*100:.2f}%)")
        print(f"åŠ æƒRecall:    {weighted_recall:.4f} ({weighted_recall*100:.2f}%)")
        
        # åˆ†æå·®å¼‚
        f1_diff = abs(weighted_f1 - acc)
        recall_diff = abs(weighted_recall - acc)
        
        print(f"åŠ æƒF1ä¸å‡†ç¡®ç‡å·®å¼‚:    {f1_diff:.4f}")
        print(f"åŠ æƒRecallä¸å‡†ç¡®ç‡å·®å¼‚: {recall_diff:.4f}")
        
        if f1_diff > 0.01 or recall_diff > 0.01:
            print("âš ï¸  å·®å¼‚è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨è®¡ç®—é—®é¢˜")
        else:
            print("âœ… æŒ‡æ ‡è®¡ç®—æ­£å¸¸")

def test_real_scenario():
    """æµ‹è¯•å®é™…åœºæ™¯ä¸­çš„ä¸å¹³è¡¡æ•°æ®"""
    print("\n" + "=" * 60)
    print("ä¸å¹³è¡¡æ•°æ®åœºæ™¯æµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡æ‹ŸMACHOæ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ
    # 0:'Be'(128), 1:'CEPH'(101), 2:'EB'(255), 3:'LPV'(365), 4:'MOA'(582), 5:'QSO'(59), 6:'RRL'(610)
    class_counts = [128, 101, 255, 365, 582, 59, 610]
    class_names = ['Be', 'CEPH', 'EB', 'LPV', 'MOA', 'QSO', 'RRL']
    
    # ç”Ÿæˆä¸å¹³è¡¡çš„æµ‹è¯•æ•°æ®
    true_labels = []
    for class_id, count in enumerate(class_counts):
        true_labels.extend([class_id] * count)
    true_labels = np.array(true_labels)
    
    # ç”Ÿæˆ80%å‡†ç¡®ç‡çš„é¢„æµ‹
    predictions = true_labels.copy()
    n_total = len(true_labels)
    n_wrong = int(n_total * 0.2)  # 20%é”™è¯¯
    wrong_indices = np.random.choice(n_total, n_wrong, replace=False)
    
    for idx in wrong_indices:
        wrong_class = np.random.randint(0, 7)
        while wrong_class == true_labels[idx]:
            wrong_class = np.random.randint(0, 7)
        predictions[idx] = wrong_class
    
    # è®¡ç®—æŒ‡æ ‡
    acc = accuracy_score(true_labels, predictions)
    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)
    weighted_f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)
    macro_recall = recall_score(true_labels, predictions, average='macro', zero_division=0)
    weighted_recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)
    
    print(f"æ•°æ®æ€»æ•°: {n_total}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(class_names, class_counts))}")
    print()
    
    print(f"å‡†ç¡®ç‡:        {acc:.4f} ({acc*100:.2f}%)")
    print(f"å®å¹³å‡F1:      {macro_f1:.4f} ({macro_f1*100:.2f}%)")
    print(f"åŠ æƒF1:        {weighted_f1:.4f} ({weighted_f1*100:.2f}%)")
    print(f"å®å¹³å‡Recall:  {macro_recall:.4f} ({macro_recall*100:.2f}%)")
    print(f"åŠ æƒRecall:    {weighted_recall:.4f} ({weighted_recall*100:.2f}%)")
    
    print("\nğŸ“Š è¯¦ç»†åˆ†æ:")
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, predictions, average=None, zero_division=0
    )
    
    for i, (name, p, r, f, s) in enumerate(zip(class_names, precision, recall, f1, support)):
        print(f"  {name:<6}: P={p:.3f}, R={r:.3f}, F1={f:.3f}, æ”¯æŒ={s}")

def debug_current_implementation():
    """è°ƒè¯•å½“å‰å®ç°çš„é—®é¢˜"""
    print("\n" + "=" * 60)
    print("å½“å‰å®ç°è°ƒè¯•")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿå½“å‰training_utils.pyä¸­çš„è®¡ç®—è¿‡ç¨‹
    np.random.seed(42)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    all_predictions = np.random.randint(0, 7, 500)
    all_labels = np.random.randint(0, 7, 500)
    
    # ç¡®ä¿æœ‰ä¸€å®šçš„å‡†ç¡®ç‡
    correct_mask = np.random.random(500) < 0.75  # 75%å‡†ç¡®ç‡
    all_predictions[correct_mask] = all_labels[correct_mask]
    
    # æŒ‰ç…§å½“å‰å®ç°è®¡ç®—
    predictions = np.array(all_predictions)
    labels = np.array(all_labels)
    
    # è®¡ç®—å‡†ç¡®ç‡ (æŒ‰ç…§å½“å‰main loopçš„æ–¹å¼)
    correct = (predictions == labels).sum()
    total = len(labels)
    accuracy = 100. * correct / total
    
    # è®¡ç®—F1å’ŒRecall
    macro_f1 = f1_score(labels, predictions, average='macro', zero_division=0)
    weighted_f1 = f1_score(labels, predictions, average='weighted', zero_division=0)
    macro_recall = recall_score(labels, predictions, average='macro', zero_division=0)
    weighted_recall = recall_score(labels, predictions, average='weighted', zero_division=0)
    
    print(f"æ¨¡æ‹Ÿå½“å‰å®ç°ç»“æœ:")
    print(f"å‡†ç¡®ç‡:        {accuracy:.2f}%")
    print(f"å®å¹³å‡F1:      {macro_f1*100:.2f}%")
    print(f"åŠ æƒF1:        {weighted_f1*100:.2f}%")
    print(f"å®å¹³å‡Recall:  {macro_recall*100:.2f}%")
    print(f"åŠ æƒRecall:    {weighted_recall*100:.2f}%")
    
    print(f"\nå·®å¼‚åˆ†æ:")
    print(f"åŠ æƒF1 vs å‡†ç¡®ç‡: {abs(weighted_f1*100 - accuracy):.2f}% å·®å¼‚")
    print(f"åŠ æƒRecall vs å‡†ç¡®ç‡: {abs(weighted_recall*100 - accuracy):.2f}% å·®å¼‚")
    
    if abs(weighted_f1*100 - accuracy) < 2 and abs(weighted_recall*100 - accuracy) < 2:
        print("âœ… è®¡ç®—æ­£å¸¸ï¼Œå·®å¼‚åœ¨åˆç†èŒƒå›´å†…")
    else:
        print("âŒ è®¡ç®—å¼‚å¸¸ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        
    return accuracy, weighted_f1*100, weighted_recall*100

if __name__ == "__main__":
    test_metrics_calculation()
    test_real_scenario()
    accuracy, wf1, wr = debug_current_implementation()
    
    print("\n" + "=" * 60)
    print("æ€»ç»“")
    print("=" * 60)
    print("ç†è®ºä¸Š:")
    print("â€¢ åŠ æƒF1åº”è¯¥éå¸¸æ¥è¿‘å‡†ç¡®ç‡ï¼ˆå·®å¼‚<2%ï¼‰")
    print("â€¢ åŠ æƒRecallåº”è¯¥ç­‰äºå‡†ç¡®ç‡ï¼ˆå¤šåˆ†ç±»æƒ…å†µä¸‹ï¼‰")
    print("â€¢ å®å¹³å‡æŒ‡æ ‡å¯èƒ½åä½ï¼ˆå—ç±»åˆ«ä¸å¹³è¡¡å½±å“ï¼‰")
    
    print("\nå¦‚æœå®é™…è®­ç»ƒä¸­çœ‹åˆ°:")
    print("â€¢ å‡†ç¡®ç‡80%ï¼Œä½†åŠ æƒF1åªæœ‰20% â†’ å¯èƒ½æ˜¯å•ä½é—®é¢˜ï¼ˆ0.2 vs 20%ï¼‰")
    print("â€¢ æˆ–è€…æ•°æ®æ”¶é›†æœ‰é—®é¢˜ï¼ˆpredictionså’Œlabelsä¸åŒ¹é…ï¼‰")
    print("â€¢ æˆ–è€…è®¡ç®—æ—¶æœºæœ‰é—®é¢˜ï¼ˆåœ¨é”™è¯¯çš„åœ°æ–¹è®¡ç®—ï¼‰")