#!/usr/bin/env python3
"""
å¤šæ ¸å¹¶è¡Œé‡é‡‡æ ·è„šæœ¬ - å¯¹ASAS, LINEAR, MACHOä¸‰ä¸ªæ•°æ®é›†è¿›è¡Œæ··åˆæ¨¡å¼é‡é‡‡æ ·
ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæé«˜æ•ˆç‡
"""

import os
import sys
import numpy as np
import torch
import pickle
from multiprocessing import Pool, Manager
from datetime import datetime
import argparse
from collections import Counter
from tqdm import tqdm
import time
import psutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/root/autodl-tmp/lnsde-contiformer')

from utils.resampling import HybridResampler, save_resampled_data, configure_chinese_font

# é…ç½®ä¸­æ–‡å­—ä½“
configure_chinese_font()

def load_dataset(dataset_name, data_dir='/root/autodl-fs/lnsde-contiformer/data'):
    """
    åŠ è½½æ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§° ('ASAS', 'LINEAR', 'MACHO')
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        X, y, times, masks æ•°æ®
    """
    # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶åæ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨originalç‰ˆæœ¬è¿›è¡Œé‡é‡‡æ ·
    dataset_files = {
        'ASAS': [
            f'{dataset_name}_original.pkl',
            f'{dataset_name}_fixed.pkl'
        ],
        'LINEAR': [
            f'{dataset_name}_original.pkl',
            f'{dataset_name}_fixed.pkl'
        ],
        'MACHO': [
            f'{dataset_name}_original.pkl',
            f'{dataset_name}_fixed.pkl'
        ]
    }
    
    if dataset_name not in dataset_files:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # ä¼˜å…ˆä½¿ç”¨fixedç‰ˆæœ¬ï¼ˆæ ·æœ¬æ•°ç¨å°‘ï¼Œå¤„ç†æ›´å¿«ï¼‰
    file_path = None
    for filename in dataset_files[dataset_name]:
        candidate_path = os.path.join(data_dir, filename)
        if os.path.exists(candidate_path):
            file_path = candidate_path
            break
    
    if file_path is None:
        available_files = os.listdir(data_dir)
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°{dataset_name}æ•°æ®é›†æ–‡ä»¶ã€‚å¯ç”¨æ–‡ä»¶: {available_files}")
    
    print(f"åŠ è½½{dataset_name}æ•°æ®é›†: {file_path}")
    
    with open(file_path, 'rb') as f:
        data = pickle.load(f)
    
    # å¤„ç†åˆ—è¡¨æ ¼å¼çš„æ•°æ®ï¼ˆæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯å­—å…¸ï¼‰
    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
        n_samples = len(data)
        
        # è·å–åºåˆ—é•¿åº¦ï¼ˆå‡è®¾æ‰€æœ‰æ ·æœ¬å…·æœ‰ç›¸åŒé•¿åº¦ï¼‰
        seq_len = len(data[0]['time'])
        n_features = 3  # time, mag, errmag
        
        # åˆå§‹åŒ–æ•°ç»„
        X = np.zeros((n_samples, seq_len, n_features))
        y = np.zeros(n_samples, dtype=int)
        times = np.zeros((n_samples, seq_len))
        masks = np.zeros((n_samples, seq_len), dtype=bool)
        
        # æå–æ•°æ®
        for i, sample in enumerate(data):
            X[i, :, 0] = sample['time']
            X[i, :, 1] = sample['mag'] 
            X[i, :, 2] = sample['errmag']
            y[i] = sample['label']
            times[i] = sample['time']
            masks[i] = sample['mask']
            
        print(f"ä»åˆ—è¡¨æ ¼å¼è½¬æ¢å®Œæˆ")
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
    
    print(f"{dataset_name} æ•°æ®å½¢çŠ¶: X={X.shape}, y={len(y)}")
    print(f"{dataset_name} ç±»åˆ«åˆ†å¸ƒ: {Counter(y)}")
    
    return X, y, times, masks


def resample_dataset_worker(args):
    """
    å•ä¸ªæ•°æ®é›†çš„é‡é‡‡æ ·å·¥ä½œå‡½æ•°
    
    Args:
        args: (dataset_name, config, shared_results, progress_dict)
    
    Returns:
        ç»“æœä¿¡æ¯
    """
    dataset_name, config, shared_results, progress_dict = args
    
    try:
        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹åŠ è½½æ•°æ®
        with progress_dict.lock:
            progress_dict.status[dataset_name] = f"ğŸ“‚ åŠ è½½{dataset_name}æ•°æ®é›†..."
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¤„ç† {dataset_name} æ•°æ®é›†é‡é‡‡æ ·")
        print(f"{'='*60}")
        
        # åŠ è½½æ•°æ®
        X, y, times, masks = load_dataset(dataset_name)
        
        # æ›´æ–°è¿›åº¦ï¼šæ•°æ®åŠ è½½å®Œæˆ
        with progress_dict.lock:
            progress_dict.status[dataset_name] = f"ğŸ”§ åˆå§‹åŒ–{dataset_name}é‡é‡‡æ ·å™¨..."
        
        # åˆ›å»ºé‡é‡‡æ ·å™¨
        resampler = HybridResampler(
            smote_k_neighbors=config['k_neighbors'],
            enn_n_neighbors=config['enn_neighbors'],
            sampling_strategy=config['sampling_strategy'],
            synthesis_mode=config['synthesis_mode'],
            noise_level=config['noise_level'],
            apply_enn=config['apply_enn'],
            random_state=config['random_state']
        )
        
        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹é‡é‡‡æ ·
        with progress_dict.lock:
            progress_dict.status[dataset_name] = f"âš¡ {dataset_name}æ··åˆæ¨¡å¼é‡é‡‡æ ·ä¸­..."
        
        # æ‰§è¡Œé‡é‡‡æ ·
        start_time = datetime.now()
        X_resampled, y_resampled, times_resampled, masks_resampled = resampler.fit_resample(
            X, y, times, masks
        )
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # ç»Ÿè®¡ç»“æœ
        original_counts = Counter(y)
        resampled_counts = Counter(y_resampled)
        
        # æ›´æ–°è¿›åº¦ï¼šä¿å­˜æ•°æ®
        with progress_dict.lock:
            progress_dict.status[dataset_name] = f"ğŸ’¾ ä¿å­˜{dataset_name}é‡é‡‡æ ·æ•°æ®..."
        
        # ä¿å­˜é‡é‡‡æ ·æ•°æ®
        save_dir = config['save_dir']
        os.makedirs(save_dir, exist_ok=True)
        
        # ç›´æ¥ä¿å­˜åˆ°dataç›®å½•ï¼Œä½¿ç”¨ç»Ÿä¸€å‘½å
        resampled_filename = f'{dataset_name}_resampled.pkl'
        resampled_path = os.path.join('/root/autodl-fs/lnsde-contiformer/data', resampled_filename)
        
        # ä¿å­˜æ•°æ®
        resampled_data = {
            'X': X_resampled,
            'y': y_resampled,
            'times': times_resampled,
            'masks': masks_resampled,
            'dataset': dataset_name,
            'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'distribution': dict(resampled_counts),
            'synthesis_mode': config['synthesis_mode'],
            'original_distribution': dict(original_counts)
        }
        
        with open(resampled_path, 'wb') as f:
            pickle.dump(resampled_data, f)
        
        print(f"é‡é‡‡æ ·æ•°æ®å·²ä¿å­˜è‡³: {resampled_path}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        if config['generate_plots']:
            # æ›´æ–°è¿›åº¦ï¼šç”Ÿæˆå¯è§†åŒ–
            with progress_dict.lock:
                progress_dict.status[dataset_name] = f"ğŸ“Š ç”Ÿæˆ{dataset_name}å¯è§†åŒ–å›¾è¡¨..."
            
            plots_dir = os.path.join(config['plots_dir'], dataset_name)
            os.makedirs(plots_dir, exist_ok=True)
            
            # ç±»åˆ«åˆ†å¸ƒå›¾
            distribution_path = os.path.join(plots_dir, f'{dataset_name}_resampling_distribution.png')
            resampler.visualize_distribution(save_path=distribution_path)
            
            # åˆæˆæ•ˆæœå¯¹æ¯”å›¾
            if len(np.unique(y)) <= 3 and len(y) >= 10:  # åªå¯¹å°ç±»åˆ«æ•°ä¸”æœ‰è¶³å¤Ÿæ ·æœ¬çš„æ•°æ®é›†ç”Ÿæˆ
                comparison_path = os.path.join(plots_dir, f'{dataset_name}_synthesis_comparison.png')
                try:
                    resampler.smote.visualize_synthesis_comparison(
                        X, y, n_examples=2, save_path=comparison_path
                    )
                except Exception as e:
                    print(f"ç”Ÿæˆ{dataset_name}åˆæˆå¯¹æ¯”å›¾å¤±è´¥: {e}")
        
        # æ›´æ–°è¿›åº¦ï¼šå®Œæˆ
        with progress_dict.lock:
            progress_dict.status[dataset_name] = f"âœ… {dataset_name}é‡é‡‡æ ·å®Œæˆï¼"
        
        result = {
            'dataset': dataset_name,
            'status': 'success',
            'original_samples': len(y),
            'resampled_samples': len(y_resampled),
            'original_distribution': dict(original_counts),
            'resampled_distribution': dict(resampled_counts),
            'processing_time': processing_time,
            'save_path': resampled_path,
            'synthesis_mode': config['synthesis_mode']
        }
        
        # å­˜å‚¨åˆ°å…±äº«ç»“æœä¸­
        with shared_results.lock:
            shared_results.results[dataset_name] = result
        
        print(f"\nâœ“ {dataset_name} é‡é‡‡æ ·å®Œæˆï¼")
        print(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"åŸå§‹æ ·æœ¬: {len(y)} -> é‡é‡‡æ ·å: {len(y_resampled)}")
        print(f"ä¿å­˜è·¯å¾„: {resampled_path}")
        
        return result
        
    except Exception as e:
        # æ›´æ–°è¿›åº¦ï¼šå¤±è´¥
        with progress_dict.lock:
            progress_dict.status[dataset_name] = f"âŒ {dataset_name}é‡é‡‡æ ·å¤±è´¥"
        
        error_msg = f"{dataset_name} é‡é‡‡æ ·å¤±è´¥: {str(e)}"
        print(f"\nâŒ {error_msg}")
        
        result = {
            'dataset': dataset_name,
            'status': 'failed',
            'error': str(e),
            'processing_time': 0
        }
        
        with shared_results.lock:
            shared_results.results[dataset_name] = result
            
        return result


def parallel_resample_datasets(datasets=None, config=None, n_processes=None):
    """
    å¹¶è¡Œé‡é‡‡æ ·å¤šä¸ªæ•°æ®é›† - ä¼˜åŒ–CPUåˆ©ç”¨ç‡
    
    Args:
        datasets: æ•°æ®é›†åˆ—è¡¨ï¼Œé»˜è®¤['ASAS', 'LINEAR', 'MACHO']
        config: é…ç½®å­—å…¸
        n_processes: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ŒNoneæ—¶è‡ªåŠ¨ä¼˜åŒ–é€‰æ‹©
    
    Returns:
        ç»“æœæ±‡æ€»
    """
    if datasets is None:
        datasets = ['ASAS', 'LINEAR', 'MACHO']
    
    # è‡ªåŠ¨ä¼˜åŒ–è¿›ç¨‹æ•°é€‰æ‹©
    if n_processes is None:
        cpu_count = psutil.cpu_count(logical=False)  # ç‰©ç†æ ¸å¿ƒæ•°
        logical_count = psutil.cpu_count(logical=True)  # é€»è¾‘æ ¸å¿ƒæ•°
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # åŸºäºCPUå’Œå†…å­˜ä¼˜åŒ–è¿›ç¨‹æ•°
        # æ¯ä¸ªé‡é‡‡æ ·è¿›ç¨‹å¤§çº¦éœ€è¦2-4GBå†…å­˜
        max_by_memory = int(memory_gb / 3)
        max_by_cpu = min(logical_count, len(datasets) * 2)  # æ¯ä¸ªæ•°æ®é›†æœ€å¤š2ä¸ªè¿›ç¨‹
        
        n_processes = min(max_by_memory, max_by_cpu, len(datasets))
        n_processes = max(1, n_processes)  # è‡³å°‘1ä¸ªè¿›ç¨‹
        
        print(f"ğŸ–¥ï¸  CPUä¿¡æ¯: {cpu_count}ç‰©ç†æ ¸å¿ƒ, {logical_count}é€»è¾‘æ ¸å¿ƒ")
        print(f"ğŸ’¾ å†…å­˜ä¿¡æ¯: {memory_gb:.1f}GB")
        print(f"âš¡ è‡ªåŠ¨é€‰æ‹©è¿›ç¨‹æ•°: {n_processes} (åŸºäº{len(datasets)}ä¸ªæ•°æ®é›†)")
    
    if config is None:
        config = {
            'k_neighbors': 5,
            'enn_neighbors': 3,
            'sampling_strategy': 'balanced',
            'synthesis_mode': 'hybrid',
            'noise_level': 0.05,
            'apply_enn': True,
            'random_state': 535411460,
            'save_dir': '/root/autodl-fs/lnsde-contiformer/data/resampled',
            'plots_dir': '/root/autodl-tmp/lnsde-contiformer/results/pics',
            'generate_plots': True
        }
    
    print(f"\n{'='*80}")
    print(f"å¤šæ ¸å¹¶è¡Œé‡é‡‡æ · - {len(datasets)}ä¸ªæ•°æ®é›†")
    print(f"æ•°æ®é›†: {', '.join(datasets)}")
    print(f"åˆæˆæ¨¡å¼: {config['synthesis_mode']}")
    print(f"å¹¶è¡Œè¿›ç¨‹æ•°: {n_processes}")
    print(f"{'='*80}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['save_dir'], exist_ok=True)
    os.makedirs(config['plots_dir'], exist_ok=True)
    
    # åˆ›å»ºç®¡ç†å™¨å’Œå…±äº«ç»“æœå­˜å‚¨
    manager = Manager()
    shared_results = manager.Namespace()
    shared_results.lock = manager.Lock()
    shared_results.results = manager.dict()
    
    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå­—å…¸
    progress_dict = manager.Namespace()
    progress_dict.lock = manager.Lock()
    progress_dict.status = manager.dict()
    
    # åˆå§‹åŒ–è¿›åº¦çŠ¶æ€
    for dataset in datasets:
        progress_dict.status[dataset] = f"â³ {dataset}ç­‰å¾…å¼€å§‹..."
    
    # å‡†å¤‡å‚æ•°
    args_list = [(dataset, config, shared_results, progress_dict) for dataset in datasets]
    
    # å¹¶è¡Œå¤„ç†
    start_time = datetime.now()
    
    print(f"\nğŸš€ å¯åŠ¨å¹¶è¡Œé‡é‡‡æ ·è¿›ç¨‹...")
    
    # åˆ›å»ºè¿›åº¦æ¡
    def show_progress():
        """å®æ—¶æ˜¾ç¤ºè¿›åº¦çŠ¶æ€"""
        last_status = {}
        completed_count = 0
        
        while completed_count < len(datasets):
            try:
                time.sleep(3)  # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡
                
                with progress_dict.lock:
                    current_status = dict(progress_dict.status)
                
                # æ£€æŸ¥çŠ¶æ€å˜åŒ–
                for dataset, status in current_status.items():
                    if dataset not in last_status or last_status[dataset] != status:
                        print(f"[{datetime.now().strftime('%H:%M:%S')}] {status}")
                        last_status[dataset] = status
                
                # ç»Ÿè®¡å®Œæˆæ•°é‡
                completed_count = sum(1 for status in current_status.values() 
                                    if "å®Œæˆ" in status or "âœ…" in status)
                
                if completed_count > 0:
                    progress_percent = (completed_count / len(datasets)) * 100
                    progress_bar = "â–ˆ" * int(progress_percent / 5) + "â–‘" * (20 - int(progress_percent / 5))
                    print(f"[{datetime.now().strftime('%H:%M:%S')}] æ€»è¿›åº¦: [{progress_bar}] {progress_percent:.1f}% ({completed_count}/{len(datasets)})")
                    
            except Exception as e:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] è¿›åº¦ç›‘æ§é”™è¯¯: {e}")
                time.sleep(1)
    
    # å¯åŠ¨è¿›åº¦ç›‘æ§çº¿ç¨‹
    import threading
    progress_thread = threading.Thread(target=show_progress, daemon=True)
    progress_thread.start()
    
    with Pool(processes=min(n_processes, len(datasets))) as pool:
        pool_results = pool.map(resample_dataset_worker, args_list)
    
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    # ç­‰å¾…è¿›åº¦çº¿ç¨‹ç»“æŸ
    progress_thread.join(timeout=1)
    
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] æ‰€æœ‰è¿›ç¨‹å®Œæˆï¼Œæ±‡æ€»ç»“æœ...")
    
    # æ±‡æ€»ç»“æœ
    results_summary = {
        'total_datasets': len(datasets),
        'successful': 0,
        'failed': 0,
        'total_processing_time': total_time,
        'results': dict(shared_results.results)
    }
    
    print(f"\n{'='*80}")
    print("ğŸ¯ å¹¶è¡Œé‡é‡‡æ ·æœ€ç»ˆç»“æœ")
    print(f"{'='*80}")
    
    total_original = 0
    total_resampled = 0
    
    for dataset in datasets:
        result = shared_results.results.get(dataset, {})
        if result.get('status') == 'success':
            results_summary['successful'] += 1
            original = result.get('original_samples', 0)
            resampled = result.get('resampled_samples', 0)
            processing_time = result.get('processing_time', 0)
            
            total_original += original
            total_resampled += resampled
            
            print(f"âœ… {dataset}: é‡é‡‡æ ·æˆåŠŸ!")
            print(f"   ğŸ“Š æ ·æœ¬æ•°: {original:,} â†’ {resampled:,} (+{resampled-original:,})")
            print(f"   â±ï¸  ç”¨æ—¶: {processing_time:.1f}ç§’")
            print(f"   ğŸ¯ ç±»åˆ«å¹³è¡¡: {result.get('resampled_distribution', {})}")
            print()
        else:
            results_summary['failed'] += 1
            print(f"âŒ {dataset}: é‡é‡‡æ ·å¤±è´¥")
            print(f"   âš ï¸  é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            print()
    
    # æ€§èƒ½ç»Ÿè®¡
    avg_time_per_dataset = total_time / len(datasets) if datasets else 0
    speedup = sum(result.get('processing_time', 0) for result in shared_results.results.values()) / max(total_time, 0.001)
    
    print(f"ğŸ† æ€§èƒ½ç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {results_summary['successful']}/{len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"   âŒ å¤±è´¥: {results_summary['failed']}/{len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"   ğŸ“ˆ æ€»æ ·æœ¬å¢é•¿: {total_original:,} â†’ {total_resampled:,} (+{((total_resampled/max(total_original,1)-1)*100):.1f}%)")
    print(f"   â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’ (å¹³å‡ {avg_time_per_dataset:.1f}ç§’/æ•°æ®é›†)")
    print(f"   ğŸš€ å¹¶è¡ŒåŠ é€Ÿ: {speedup:.1f}x")
    print(f"   ğŸ’¾ åˆæˆæ¨¡å¼: {config['synthesis_mode'].upper()}")
    
    if results_summary['successful'] > 0:
        print(f"\nğŸ’¾ é‡é‡‡æ ·æ–‡ä»¶ä¿å­˜ä½ç½®:")
        for dataset in datasets:
            result = shared_results.results.get(dataset, {})
            if result.get('status') == 'success':
                print(f"   {dataset}: /root/autodl-fs/lnsde-contiformer/data/{dataset}_resampled.pkl")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_path = os.path.join(config['save_dir'], f'parallel_resampling_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pkl')
    with open(summary_path, 'wb') as f:
        pickle.dump(results_summary, f)
    print(f"ç»“æœæ±‡æ€»å·²ä¿å­˜: {summary_path}")
    
    return results_summary


def main():
    parser = argparse.ArgumentParser(description='å¤šæ ¸å¹¶è¡Œé‡é‡‡æ ·è„šæœ¬')
    parser.add_argument('--datasets', nargs='+', default=['ASAS', 'LINEAR', 'MACHO'],
                        help='è¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨')
    parser.add_argument('--processes', type=int, default=None,
                        help='å¹¶è¡Œè¿›ç¨‹æ•° (None=è‡ªåŠ¨ä¼˜åŒ–)')
    parser.add_argument('--mode', choices=['interpolation', 'warping', 'hybrid'], default='hybrid',
                        help='åˆæˆæ¨¡å¼')
    parser.add_argument('--noise-level', type=float, default=0.05,
                        help='å™ªå£°æ°´å¹³')
    parser.add_argument('--no-plots', action='store_true',
                        help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡')
    parser.add_argument('--no-enn', action='store_true',
                        help='ä¸ä½¿ç”¨ENNæ¸…ç†')
    
    args = parser.parse_args()
    
    config = {
        'k_neighbors': 5,
        'enn_neighbors': 3,
        'sampling_strategy': 'balanced',
        'synthesis_mode': args.mode,
        'noise_level': args.noise_level,
        'apply_enn': not args.no_enn,
        'random_state': 535411460,
        'save_dir': '/root/autodl-fs/lnsde-contiformer/data/resampled',
        'plots_dir': '/root/autodl-tmp/lnsde-contiformer/results/pics',
        'generate_plots': not args.no_plots
    }
    
    results = parallel_resample_datasets(
        datasets=args.datasets,
        config=config,
        n_processes=args.processes
    )
    
    print(f"\nğŸ‰ å¹¶è¡Œé‡é‡‡æ ·å®Œæˆï¼")
    return results


if __name__ == "__main__":
    main()