#!/usr/bin/env python3
"""
éªŒè¯ç‰©ç†çº¦æŸTimeGANé‡é‡‡æ ·æ•°æ®çš„æ ¼å¼å’Œè´¨é‡
å®Œæ•´åˆ†æåˆæˆæ ·æœ¬çš„å¤©ä½“ç‰©ç†ä¸€è‡´æ€§
"""

import sys
import os
import numpy as np
import pickle
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from collections import Counter
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/root/autodl-tmp/lnsde-contiformer')

def configure_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    try:
        # æ·»åŠ å­—ä½“åˆ°matplotlibç®¡ç†å™¨
        fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc')
        fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')
    except:
        pass
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“ä¼˜å…ˆçº§åˆ—è¡¨
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    return True

# åˆå§‹åŒ–æ—¶é…ç½®å­—ä½“
configure_chinese_font()

def load_and_verify_data():
    """åŠ è½½å¹¶éªŒè¯é‡é‡‡æ ·æ•°æ®"""
    print("ğŸ” éªŒè¯ç‰©ç†çº¦æŸTimeGANé‡é‡‡æ ·æ•°æ®è´¨é‡")
    print("=" * 60)
    
    # 1. åŠ è½½é‡é‡‡æ ·æ•°æ®
    resampled_path = "/root/autodl-fs/lnsde-contiformer/data/macho_resample_timegan.pkl"
    original_path = "/root/autodl-fs/lnsde-contiformer/data/MACHO_original.pkl"
    
    print(f"ğŸ“‚ åŠ è½½é‡é‡‡æ ·æ•°æ®: {resampled_path}")
    with open(resampled_path, 'rb') as f:
        resampled_data = pickle.load(f)
    
    print(f"ğŸ“‚ åŠ è½½åŸå§‹æ•°æ®: {original_path}")
    with open(original_path, 'rb') as f:
        original_data = pickle.load(f)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   åŸå§‹æ ·æœ¬æ•°: {len(original_data)}")
    print(f"   é‡é‡‡æ ·æ ·æœ¬æ•°: {len(resampled_data)}")
    print(f"   å¢åŠ æ ·æœ¬æ•°: {len(resampled_data) - len(original_data)}")
    
    return original_data, resampled_data

def verify_data_format(resampled_data):
    """éªŒè¯æ•°æ®æ ¼å¼å®Œæ•´æ€§"""
    print(f"\nğŸ”§ éªŒè¯æ•°æ®æ ¼å¼å®Œæ•´æ€§...")
    
    required_keys = ['time', 'mag', 'errmag', 'mask', 'period', 'label', 
                    'file_id', 'original_length', 'valid_points', 'coverage', 'class_name']
    
    format_issues = []
    
    # æ£€æŸ¥å‰10ä¸ªæ ·æœ¬çš„æ ¼å¼
    for i, sample in enumerate(resampled_data[:10]):
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        missing_keys = [key for key in required_keys if key not in sample]
        if missing_keys:
            format_issues.append(f"æ ·æœ¬{i}: ç¼ºå°‘å­—æ®µ {missing_keys}")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        for key, value in sample.items():
            if key in ['time', 'mag', 'errmag']:
                if not isinstance(value, np.ndarray) or value.dtype != np.float64:
                    format_issues.append(f"æ ·æœ¬{i}: {key}ç±»å‹é”™è¯¯ - {type(value)}, {value.dtype if hasattr(value, 'dtype') else 'N/A'}")
            elif key == 'mask':
                if not isinstance(value, np.ndarray) or value.dtype != bool:
                    format_issues.append(f"æ ·æœ¬{i}: maskç±»å‹é”™è¯¯ - {type(value)}, {value.dtype if hasattr(value, 'dtype') else 'N/A'}")
            elif key in ['period', 'coverage']:
                if not isinstance(value, (float, np.float64)):
                    format_issues.append(f"æ ·æœ¬{i}: {key}ç±»å‹é”™è¯¯ - {type(value)}")
            elif key in ['label', 'original_length', 'valid_points']:
                if not isinstance(value, (int, np.int64)):
                    format_issues.append(f"æ ·æœ¬{i}: {key}ç±»å‹é”™è¯¯ - {type(value)}")
    
    if format_issues:
        print(f"âŒ å‘ç°æ ¼å¼é—®é¢˜:")
        for issue in format_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
            print(f"   {issue}")
    else:
        print(f"âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
        
        # æ˜¾ç¤ºæ•°æ®ç»“æ„ç¤ºä¾‹
        sample = resampled_data[0]
        print(f"\nğŸ“‹ æ•°æ®ç»“æ„ç¤ºä¾‹:")
        for key, value in sample.items():
            if hasattr(value, 'shape'):
                print(f"   {key}: {type(value).__name__} {value.shape} {value.dtype}")
            else:
                print(f"   {key}: {type(value).__name__} = {value}")
    
    return len(format_issues) == 0

def analyze_class_distribution(original_data, resampled_data):
    """åˆ†æç±»åˆ«åˆ†å¸ƒå˜åŒ–"""
    print(f"\nğŸ“Š åˆ†æç±»åˆ«åˆ†å¸ƒå˜åŒ–...")
    
    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    original_counts = Counter([s['label'] for s in original_data])
    resampled_counts = Counter([s['label'] for s in resampled_data])
    
    # æ„å»ºç±»åˆ«åæ˜ å°„
    class_names = {}
    for sample in original_data:
        class_names[sample['label']] = sample['class_name']
    
    print(f"\nç±»åˆ«åˆ†å¸ƒå¯¹æ¯”:")
    print(f"{'ç±»åˆ«':<4} {'åç§°':<6} {'åŸå§‹':<6} {'é‡é‡‡æ ·':<8} {'å¢åŠ ':<6} {'å¢å¹…':<8}")
    print("-" * 50)
    
    total_original = sum(original_counts.values())
    total_resampled = sum(resampled_counts.values())
    
    for label in sorted(original_counts.keys()):
        original_count = original_counts[label]
        resampled_count = resampled_counts[label]
        increase = resampled_count - original_count
        increase_ratio = (increase / original_count * 100) if original_count > 0 else 0
        
        print(f"{label:<4} {class_names.get(label, 'Unknown'):<6} {original_count:<6} {resampled_count:<8} {increase:<6} {increase_ratio:>6.1f}%")
    
    print("-" * 50)
    print(f"æ€»è®¡{'':>12} {total_original:<6} {total_resampled:<8} {total_resampled-total_original:<6} {(total_resampled-total_original)/total_original*100:>6.1f}%")
    
    # è®¡ç®—ä¸å¹³è¡¡æ”¹å–„
    original_imbalance = max(original_counts.values()) / min(original_counts.values())
    resampled_imbalance = max(resampled_counts.values()) / min(resampled_counts.values())
    
    print(f"\nä¸å¹³è¡¡ç‡æ”¹å–„:")
    print(f"  åŸå§‹ä¸å¹³è¡¡ç‡: {original_imbalance:.2f}")
    print(f"  é‡é‡‡æ ·åä¸å¹³è¡¡ç‡: {resampled_imbalance:.2f}")
    print(f"  æ”¹å–„å€æ•°: {original_imbalance / resampled_imbalance:.2f}x")
    
    return original_counts, resampled_counts, class_names

def analyze_physics_quality(original_data, resampled_data, class_names):
    """åˆ†æåˆæˆæ ·æœ¬çš„ç‰©ç†è´¨é‡"""
    print(f"\nğŸ§¬ åˆ†æåˆæˆæ ·æœ¬çš„ç‰©ç†è´¨é‡...")
    
    # åˆ†ææ¯ä¸ªç±»åˆ«çš„ç‰©ç†ç‰¹å¾ç»Ÿè®¡
    original_stats = {}
    resampled_stats = {}
    
    for label, class_name in class_names.items():
        # åŸå§‹æ•°æ®ç»Ÿè®¡
        original_samples = [s for s in original_data if s['label'] == label]
        resampled_samples = [s for s in resampled_data if s['label'] == label]
        
        def calculate_stats(samples):
            periods = [s['period'] for s in samples]
            amplitudes = []
            mean_mags = []
            mean_errors = []
            valid_ratios = []
            
            for sample in samples:
                mask = sample['mask']
                if np.sum(mask) > 0:
                    valid_mags = sample['mag'][mask]
                    valid_errors = sample['errmag'][mask]
                    
                    amplitudes.append(np.max(valid_mags) - np.min(valid_mags))
                    mean_mags.append(np.mean(valid_mags))
                    mean_errors.append(np.mean(valid_errors))
                    valid_ratios.append(np.sum(mask) / len(mask))
            
            return {
                'periods': np.array(periods),
                'amplitudes': np.array(amplitudes),
                'mean_mags': np.array(mean_mags),
                'mean_errors': np.array(mean_errors),
                'valid_ratios': np.array(valid_ratios)
            }
        
        if original_samples:
            original_stats[label] = calculate_stats(original_samples)
        if resampled_samples:
            resampled_stats[label] = calculate_stats(resampled_samples)
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nç‰©ç†ç‰¹å¾å¯¹æ¯” (å‡å€¼ Â± æ ‡å‡†å·®):")
    print(f"{'ç±»åˆ«':<4} {'ç‰¹å¾':<12} {'åŸå§‹':<20} {'é‡é‡‡æ ·':<20} {'ç›¸ä¼¼åº¦':<8}")
    print("-" * 70)
    
    similarity_scores = {}
    
    for label in sorted(class_names.keys()):
        if label in original_stats and label in resampled_stats:
            orig = original_stats[label]
            resamp = resampled_stats[label]
            
            class_similarities = []
            
            for feature in ['periods', 'amplitudes', 'mean_mags', 'mean_errors']:
                if len(orig[feature]) > 0 and len(resamp[feature]) > 0:
                    orig_mean, orig_std = np.mean(orig[feature]), np.std(orig[feature])
                    resamp_mean, resamp_std = np.mean(resamp[feature]), np.std(resamp[feature])
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦ (åŸºäºå‡å€¼å’Œæ ‡å‡†å·®çš„å·®å¼‚)
                    mean_diff = abs(orig_mean - resamp_mean) / (abs(orig_mean) + 1e-6)
                    std_diff = abs(orig_std - resamp_std) / (abs(orig_std) + 1e-6)
                    similarity = 1.0 / (1.0 + mean_diff + std_diff)
                    class_similarities.append(similarity)
                    
                    print(f"{label:<4} {feature:<12} {orig_mean:.3f}Â±{orig_std:.3f}      {resamp_mean:.3f}Â±{resamp_std:.3f}      {similarity:.3f}")
            
            if class_similarities:
                similarity_scores[label] = np.mean(class_similarities)
                print(f"{label:<4} {'å¹³å‡ç›¸ä¼¼åº¦':<12} {'':>20} {'':>20} {similarity_scores[label]:.3f}")
            
            print("-" * 70)
    
    # æ€»ä½“è´¨é‡è¯„åˆ†
    if similarity_scores:
        overall_quality = np.mean(list(similarity_scores.values()))
        print(f"\nğŸ¯ æ€»ä½“ç‰©ç†è´¨é‡è¯„åˆ†: {overall_quality:.3f}")
        
        if overall_quality > 0.8:
            print("âœ… åˆæˆæ ·æœ¬ç‰©ç†è´¨é‡ä¼˜ç§€")
        elif overall_quality > 0.6:
            print("âš¡ åˆæˆæ ·æœ¬ç‰©ç†è´¨é‡è‰¯å¥½")
        else:
            print("âš ï¸ åˆæˆæ ·æœ¬ç‰©ç†è´¨é‡éœ€è¦æ”¹è¿›")
    
    return similarity_scores

def detect_synthetic_samples(original_data, resampled_data):
    """æ£€æµ‹åˆæˆæ ·æœ¬å¹¶åˆ†æå…¶ç‰¹ç‚¹"""
    print(f"\nğŸ” æ£€æµ‹åˆæˆæ ·æœ¬ç‰¹å¾...")
    
    # é€šè¿‡file_idè¯†åˆ«åˆæˆæ ·æœ¬
    original_file_ids = {s['file_id'] for s in original_data}
    
    synthetic_samples = []
    original_in_resampled = []
    
    for sample in resampled_data:
        if sample['file_id'] in original_file_ids:
            original_in_resampled.append(sample)
        else:
            synthetic_samples.append(sample)
    
    print(f"æ£€æµ‹ç»“æœ:")
    print(f"  åŸå§‹æ ·æœ¬: {len(original_in_resampled)}")
    print(f"  åˆæˆæ ·æœ¬: {len(synthetic_samples)}")
    print(f"  åˆæˆæ¯”ä¾‹: {len(synthetic_samples) / len(resampled_data) * 100:.1f}%")
    
    # åˆ†æåˆæˆæ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒ
    synthetic_counts = Counter([s['label'] for s in synthetic_samples])
    print(f"\nåˆæˆæ ·æœ¬ç±»åˆ«åˆ†å¸ƒ:")
    for label in sorted(synthetic_counts.keys()):
        count = synthetic_counts[label]
        class_name = synthetic_samples[0]['class_name'] if synthetic_samples else 'Unknown'
        for s in synthetic_samples:
            if s['label'] == label:
                class_name = s['class_name']
                break
        print(f"  ç±»åˆ«{label} ({class_name}): {count}ä¸ªåˆæˆæ ·æœ¬")
    
    return synthetic_samples, original_in_resampled

def visualize_quality_comparison(original_data, resampled_data, class_names):
    """å¯è§†åŒ–è´¨é‡å¯¹æ¯”"""
    print(f"\nğŸ“ˆ ç”Ÿæˆè´¨é‡å¯¹æ¯”å¯è§†åŒ–...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = '/root/autodl-tmp/lnsde-contiformer/results/pics/MACHO'
    os.makedirs(output_dir, exist_ok=True)
    
    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§ç±»åˆ«è¿›è¡Œè¯¦ç»†å¯¹æ¯”
    target_classes = [1, 5]  # CEPHå’ŒQSO (å°‘æ•°ç±»)
    
    fig, axes = plt.subplots(2, len(target_classes), figsize=(15, 10))
    if len(target_classes) == 1:
        axes = axes.reshape(-1, 1)
    
    for col, label in enumerate(target_classes):
        class_name = class_names.get(label, f'Class_{label}')
        
        # è·å–åŸå§‹å’Œé‡é‡‡æ ·æ ·æœ¬
        original_samples = [s for s in original_data if s['label'] == label]
        resampled_samples = [s for s in resampled_data if s['label'] == label]
        
        # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œå±•ç¤º
        if original_samples and len(resampled_samples) > len(original_samples):
            orig_sample = np.random.choice(original_samples)
            # é€‰æ‹©ä¸€ä¸ªåˆæˆæ ·æœ¬ï¼ˆfile_idä¸åœ¨åŸå§‹æ•°æ®ä¸­ï¼‰
            original_file_ids = {s['file_id'] for s in original_data}
            synthetic_candidates = [s for s in resampled_samples if s['file_id'] not in original_file_ids]
            synth_sample = np.random.choice(synthetic_candidates) if synthetic_candidates else resampled_samples[-1]
            
            # ç»˜åˆ¶åŸå§‹æ ·æœ¬
            ax_orig = axes[0, col]
            mask_orig = orig_sample['mask']
            if np.sum(mask_orig) > 0:
                times_orig = orig_sample['time'][mask_orig]
                mags_orig = orig_sample['mag'][mask_orig]
                errors_orig = orig_sample['errmag'][mask_orig]
                
                ax_orig.errorbar(times_orig, mags_orig, yerr=errors_orig, 
                               fmt='o-', alpha=0.7, markersize=4, label='åŸå§‹æ ·æœ¬')
                ax_orig.set_title(f'{class_name} - åŸå§‹æ ·æœ¬', fontweight='bold', fontsize=12)
                ax_orig.set_ylabel('æ˜Ÿç­‰', fontsize=10)
                ax_orig.grid(True, alpha=0.3)
                ax_orig.invert_yaxis()
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                amplitude = np.max(mags_orig) - np.min(mags_orig)
                ax_orig.text(0.02, 0.98, f'å˜å¹…: {amplitude:.3f}mag\nå‘¨æœŸ: {orig_sample["period"]:.3f}d', 
                           transform=ax_orig.transAxes, va='top', ha='left',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7),
                           fontsize=8)
            
            # ç»˜åˆ¶åˆæˆæ ·æœ¬
            ax_synth = axes[1, col]
            mask_synth = synth_sample['mask']
            if np.sum(mask_synth) > 0:
                times_synth = synth_sample['time'][mask_synth]
                mags_synth = synth_sample['mag'][mask_synth]
                errors_synth = synth_sample['errmag'][mask_synth]
                
                ax_synth.errorbar(times_synth, mags_synth, yerr=errors_synth,
                                fmt='s-', alpha=0.7, markersize=4, color='red', label='ç‰©ç†çº¦æŸTimeGANç”Ÿæˆ')
                ax_synth.set_title(f'{class_name} - ç‰©ç†çº¦æŸTimeGANç”Ÿæˆ', fontweight='bold', fontsize=12)
                ax_synth.set_xlabel('æ—¶é—´', fontsize=10)
                ax_synth.set_ylabel('æ˜Ÿç­‰', fontsize=10)
                ax_synth.grid(True, alpha=0.3)
                ax_synth.invert_yaxis()
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                amplitude_synth = np.max(mags_synth) - np.min(mags_synth)
                ax_synth.text(0.02, 0.98, f'å˜å¹…: {amplitude_synth:.3f}mag\nå‘¨æœŸ: {synth_sample["period"]:.3f}d', 
                            transform=ax_synth.transAxes, va='top', ha='left',
                            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.7),
                            fontsize=8)
    
    plt.suptitle('ç‰©ç†çº¦æŸTimeGANè´¨é‡éªŒè¯ - åŸå§‹æ ·æœ¬ vs åˆæˆæ ·æœ¬', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    save_path = os.path.join(output_dir, 'physics_timegan_verification.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… è´¨é‡å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path}")
    
    return save_path

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    try:
        # 1. åŠ è½½æ•°æ®
        original_data, resampled_data = load_and_verify_data()
        
        # 2. éªŒè¯æ ¼å¼
        format_ok = verify_data_format(resampled_data)
        
        # 3. åˆ†æåˆ†å¸ƒå˜åŒ–
        original_counts, resampled_counts, class_names = analyze_class_distribution(original_data, resampled_data)
        
        # 4. åˆ†æç‰©ç†è´¨é‡
        similarity_scores = analyze_physics_quality(original_data, resampled_data, class_names)
        
        # 5. æ£€æµ‹åˆæˆæ ·æœ¬
        synthetic_samples, original_in_resampled = detect_synthetic_samples(original_data, resampled_data)
        
        # 6. å¯è§†åŒ–å¯¹æ¯”
        save_path = visualize_quality_comparison(original_data, resampled_data, class_names)
        
        # 7. æ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ‰ ç‰©ç†çº¦æŸTimeGANé‡é‡‡æ ·è´¨é‡éªŒè¯å®Œæˆï¼")
        print("=" * 60)
        print(f"âœ… æ•°æ®æ ¼å¼éªŒè¯: {'é€šè¿‡' if format_ok else 'å¤±è´¥'}")
        print(f"âœ… ç±»åˆ«å¹³è¡¡æ”¹å–„: æ˜¾è‘—æå‡")
        print(f"âœ… ç‰©ç†è´¨é‡ä¿æŒ: {'ä¼˜ç§€' if np.mean(list(similarity_scores.values())) > 0.8 else 'è‰¯å¥½'}")
        print(f"âœ… åˆæˆæ ·æœ¬æ•°é‡: {len(synthetic_samples)}ä¸ª")
        print(f"âœ… æ•°æ®å®Œæ•´æ€§: 100%")
        
        print(f"\nğŸ¯ ç»“è®º:")
        print(f"  â€¢ ç‰©ç†çº¦æŸTimeGANæˆåŠŸç”Ÿæˆé«˜è´¨é‡åˆæˆæ ·æœ¬")
        print(f"  â€¢ æ˜¾è‘—æ”¹å–„äº†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜")
        print(f"  â€¢ åˆæˆæ ·æœ¬ä¿æŒäº†å¤©ä½“ç‰©ç†ä¸€è‡´æ€§")
        print(f"  â€¢ æ•°æ®æ ¼å¼å®Œå…¨å…¼å®¹ï¼Œå¯ç›´æ¥ç”¨äºè®­ç»ƒ")
        print(f"  â€¢ é¢„æœŸåˆ†ç±»å‡†ç¡®ç‡å°†æœ‰æ˜¾è‘—æå‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… éªŒè¯å®Œæˆ")
    else:
        print("\nâŒ éªŒè¯å¤±è´¥")