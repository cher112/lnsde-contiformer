#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–åçš„æ¡†æ¶
éªŒè¯ç¨³å®šæ€§ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""

import torch
import sys
import time
sys.path.append('/root/autodl-tmp/lnsde-contiformer')

from models import LinearNoiseSDEContiformer
from utils import get_device, set_seed
from utils.stability_fixes import StableTrainingManager, create_optimized_training_config
import pickle
import numpy as np

def quick_test():
    """å¿«é€Ÿæµ‹è¯•è®­ç»ƒç¨³å®šæ€§"""
    print("="*60)
    print("æ¡†æ¶ä¼˜åŒ–æµ‹è¯•")
    print("="*60)
    
    # é…ç½®
    config = create_optimized_training_config()
    device = get_device('auto')
    set_seed(42)
    
    print(f"\nä¼˜åŒ–é…ç½®:")
    print(f"  Lionå­¦ä¹ ç‡: {config['learning_rate']:.2e}")
    print(f"  æ¢¯åº¦è£å‰ª: {config['gradient_clip']}")
    print(f"  æ ‡ç­¾å¹³æ»‘: {config['label_smoothing']}")
    print(f"  ä¿®å¤é›¶è¯¯å·®: {config['fix_zero_errors']}")
    print(f"  ä¿®å¤æ—¶é—´å•è°ƒæ€§: {config['fix_time_monotonicity']}")
    
    # åˆ›å»ºæœ€ç®€å•çš„æ¨¡å‹
    model = LinearNoiseSDEContiformer(
        input_dim=3,
        hidden_channels=32,
        num_classes=7,
        contiformer_dim=64,
        n_heads=2,
        n_layers=1,
        use_sde=False,  # å…ˆç¦ç”¨SDE
        use_contiformer=False,
        use_cga=False
    ).to(device)
    
    print(f"\næ¨¡å‹é…ç½®:")
    print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  è®¾å¤‡: {device}")
    
    # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
    manager = StableTrainingManager(model, config)
    
    # åŠ è½½å°‘é‡æ•°æ®æµ‹è¯•
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    data_path = '/root/autodl-fs/lnsde-contiformer/data/MACHO_original.pkl'
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    # åªä½¿ç”¨å‰100ä¸ªæ ·æœ¬
    data = data[:100]
    
    # å‡†å¤‡æ‰¹æ¬¡
    batch_size = 8
    batches = []
    
    for i in range(0, len(data), batch_size):
        batch_data = data[i:i+batch_size]
        
        # æå–æ•°æ®
        times_list = []
        values_list = []
        labels_list = []
        max_len = 0
        
        for sample in batch_data:
            t = sample['time'][:200]  # é™åˆ¶é•¿åº¦
            m = sample['mag'][:200]
            e = sample['errmag'][:200]
            
            times_list.append(t)
            values_list.append(np.stack([m, e], axis=-1))
            labels_list.append(sample['label'])
            max_len = max(max_len, len(t))
        
        # Padding
        times_tensor = torch.zeros(len(batch_data), max_len)
        values_tensor = torch.zeros(len(batch_data), max_len, 2)
        mask_tensor = torch.zeros(len(batch_data), max_len, dtype=torch.bool)
        
        for j, (t, v) in enumerate(zip(times_list, values_list)):
            length = len(t)
            times_tensor[j, :length] = torch.tensor(t, dtype=torch.float32)
            values_tensor[j, :length] = torch.tensor(v, dtype=torch.float32)
            mask_tensor[j, :length] = True
        
        labels_tensor = torch.tensor(labels_list, dtype=torch.long)
        
        batches.append((times_tensor, values_tensor, labels_tensor, mask_tensor))
        
        if len(batches) >= 5:  # åªç”¨5ä¸ªæ‰¹æ¬¡æµ‹è¯•
            break
    
    print(f"å‡†å¤‡äº† {len(batches)} ä¸ªæ‰¹æ¬¡")
    
    # è®­ç»ƒæµ‹è¯•
    print("\nå¼€å§‹è®­ç»ƒæµ‹è¯•...")
    start_time = time.time()
    
    losses = []
    accs = []
    
    for i, batch in enumerate(batches):
        result = manager.train_step(batch)
        
        if not result['skipped']:
            losses.append(result['loss'])
            accs.append(result['acc'])
            print(f"Batch {i+1}: Loss={result['loss']:.4f}, Acc={result['acc']:.4f}")
        else:
            print(f"Batch {i+1}: SKIPPED (NaN detected)")
    
    elapsed = time.time() - start_time
    
    # ç»Ÿè®¡
    print(f"\næµ‹è¯•å®Œæˆ (è€—æ—¶: {elapsed:.2f}ç§’)")
    if losses:
        print(f"å¹³å‡Loss: {np.mean(losses):.4f}")
        print(f"å¹³å‡Acc: {np.mean(accs):.4f}")
        print(f"NaNæ‰¹æ¬¡: {len(batches) - len(losses)}")
        
        if np.mean(losses) < 10 and len(losses) == len(batches):
            print("\nâœ… æ¡†æ¶ä¼˜åŒ–æˆåŠŸï¼")
            print("  - æ— NaN Loss")
            print("  - æ•°å€¼ç¨³å®š")
            print("  - GPUè¿ç®—æ­£å¸¸")
        else:
            print("\nâš ï¸ ä»æœ‰ç¨³å®šæ€§é—®é¢˜")
    else:
        print("âŒ æ‰€æœ‰æ‰¹æ¬¡éƒ½å¤±è´¥äº†")
    
    # æµ‹è¯•å¸¦SDEçš„ç‰ˆæœ¬
    print("\n" + "="*40)
    print("æµ‹è¯•SDEç»„ä»¶...")
    
    model_sde = LinearNoiseSDEContiformer(
        input_dim=3,
        hidden_channels=32,
        num_classes=7,
        contiformer_dim=64,
        n_heads=2,
        n_layers=1,
        use_sde=True,  # å¯ç”¨SDE
        use_contiformer=False,
        use_cga=False,
        dt=0.1,  # æ›´å¤§çš„æ­¥é•¿
        sde_method='euler'
    ).to(device)
    
    manager_sde = StableTrainingManager(model_sde, config)
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    batch = batches[0]
    result = manager_sde.train_step(batch)
    
    if not result['skipped']:
        print(f"âœ… SDEæµ‹è¯•æˆåŠŸ: Loss={result['loss']:.4f}")
    else:
        print("âŒ SDEä»æœ‰NaNé—®é¢˜")
    
    return len(losses) == len(batches)


if __name__ == "__main__":
    success = quick_test()
    
    print("\n" + "="*60)
    print("ä¼˜åŒ–æ€»ç»“")
    print("="*60)
    
    print("""
å…³é”®ä¿®å¤:
1. âœ… Maské™¤é›¶é”™è¯¯ - æ·»åŠ epsé¿å…é™¤é›¶
2. âœ… Lionå­¦ä¹ ç‡ - é™ä½åˆ°5e-6ï¼ˆåŸæ¥çš„1/20ï¼‰
3. âœ… é›¶è¯¯å·®å¤„ç† - clampåˆ°æœ€å°å€¼1e-6
4. âœ… æ—¶é—´å•è°ƒæ€§ - è‡ªåŠ¨ä¿®å¤éé€’å¢åºåˆ—
5. âœ… çº¯Tensorè¿”å› - é¿å…tupleï¼Œç»Ÿä¸€GPUè¿ç®—

æµ‹è¯•å‘½ä»¤:
# æœ€ç¨³å®šé…ç½®ï¼ˆæ¨èï¼‰
python main.py --use_stable_training --use_sde 0 --use_contiformer 0 --epochs 1 --batch_size 8

# é€æ­¥å¯ç”¨ç»„ä»¶
python main.py --use_stable_training --use_sde 1 --use_contiformer 0 --epochs 1

# å®Œæ•´é…ç½®
python main.py --use_stable_training --learning_rate 5e-6 --gradient_clip 0.5 --epochs 1
""")
    
    if success:
        print("ğŸ‰ æ¡†æ¶ä¼˜åŒ–å®Œæˆï¼Œå¯ä»¥ç¨³å®šè®­ç»ƒï¼")
    else:
        print("âš ï¸ è¯·è¿›ä¸€æ­¥è°ƒè¯•")