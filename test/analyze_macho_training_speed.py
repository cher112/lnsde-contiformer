#!/usr/bin/env python3
"""
æ£€æŸ¥MACHO TimeGANé‡é‡‡æ ·æ•°æ®ï¼Œåˆ†æè®­ç»ƒå˜æ…¢çš„åŸå› 
"""

import pickle
import numpy as np
from collections import Counter
import sys

def analyze_macho_timegan_data():
    """åˆ†æMACHO TimeGANæ•°æ®"""
    print("ğŸ” åˆ†æMACHO TimeGANé‡é‡‡æ ·æ•°æ®...")
    
    # åŠ è½½TimeGANæ•°æ®
    timegan_path = '/root/autodl-fs/lnsde-contiformer/data/macho_resample_timegan.pkl'
    with open(timegan_path, 'rb') as f:
        timegan_data = pickle.load(f)
    
    # åŠ è½½åŸå§‹æ•°æ®å¯¹æ¯”
    original_path = '/root/autodl-fs/lnsde-contiformer/data/MACHO_original.pkl'
    with open(original_path, 'rb') as f:
        original_data = pickle.load(f)
    
    print(f"åŸå§‹æ•°æ®: {len(original_data)} æ ·æœ¬")
    print(f"TimeGANæ•°æ®: {len(timegan_data)} æ ·æœ¬")
    print(f"æ•°æ®å¢é•¿: {len(timegan_data) - len(original_data)} æ ·æœ¬ ({(len(timegan_data)/len(original_data) - 1)*100:.1f}%)")
    
    # æ£€æŸ¥æ ·æœ¬ç»“æ„
    print(f"\nğŸ“Š æ ·æœ¬ç»“æ„å¯¹æ¯”:")
    orig_sample = original_data[0]
    timegan_sample = timegan_data[0]
    
    print(f"åŸå§‹æ ·æœ¬å­—æ®µ: {list(orig_sample.keys())}")
    print(f"TimeGANæ ·æœ¬å­—æ®µ: {list(timegan_sample.keys())}")
    
    # æ£€æŸ¥åºåˆ—é•¿åº¦åˆ†å¸ƒ
    print(f"\nğŸ“ åºåˆ—é•¿åº¦åˆ†æ:")
    
    def analyze_lengths(data, name):
        lengths = []
        for sample in data:
            if 'mask' in sample:
                mask = sample['mask']
                valid_length = np.sum(mask.astype(bool))
            else:
                valid_length = len(sample['time'])
            lengths.append(valid_length)
        
        print(f"{name}:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(lengths):.1f}")
        print(f"  æœ€å¤§é•¿åº¦: {np.max(lengths)}")
        print(f"  æœ€å°é•¿åº¦: {np.min(lengths)}")
        print(f"  é•¿åº¦æ ‡å‡†å·®: {np.std(lengths):.1f}")
        
        return lengths
    
    orig_lengths = analyze_lengths(original_data, "åŸå§‹æ•°æ®")
    timegan_lengths = analyze_lengths(timegan_data, "TimeGANæ•°æ®")
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    print(f"\nğŸ·ï¸ ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”:")
    orig_labels = [item['label'] for item in original_data]
    timegan_labels = [item['label'] for item in timegan_data]
    
    orig_counts = Counter(orig_labels)
    timegan_counts = Counter(timegan_labels)
    
    print(f"åŸå§‹ç±»åˆ«åˆ†å¸ƒ: {dict(orig_counts)}")
    print(f"TimeGANç±»åˆ«åˆ†å¸ƒ: {dict(timegan_counts)}")
    
    # æ£€æŸ¥æ•°æ®ç±»å‹å’Œå¤§å°
    print(f"\nğŸ’¾ æ•°æ®å¤§å°åˆ†æ:")
    
    def get_sample_size(sample):
        total_size = 0
        for key, value in sample.items():
            if hasattr(value, 'nbytes'):
                total_size += value.nbytes
            else:
                total_size += sys.getsizeof(value)
        return total_size
    
    orig_sample_size = get_sample_size(orig_sample)
    timegan_sample_size = get_sample_size(timegan_sample)
    
    print(f"åŸå§‹æ ·æœ¬å¤§å°: {orig_sample_size} å­—èŠ‚")
    print(f"TimeGANæ ·æœ¬å¤§å°: {timegan_sample_size} å­—èŠ‚")
    print(f"å•æ ·æœ¬å¢é•¿: {timegan_sample_size - orig_sample_size} å­—èŠ‚")
    
    total_orig_size = orig_sample_size * len(original_data)
    total_timegan_size = timegan_sample_size * len(timegan_data)
    
    print(f"æ€»æ•°æ®å¤§å°:")
    print(f"  åŸå§‹: {total_orig_size / 1024 / 1024:.1f} MB")
    print(f"  TimeGAN: {total_timegan_size / 1024 / 1024:.1f} MB")
    print(f"  å¢é•¿: {(total_timegan_size - total_orig_size) / 1024 / 1024:.1f} MB")
    
    # åˆ†æå¯èƒ½çš„è®­ç»ƒæ…¢åŸå› 
    print(f"\nğŸŒ å¯èƒ½çš„è®­ç»ƒå˜æ…¢åŸå› åˆ†æ:")
    
    # 1. æ ·æœ¬æ•°é‡å¢åŠ 
    sample_increase = len(timegan_data) / len(original_data)
    print(f"1. æ ·æœ¬æ•°é‡å¢åŠ  {sample_increase:.2f}x - ç›´æ¥å½±å“è®­ç»ƒæ—¶é—´")
    
    # 2. åºåˆ—é•¿åº¦å˜åŒ–
    avg_orig_len = np.mean(orig_lengths)
    avg_timegan_len = np.mean(timegan_lengths)
    length_ratio = avg_timegan_len / avg_orig_len
    print(f"2. å¹³å‡åºåˆ—é•¿åº¦: {avg_orig_len:.1f} â†’ {avg_timegan_len:.1f} ({length_ratio:.2f}x)")
    
    # 3. æ•°æ®å¤æ‚åº¦
    orig_unique_lengths = len(set(orig_lengths))
    timegan_unique_lengths = len(set(timegan_lengths))
    print(f"3. åºåˆ—é•¿åº¦å¤šæ ·æ€§: {orig_unique_lengths} â†’ {timegan_unique_lengths} ç§")
    
    # 4. å†…å­˜ä½¿ç”¨ä¼°ç®—
    memory_ratio = total_timegan_size / total_orig_size
    print(f"4. å†…å­˜ä½¿ç”¨å¢é•¿: {memory_ratio:.2f}x")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é•¿çš„åºåˆ—
    print(f"\nâš ï¸ å¼‚å¸¸æ£€æŸ¥:")
    long_sequences = [l for l in timegan_lengths if l > 1000]
    if long_sequences:
        print(f"å‘ç° {len(long_sequences)} ä¸ªè¶…é•¿åºåˆ— (>1000ç‚¹)")
        print(f"æœ€é•¿åºåˆ—: {max(long_sequences)} ç‚¹")
    else:
        print("æœªå‘ç°å¼‚å¸¸é•¿åºåˆ—")
    
    # è®¡ç®—ç†è®ºè®­ç»ƒæ—¶é—´å¢é•¿
    # è®­ç»ƒæ—¶é—´ â‰ˆ æ ·æœ¬æ•° Ã— åºåˆ—é•¿åº¦ Ã— æ¨¡å‹å¤æ‚åº¦
    theoretical_slowdown = sample_increase * length_ratio
    print(f"\nğŸ“ˆ ç†è®ºè®­ç»ƒæ—¶é—´å¢é•¿: {theoretical_slowdown:.2f}x")
    
    return {
        'sample_increase': sample_increase,
        'length_ratio': length_ratio, 
        'memory_ratio': memory_ratio,
        'theoretical_slowdown': theoretical_slowdown
    }

def suggest_optimization():
    """å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ"""
    print(f"\nğŸ’¡ è®­ç»ƒåŠ é€Ÿå»ºè®®:")
    
    print(f"1. å‡å°æ‰¹æ¬¡å¤§å° (batch_size):")
    print(f"   --batch_size 32  # ä»64å‡åˆ°32")
    print(f"   --batch_size 16  # æ›´ä¿å®ˆçš„é€‰æ‹©")
    
    print(f"2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯:")
    print(f"   --gradient_accumulation_steps 4  # ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°")
    
    print(f"3. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹:")
    print(f"   --use_gradient_checkpoint  # èŠ‚çœæ˜¾å­˜")
    
    print(f"4. è°ƒæ•´å·¥ä½œè¿›ç¨‹:")
    print(f"   --num_workers 8  # å‡å°‘æ•°æ®åŠ è½½æ—¶é—´")
    print(f"   --prefetch_factor 2  # å‡å°‘å†…å­˜å ç”¨")
    
    print(f"5. ä½¿ç”¨æ›´å¿«çš„SDEé…ç½®:")
    print(f"   --sde_config 3  # æ—¶é—´ä¼˜å…ˆé…ç½®")
    
    print(f"6. æ—©æœŸæµ‹è¯•:")
    print(f"   --epochs 10  # å…ˆç”¨å°‘é‡epochæµ‹è¯•")
    
    print(f"7. å¦‚æœæ˜¾å­˜ä¸è¶³:")
    print(f"   --no_amp  # ç¦ç”¨æ··åˆç²¾åº¦")
    print(f"   --hidden_channels 64  # å‡å°æ¨¡å‹è§„æ¨¡")

if __name__ == "__main__":
    stats = analyze_macho_timegan_data()
    suggest_optimization()
    
    print(f"\nğŸ“‹ æ€»ç»“:")
    print(f"MACHO TimeGANæ•°æ®å¢é•¿äº† {stats['theoretical_slowdown']:.1f}xï¼Œä¸»è¦åŸå› :")
    print(f"  â€¢ æ ·æœ¬æ•°é‡: +{(stats['sample_increase']-1)*100:.0f}%")
    print(f"  â€¢ åºåˆ—é•¿åº¦: {stats['length_ratio']:.2f}x")
    print(f"  â€¢ å†…å­˜ä½¿ç”¨: {stats['memory_ratio']:.2f}x")
    print(f"å»ºè®®ä½¿ç”¨å°æ‰¹æ¬¡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯æ¥åŠ é€Ÿè®­ç»ƒã€‚")