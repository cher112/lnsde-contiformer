#!/usr/bin/env python3
"""
LINEARæ•°æ®é›†å¿«é€Ÿé‡é‡‡æ · - ä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1
ä½¿ç”¨æ··åˆé‡é‡‡æ ·é¿å…TimeGANçš„å¤æ‚åº¦é—®é¢˜
"""

import os
import sys
import pickle
import numpy as np
import torch
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/root/autodl-tmp/lnsde-contiformer')

from utils.resampling import HybridResampler

def configure_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    import matplotlib.font_manager as fm
    
    # æ·»åŠ å­—ä½“åˆ°matplotlibç®¡ç†å™¨
    fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc')
    fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')
    
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    return True

configure_chinese_font()

def pad_sequences_to_fixed_length(X_list, max_length=512):
    """å°†å˜é•¿åºåˆ—å¡«å……åˆ°å›ºå®šé•¿åº¦"""
    print(f"ğŸ“ å°†åºåˆ—ç»Ÿä¸€å¡«å……åˆ°é•¿åº¦: {max_length}")
    
    n_features = X_list[0].shape[1]  # é€šå¸¸æ˜¯3 [time, mag, error]
    padded_X = np.zeros((len(X_list), max_length, n_features), dtype=np.float32)
    
    for i, seq in enumerate(X_list):
        seq_len = min(len(seq), max_length)
        padded_X[i, :seq_len, :] = seq[:seq_len]
    
    return padded_X

def apply_linear_hybrid_resampling():
    """åº”ç”¨LINEARæ•°æ®é›†æ··åˆé‡é‡‡æ ·ï¼Œä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1"""
    print(f"ğŸš€ LINEARæ··åˆé‡é‡‡æ · - ä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1")
    print("=" * 50)
    
    # åŠ è½½åŸå§‹æ•°æ®
    data_path = '/root/autodl-fs/lnsde-contiformer/data/LINEAR_original.pkl'
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"åŸå§‹æ•°æ®: {len(data)}æ ·æœ¬")
    
    # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
    X_list, y_list = [], []
    
    for item in data:
        # æå–ç‰¹å¾
        mask = item['mask'].astype(bool)
        times = item['time'][mask]
        mags = item['mag'][mask]
        errors = item['errmag'][mask]
        
        if len(times) < 10:  # è¿‡æ»¤å¤ªçŸ­çš„åºåˆ—
            continue
        
        # æ„å»ºç‰¹å¾çŸ©é˜µ [time, mag, error]
        features = np.column_stack([times, mags, errors])
        
        X_list.append(features)
        y_list.append(item['label'])
    
    # ç»Ÿä¸€åºåˆ—é•¿åº¦
    X = pad_sequences_to_fixed_length(X_list, max_length=512)
    y = np.array(y_list, dtype=np.int64)
    
    print(f"æœ‰æ•ˆæ ·æœ¬: {len(X)}ä¸ª")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {Counter(y.tolist())}")
    
    # åˆ†æç±»åˆ«åˆ†å¸ƒ
    class_counts = Counter(y.tolist())
    print(f"\nğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ:")
    for cls in sorted(class_counts.keys()):
        count = class_counts[cls]
        class_name_map = {0: 'Beta_Persei', 1: 'Delta_Scuti', 2: 'RR_Lyrae_FM', 3: 'RR_Lyrae_FO', 4: 'W_Ursae_Maj'}
        name = class_name_map.get(cls, f'Class_{cls}')
        print(f"  ç±»åˆ«{cls} ({name}): {count}æ ·æœ¬")
    
    # é‡é‡‡æ ·ç­–ç•¥ - é‡ç‚¹å¢å¼ºç±»åˆ«0å’Œ1
    target_strategy = {
        0: 400,   # Beta_Persei: 291 â†’ 400 (+37%)
        1: 300,   # Delta_Scuti: 70 â†’ 300 (+328%) é‡ç‚¹å¢å¼º  
        2: 2234,  # RR_Lyrae_FM: ä¿æŒä¸å˜
        3: 749,   # RR_Lyrae_FO: ä¿æŒä¸å˜
        4: 1860   # W_Ursae_Maj: ä¿æŒä¸å˜
    }
    
    print(f"\nğŸ¯ é‡é‡‡æ ·ç›®æ ‡:")
    for cls, target_count in target_strategy.items():
        current = class_counts.get(cls, 0)
        increase = target_count - current
        increase_rate = increase / current if current > 0 else 0
        print(f"  ç±»åˆ«{cls}: {current} â†’ {target_count} (+{increase}, +{increase_rate:.0%})")
    
    # ä½¿ç”¨æ··åˆé‡é‡‡æ ·å™¨ - é¿å…TimeGANå¤æ‚åº¦
    resampler = HybridResampler(
        synthesis_mode='hybrid',  # ä½¿ç”¨æ··åˆæ¨¡å¼è€ŒéTimeGAN
        smote_k_neighbors=8,      # å¢åŠ é‚»å±…æ•°æå‡è´¨é‡
        noise_level=0.02,         # ä½å™ªå£°ä¿æŒæ•°æ®è´¨é‡
        sampling_strategy=target_strategy,
        apply_enn=True,           # åº”ç”¨ENNæ¸…ç†
        random_state=42
    )
    
    print(f"\nğŸ“ˆ å¼€å§‹æ··åˆé‡é‡‡æ ·...")
    print(f"é…ç½®: æ··åˆæ¨¡å¼, 8é‚»å±…, ENNæ¸…ç†")
    
    # æ‰§è¡Œé‡é‡‡æ · - HybridResamplerè¿”å›4ä¸ªå€¼
    X_resampled, y_resampled, times_resampled, masks_resampled = resampler.fit_resample(X, y)
    
    print(f"\nâœ… é‡é‡‡æ ·å®Œæˆ!")
    print(f"é‡é‡‡æ ·åæ ·æœ¬æ•°: {len(X_resampled)}")
    print(f"é‡é‡‡æ ·åç±»åˆ«åˆ†å¸ƒ: {Counter(y_resampled)}")
    
    # ç»Ÿè®¡æ•ˆæœ
    original_counts = Counter(y)
    resampled_counts = Counter(y_resampled)
    
    print(f"\nğŸ“Š å¢å¼ºæ•ˆæœç»Ÿè®¡:")
    for cls in sorted(set(y)):
        orig_count = original_counts[cls]
        resamp_count = resampled_counts[cls]
        increase = resamp_count - orig_count
        increase_rate = increase / orig_count if orig_count > 0 else 0
        
        print(f"  ç±»åˆ«{cls}: {orig_count} â†’ {resamp_count} (+{increase}, +{increase_rate:.0%})")
    
    return X_resampled, y_resampled

def convert_to_standard_format(X_resampled, y_resampled, original_data):
    """å°†é‡é‡‡æ ·æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†pklæ ¼å¼"""
    print(f"\nğŸ”„ è½¬æ¢ä¸ºæ ‡å‡†æ•°æ®æ ¼å¼...")
    
    # åˆ›å»ºç±»åˆ«åç§°æ˜ å°„
    class_name_mapping = {}
    for item in original_data:
        class_name_mapping[item['label']] = item['class_name']
    
    print(f"ç±»åˆ«æ˜ å°„: {class_name_mapping}")
    
    # è½¬æ¢é‡é‡‡æ ·æ•°æ®
    converted_data = []
    
    for i, (features, label) in enumerate(zip(X_resampled, y_resampled)):
        # features: [512, 3] - [time, mag, error]ï¼Œä½†åªæœ‰å‰é¢éƒ¨åˆ†æ˜¯æœ‰æ•ˆçš„
        
        # æ‰¾åˆ°æœ‰æ•ˆæ•°æ®çš„ç»“æŸä½ç½®ï¼ˆå‡è®¾æ—¶é—´ä¸º0è¡¨ç¤ºpaddingï¼‰
        valid_mask = features[:, 0] != 0  # æ—¶é—´ä¸ä¸º0çš„ç‚¹æ˜¯æœ‰æ•ˆçš„
        if not np.any(valid_mask):
            valid_mask[0] = True  # è‡³å°‘ä¿ç•™ä¸€ä¸ªç‚¹
        
        times = features[valid_mask, 0].astype(np.float32)
        mags = features[valid_mask, 1].astype(np.float32)
        errors = features[valid_mask, 2].astype(np.float32)
        
        seq_len = len(times)
        
        # åˆ›å»ºmask
        mask = np.ones(seq_len, dtype=bool)
        
        # ä¼°è®¡å‘¨æœŸ
        if seq_len > 1:
            time_span = times.max() - times.min()
            estimated_period = time_span / max(1, seq_len // 10)
        else:
            estimated_period = 1.0
        
        # æ„å»ºæ ‡å‡†æ ¼å¼çš„æ•°æ®é¡¹
        data_item = {
            'time': times,
            'mag': mags,
            'errmag': errors,
            'mask': mask,
            'period': np.float32(estimated_period),
            'label': int(label),
            'class_name': class_name_mapping.get(label, f'Class_{label}'),
            'file_id': f'hybrid_generated_{i}',
            'original_length': seq_len,
            'valid_points': seq_len,
            'coverage': 1.0
        }
        
        converted_data.append(data_item)
    
    print(f"âœ… è½¬æ¢å®Œæˆ: {len(converted_data)}æ ·æœ¬")
    return converted_data

def save_linear_resampled_data(converted_data):
    """ä¿å­˜LINEARé‡é‡‡æ ·æ•°æ®"""
    print(f"\nğŸ’¾ ä¿å­˜LINEARé‡é‡‡æ ·æ•°æ®...")
    
    # ä½¿ç”¨å’ŒTimeGANç›¸åŒçš„å‘½åæ¨¡å¼ä»¥ä¾¿å…¼å®¹
    output_path = '/root/autodl-fs/lnsde-contiformer/data/LINEAR_resample_hybrid.pkl'
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'wb') as f:
        pickle.dump(converted_data, f)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    
    # éªŒè¯ä¿å­˜çš„æ•°æ®
    with open(output_path, 'rb') as f:
        verified_data = pickle.load(f)
    
    print(f"éªŒè¯: åŠ è½½äº†{len(verified_data)}æ ·æœ¬")
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    labels = [item['label'] for item in verified_data]
    class_counts = Counter(labels)
    print(f"æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ: {dict(class_counts)}")
    
    return output_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LINEARæ··åˆé‡é‡‡æ · - ä¸“é—¨ä¼˜åŒ–ç±»åˆ«0å’Œ1")
    print("=" * 60)
    
    # 1. åº”ç”¨æ··åˆé‡é‡‡æ ·
    X_resampled, y_resampled = apply_linear_hybrid_resampling()
    
    # 2. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    original_data = []
    with open('/root/autodl-fs/lnsde-contiformer/data/LINEAR_original.pkl', 'rb') as f:
        original_data = pickle.load(f)
    
    converted_data = convert_to_standard_format(X_resampled, y_resampled, original_data)
    
    # 3. ä¿å­˜æ•°æ®
    output_path = save_linear_resampled_data(converted_data)
    
    print(f"\nğŸ‰ LINEARé‡é‡‡æ ·å®Œæˆ!")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {output_path}")
    print(f"\nğŸ¯ ä¸»è¦æ”¹è¿›:")
    print(f"  â€¢ ç±»åˆ«0 (Beta_Persei): 291 â†’ 400æ ·æœ¬ (+37%)")
    print(f"  â€¢ ç±»åˆ«1 (Delta_Scuti): 70 â†’ 300æ ·æœ¬ (+328%)")
    print(f"  â€¢ ä½¿ç”¨æ··åˆé‡é‡‡æ ·æŠ€æœ¯æå‡ç±»åˆ«åŒºåˆ†æ€§")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"python main.py --dataset 2 --use_resampling --resampled_data_path {output_path}")

if __name__ == "__main__":
    main()