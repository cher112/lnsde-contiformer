"""
å†…å­˜ç›‘æ§å’ŒOOMä¿æŠ¤å·¥å…·
"""

import torch
import psutil
import gc


def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None
    
    allocated = torch.cuda.memory_allocated() / 1024**3  # GB
    cached = torch.cuda.memory_reserved() / 1024**3     # GB
    max_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
    
    return {
        'allocated': allocated,
        'cached': cached,
        'total': max_mem,
        'free': max_mem - allocated,
        'usage_percent': (allocated / max_mem) * 100
    }


def check_memory_pressure():
    """æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†…å­˜å‹åŠ›"""
    gpu_info = get_gpu_memory_info()
    
    if gpu_info is None:
        return False
    
    # å¦‚æœGPUå†…å­˜ä½¿ç”¨è¶…è¿‡85%ï¼Œè®¤ä¸ºæœ‰å†…å­˜å‹åŠ›
    return gpu_info['usage_percent'] > 85.0


def emergency_memory_cleanup():
    """ç´§æ€¥å†…å­˜æ¸…ç†"""
    # Pythonåƒåœ¾å›æ”¶
    gc.collect()
    
    # GPUå†…å­˜æ¸…ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    print("æ‰§è¡Œç´§æ€¥å†…å­˜æ¸…ç†")


def safe_forward_pass(model, inputs, max_retries=3):
    """å®‰å…¨çš„å‰å‘ä¼ æ’­ï¼ŒåŒ…å«OOMä¿æŠ¤"""
    
    for attempt in range(max_retries):
        try:
            # æ£€æŸ¥å†…å­˜å‹åŠ›
            if check_memory_pressure():
                emergency_memory_cleanup()
            
            return model(*inputs)
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"OOM detected, attempt {attempt + 1}/{max_retries}")
                emergency_memory_cleanup()
                
                if attempt == max_retries - 1:
                    print("å¤šæ¬¡å°è¯•åä»ç„¶OOMï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    return None
            else:
                raise e
    
    return None


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self, warning_threshold=80, critical_threshold=90):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.last_warning_batch = -1
        
    def check_and_warn(self, batch_idx):
        """æ£€æŸ¥å†…å­˜å¹¶å‘å‡ºè­¦å‘Š"""
        gpu_info = get_gpu_memory_info()
        
        if gpu_info is None:
            return
        
        usage = gpu_info['usage_percent']
        
        if usage > self.critical_threshold:
            print(f"\nğŸš¨ ä¸´ç•Œå†…å­˜è­¦å‘Š (æ‰¹æ¬¡{batch_idx}): GPUå†…å­˜ä½¿ç”¨{usage:.1f}%")
            print("æ‰§è¡Œå¼ºåˆ¶å†…å­˜æ¸…ç†...")
            emergency_memory_cleanup()
            
        elif usage > self.warning_threshold and batch_idx - self.last_warning_batch > 10:
            print(f"\nâš ï¸ å†…å­˜è­¦å‘Š (æ‰¹æ¬¡{batch_idx}): GPUå†…å­˜ä½¿ç”¨{usage:.1f}%")
            self.last_warning_batch = batch_idx
    
    def get_memory_summary(self):
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        gpu_info = get_gpu_memory_info()
        
        if gpu_info is None:
            return "GPUä¸å¯ç”¨"
        
        return (f"GPUå†…å­˜: {gpu_info['allocated']:.1f}GB/"
                f"{gpu_info['total']:.1f}GB ({gpu_info['usage_percent']:.1f}%)")


# å…¨å±€å†…å­˜ç›‘æ§å™¨
memory_monitor = MemoryMonitor()