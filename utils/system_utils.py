"""
ç³»ç»Ÿç›¸å…³å·¥å…·å‡½æ•°
"""

import os
import random
import torch
import numpy as np
import gc
import subprocess

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


def get_gpu_memory_usage():
    """è·å–æ‰€æœ‰GPUçš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆé€šè¿‡nvidia-smiï¼‰"""
    if not torch.cuda.is_available():
        return []
    
    gpu_info = []
    
    try:
        # ä½¿ç”¨nvidia-smiè·å–å®é™…çš„GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.total,memory.used,memory.free', 
                               '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 5:
                        gpu_id = int(parts[0])
                        gpu_name = parts[1]
                        total_mb = float(parts[2])
                        used_mb = float(parts[3])
                        free_mb = float(parts[4])
                        
                        total_gb = total_mb / 1024
                        used_gb = used_mb / 1024
                        usage_percent = (used_gb / total_gb) * 100
                        
                        gpu_info.append({
                            'id': gpu_id,
                            'name': gpu_name,
                            'allocated_gb': used_gb,  # å®é™…å·²ä½¿ç”¨å†…å­˜
                            'total_gb': total_gb,
                            'usage_percent': usage_percent,
                            'available_gb': free_mb / 1024
                        })
        else:
            print(f"nvidia-smiå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
            
    except Exception as e:
        print(f"æ— æ³•é€šè¿‡nvidia-smiè·å–GPUä¿¡æ¯: {e}")
        # å›é€€åˆ°PyTorchæ–¹æ³•
        return get_pytorch_gpu_info()
    
    return gpu_info


def get_pytorch_gpu_info():
    """å›é€€æ–¹æ³•ï¼šä½¿ç”¨PyTorchè·å–GPUä¿¡æ¯"""
    gpu_info = []
    for i in range(torch.cuda.device_count()):
        try:
            allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
            usage_percent = (allocated / total) * 100
            
            gpu_info.append({
                'id': i,
                'name': torch.cuda.get_device_name(i),
                'allocated_gb': allocated,
                'total_gb': total,
                'usage_percent': usage_percent,
                'available_gb': total - allocated
            })
        except Exception as e:
            print(f"æ— æ³•è·å–GPU {i} ä¿¡æ¯: {e}")
    
    return gpu_info


def find_best_gpu(usage_threshold=15.0):
    """
    æ‰¾åˆ°æœ€é€‚åˆçš„GPU
    ä¼˜å…ˆé€‰æ‹©å†…å­˜ä½¿ç”¨ç‡ä½äºé˜ˆå€¼çš„GPUï¼Œé¿å…ä¸å…¶ä»–è¿›ç¨‹ç«äº‰
    
    Args:
        usage_threshold: å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œä½äºæ­¤å€¼è®¤ä¸ºGPUç©ºé—²
    
    Returns:
        æœ€ä½³GPUçš„IDï¼Œ-1è¡¨ç¤ºæ²¡æœ‰åˆé€‚çš„GPU
    """
    gpu_info = get_gpu_memory_usage()
    if not gpu_info:
        return -1
    
    print("=== GPUçŠ¶æ€ï¼ˆåŸºäºnvidia-smiå®é™…ä½¿ç”¨æƒ…å†µï¼‰===")
    
    # é¦–å…ˆå°è¯•æ‰¾åˆ°å®Œå…¨ç©ºé—²çš„GPUï¼ˆä½¿ç”¨ç‡ < usage_thresholdï¼‰
    available_gpus = []
    busy_gpus = []
    
    for gpu in gpu_info:
        status_emoji = ""
        status_text = ""
        
        if gpu['usage_percent'] < usage_threshold:
            status_emoji = "ğŸŸ¢"
            status_text = "ç©ºé—²å¯ç”¨"
            available_gpus.append(gpu)
        elif gpu['usage_percent'] < 50:
            status_emoji = "ğŸŸ¡"
            status_text = "è½»åº¦ä½¿ç”¨"
            busy_gpus.append(gpu)
        elif gpu['usage_percent'] < 80:
            status_emoji = "ğŸŸ "
            status_text = "ä¸­åº¦ä½¿ç”¨"
            busy_gpus.append(gpu)
        else:
            status_emoji = "ğŸ”´"
            status_text = "é«˜è´Ÿè½½"
            busy_gpus.append(gpu)
        
        print(f"GPU {gpu['id']}: {gpu['name']}")
        print(f"  å†…å­˜: {gpu['allocated_gb']:.1f}GB / {gpu['total_gb']:.1f}GB ({gpu['usage_percent']:.1f}%) {status_emoji} {status_text}")
    
    # é€‰æ‹©ç­–ç•¥
    if available_gpus:
        # æœ‰ç©ºé—²GPUï¼Œé€‰æ‹©ä½¿ç”¨ç‡æœ€ä½çš„
        best_gpu = min(available_gpus, key=lambda x: x['usage_percent'])
        print(f"\nâœ… é€‰æ‹©ç©ºé—²GPU {best_gpu['id']}: {best_gpu['name']} (ä½¿ç”¨ç‡: {best_gpu['usage_percent']:.1f}%)")
        return best_gpu['id']
    elif busy_gpus:
        # æ²¡æœ‰å®Œå…¨ç©ºé—²çš„GPUï¼Œé€‰æ‹©ä½¿ç”¨ç‡æœ€ä½çš„å¿™ç¢ŒGPU
        best_gpu = min(busy_gpus, key=lambda x: x['usage_percent'])
        print(f"\nâš ï¸ æ‰€æœ‰GPUéƒ½åœ¨ä½¿ç”¨ä¸­ï¼Œé€‰æ‹©è´Ÿè½½æœ€ä½çš„GPU {best_gpu['id']}: {best_gpu['name']} (ä½¿ç”¨ç‡: {best_gpu['usage_percent']:.1f}%)")
        print("   å»ºè®®ï¼šç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œå¯èƒ½ä¼šä¸å…¶ä»–è¿›ç¨‹ç«äº‰")
        return best_gpu['id']
    else:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„GPU")
        return -1


def get_device(device_arg, gpu_id=-1):
    """è·å–è®¡ç®—è®¾å¤‡ï¼Œæ”¯æŒGPUé€‰æ‹©"""
    if device_arg == 'auto':
        if not torch.cuda.is_available():
            device = torch.device('cpu')
            print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            return device
        
        # æ ¹æ®gpu_idå‚æ•°é€‰æ‹©GPU
        if gpu_id >= 0:
            # æ£€æŸ¥æŒ‡å®šçš„GPUæ˜¯å¦å­˜åœ¨
            if gpu_id >= torch.cuda.device_count():
                print(f"æŒ‡å®šçš„GPU {gpu_id} ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³GPU")
                gpu_id = find_best_gpu()
            else:
                print(f"ä½¿ç”¨æŒ‡å®šçš„GPU {gpu_id}")
        else:
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³GPU
            gpu_id = find_best_gpu()
        
        if gpu_id >= 0:
            device = torch.device(f'cuda:{gpu_id}')
            # è®¾ç½®å½“å‰GPU
            torch.cuda.set_device(gpu_id)
        else:
            device = torch.device('cpu')
            print("æ— å¯ç”¨GPUï¼Œä½¿ç”¨CPU")
    else:
        device = torch.device(device_arg)
        if 'cuda' in device_arg and not torch.cuda.is_available():
            print("CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
            device = torch.device('cpu')
    
    return device