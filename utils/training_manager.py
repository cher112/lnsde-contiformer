"""
è®­ç»ƒç®¡ç†å™¨ - æ•´åˆå®Œæ•´çš„è®­ç»ƒæµç¨‹
"""

import os
import time
import torch
from torch.cuda.amp import GradScaler
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns

from .system_utils import clear_gpu_memory
from .model_utils import save_checkpoint, setup_model_save_paths
from .logging_utils import update_log, print_epoch_summary
from .training_utils import train_epoch, validate_epoch
from .visualization import generate_training_visualizations


class TrainingManager:
    def __init__(self, model, train_loader, val_loader, optimizer, criterion, device, args, dataset_config, scheduler=None):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.args = args
        self.dataset_config = dataset_config
        self.scheduler = scheduler
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = hasattr(args, 'use_amp') and args.use_amp
        self.scaler = GradScaler() if self.use_amp else None
        
        # ä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ - models å­ç›®å½•å·²åœ¨ timestamp_path ä¸­åˆ›å»º
        self.model_save_dir = os.path.join(args.save_dir, "models")
        os.makedirs(self.model_save_dir, exist_ok=True)
        
    def run_training(self, log_path, log_data, best_val_acc=0.0, start_epoch=0):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("=== å¼€å§‹è®­ç»ƒ ===")
        
        if start_epoch > 0:
            print(f"ä»ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ...")
        else:
            print("ä»å¤´å¼€å§‹è®­ç»ƒ...")
        
        for epoch in range(start_epoch, self.args.epochs):
            epoch_start_time = time.time()
            print(f"\nEpoch [{epoch + 1}/{self.args.epochs}]")
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc, train_class_acc, train_metrics = train_epoch(
                self.model, self.train_loader, self.optimizer, self.criterion, 
                self.device, self.args.model_type, self.dataset_config, self.scaler,
                getattr(self.args, 'gradient_accumulation_steps', 1), epoch,
                getattr(self.args, 'gradient_clip', 1.0)  # ä¼ é€’æ¢¯åº¦è£å‰ªå‚æ•°
            )
            
            # éªŒè¯é˜¶æ®µ - æ€»æ˜¯è®¡ç®—æ··æ·†çŸ©é˜µ
            val_loss, val_acc, val_class_acc, val_metrics = validate_epoch(
                self.model, self.val_loader, self.criterion, 
                self.device, self.args.model_type, self.dataset_config
            )
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_acc > best_val_acc
            
            epoch_time = time.time() - epoch_start_time
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ - ReduceLROnPlateauç°åœ¨ç›‘æ§éªŒè¯å‡†ç¡®ç‡
            if self.scheduler is not None:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_acc)  # ä¼ å…¥éªŒè¯å‡†ç¡®ç‡
                else:
                    self.scheduler.step()
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # æ··æ·†çŸ©é˜µå·²åœ¨éªŒè¯æ—¶è®¡ç®—å®Œæˆï¼Œæ— éœ€é‡å¤è®¡ç®—
            # å¦‚æœéªŒè¯æ—¶æ²¡æœ‰è®¡ç®—æ··æ·†çŸ©é˜µï¼Œåˆ™åœ¨è¿™é‡Œè®¡ç®—
            if val_metrics is None:
                val_metrics = {}
            
            # æ£€æŸ¥éªŒè¯ç»“æœæ˜¯å¦å·²åŒ…å«æ··æ·†çŸ©é˜µ
            if 'confusion_matrix' not in val_metrics or val_metrics['confusion_matrix'] is None:
                try:
                    print("âš¡ è¡¥å……è®¡ç®—æ··æ·†çŸ©é˜µ...")
                    val_cm = self._calculate_confusion_matrix_only()
                    val_metrics['confusion_matrix'] = val_cm
                except Exception as e:
                    print(f"âŒ è®¡ç®—æ··æ·†çŸ©é˜µæ—¶å‡ºé”™: {e}")
            else:
                print("âœ… ä½¿ç”¨éªŒè¯æ—¶å·²è®¡ç®—çš„æ··æ·†çŸ©é˜µ (é›¶é¢å¤–æˆæœ¬)")
            
            # æ‰“å°è½®æ¬¡æ€»ç»“
            print_epoch_summary(
                epoch + 1, self.args.epochs, train_loss, train_acc,
                val_loss, val_acc, val_class_acc if is_best else None, 
                epoch_time, current_lr, train_metrics, val_metrics
            )
            
            # æ›´æ–°æ—¥å¿—
            update_log(
                log_path, log_data, epoch + 1, train_loss, train_acc,
                val_loss, val_acc, val_class_acc, epoch_time, current_lr, 
                train_metrics, val_metrics, is_best
            )
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self._save_checkpoints(epoch, val_loss, val_acc, best_val_acc)
            
            # æ›´æ–°æœ€ä½³éªŒè¯å‡†ç¡®ç‡
            if is_best:
                best_val_acc = val_acc
                print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}%")
            
            # æ¸…ç†GPUå†…å­˜
            clear_gpu_memory()
        
        # ç§»é™¤å¤šä½™è¾“å‡º
        # print(f"\n=== è®­ç»ƒå®Œæˆ ===")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}%")
        
        # ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨
        print("=== ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨ ===")
        try:
            # ä»argsè·å–æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²
            model_type_map = {1: 'langevin', 2: 'linear_noise', 3: 'geometric'}
            model_type_str = model_type_map.get(self.args.model_type, 'unknown')
            
            # ä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ - timestamp_dir å°±æ˜¯ args.save_dir
            timestamp_dir = self.args.save_dir
            
            generated_files = generate_training_visualizations(
                log_path, self.args.dataset_name, model_type_str, timestamp_dir
            )
            
            if generated_files:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªå¯è§†åŒ–å›¾è¡¨")
            else:
                print("âš ï¸  å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
        
        # ç”Ÿæˆå®Œæ•´æ•°æ®é›†çš„æ··æ·†çŸ©é˜µ
        print("=== ç”Ÿæˆæ··æ·†çŸ©é˜µ ===")
        try:
            self._generate_confusion_matrix()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ··æ·†çŸ©é˜µæ—¶å‡ºé”™: {e}")
        
        return best_val_acc
    
    def _save_checkpoints(self, epoch, val_loss, val_acc, best_val_acc):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ - ä½¿ç”¨è¯¦ç»†å‚æ•°å‘½å"""
        # ç”Ÿæˆè¯¦ç»†çš„æ¨¡å‹åç§°
        model_name_parts = [
            self.args.dataset_name,
            self.args.model_type,
            f"sde{'1' if self.args.use_sde else '0'}",
            f"cf{'1' if self.args.use_contiformer else '0'}", 
            f"cga{'1' if self.args.use_cga else '0'}",
            f"hc{self.args.hidden_channels}",
            f"cd{self.args.contiformer_dim}",
            f"lr{self.args.learning_rate:.0e}",
            f"bs{self.args.batch_size}"
        ]
        base_model_name = "_".join(model_name_parts)
        
        # å®šæœŸä¿å­˜epochæ¨¡å‹
        if (epoch + 1) % self.args.save_interval == 0:
            epoch_save_path = os.path.join(
                self.model_save_dir,
                f"{base_model_name}_epoch{epoch + 1}.pth"
            )
            save_checkpoint(self.model, self.optimizer, epoch, val_loss, val_acc, epoch_save_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_save_path = os.path.join(
                self.model_save_dir,
                f"{base_model_name}_best.pth"
            )
            
            # æ·»åŠ æ¨¡å‹å‚æ•°åˆ°æ£€æŸ¥ç‚¹
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'loss': val_loss,
                'accuracy': val_acc,
                'best_val_accuracy': val_acc,
                'model_params': {
                    'hidden_channels': self.args.hidden_channels,
                    'contiformer_dim': self.args.contiformer_dim,
                    'n_heads': self.args.n_heads,
                    'n_layers': self.args.n_layers,
                    'sde_method': self.args.sde_method,
                    'dt': self.args.dt,
                    'rtol': self.args.rtol,
                    'atol': self.args.atol,
                    'use_sde': self.args.use_sde,
                    'use_contiformer': self.args.use_contiformer,
                    'use_cga': self.args.use_cga
                },
                'training_params': {
                    'learning_rate': self.args.learning_rate,
                    'batch_size': self.args.batch_size,
                    'epochs': self.args.epochs,
                    'dataset': self.args.dataset_name
                }
            }
            
            torch.save(checkpoint, best_save_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {best_save_path}")

    def _calculate_confusion_matrix_only(self):
        """åªè®¡ç®—æ··æ·†çŸ©é˜µï¼Œä¸ç”Ÿæˆå›¾ç‰‡ - ç”¨äºæ¯ä¸ªepochçš„æ—¥å¿—è®°å½•"""
        self.model.eval()
        all_predictions = []
        all_labels = []
        
        # åªåœ¨éªŒè¯é›†ä¸Šè®¡ç®—
        with torch.no_grad():
            for batch in self.val_loader:
                if isinstance(batch, dict):
                    features = batch["features"].to(self.device)
                    labels = batch["labels"].to(self.device)
                    mask = batch.get("mask", None)
                    if mask is not None:
                        mask = mask.to(self.device)
                else:
                    features, labels = batch
                    features = features.to(self.device)
                    labels = labels.to(self.device)
                    mask = None
                
                # å‰å‘ä¼ æ’­
                if self.args.model_type in ["linear_noise", "langevin", "geometric"]:
                    outputs = self.model(features, mask)
                else:
                    outputs = self.model(features)
                
                predictions = torch.argmax(outputs, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¡ç®—æ··æ·†çŸ©é˜µ
        y_true = np.array(all_labels)
        y_pred = np.array(all_predictions)
        
        # è·å–ç±»åˆ«æ•°é‡
        class_names = self._get_class_names()
        num_classes = len(class_names)
        cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))
        
        return cm
        
        
    def _generate_confusion_matrix(self):
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†çš„æ··æ·†çŸ©é˜µ"""
        print("æ­£åœ¨ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        all_predictions = []
        all_labels = []
        
        # åœ¨å®Œæ•´æ•°æ®é›†ï¼ˆè®­ç»ƒ+éªŒè¯ï¼‰ä¸Šè¯„ä¼°
        datasets = [("Train", self.train_loader), ("Val", self.val_loader)]
        
        with torch.no_grad():
            for dataset_name, dataloader in datasets:
                for batch in dataloader:
                    if isinstance(batch, dict):
                        features = batch["features"].to(self.device)
                        labels = batch["labels"].to(self.device)
                        mask = batch.get("mask", None)
                        if mask is not None:
                            mask = mask.to(self.device)
                    else:
                        features, labels = batch
                        features = features.to(self.device)
                        labels = labels.to(self.device)
                        mask = None
                    
                    # å‰å‘ä¼ æ’­
                    if self.args.model_type in ["linear_noise", "langevin", "geometric"]:
                        outputs = self.model(features, mask)
                    else:
                        outputs = self.model(features)
                    
                    predictions = torch.argmax(outputs, dim=1)
                    all_predictions.extend(predictions.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        y_true = np.array(all_labels)
        y_pred = np.array(all_predictions)
        
        # è·å–ç±»åˆ«åç§°
        class_names = self._get_class_names()
        
        # ç”Ÿæˆæ··æ·†çŸ©é˜µ - é€‚é…æ­£ç¡®çš„ç±»åˆ«æ•°é‡
        num_classes = len(class_names)
        cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))
        
        # é…ç½®ä¸­æ–‡å­—ä½“
        self._configure_chinese_font()
        
        # åˆ›å»ºåŒæ··æ·†çŸ©é˜µå¯è§†åŒ– - åŸå§‹æ•°é‡ + å½’ä¸€åŒ–ç™¾åˆ†æ¯”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # å·¦å›¾ï¼šåŸå§‹æ•°é‡æ··æ·†çŸ©é˜µ
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names,
                   ax=ax1, cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
        
        ax1.set_title(f'{self.args.dataset_name} åŸå§‹æ··æ·†çŸ©é˜µ\n'
                     f'æ¨¡å‹: {self.args.model_type} | æ€»å‡†ç¡®ç‡: {np.trace(cm)/np.sum(cm):.3f}', 
                     fontsize=14, fontweight='bold')
        ax1.set_xlabel('é¢„æµ‹ç±»åˆ«')
        ax1.set_ylabel('çœŸå®ç±»åˆ«')
        
        # å³å›¾ï¼šå½’ä¸€åŒ–ç™¾åˆ†æ¯”æ··æ·†çŸ©é˜µ
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        cm_percent = np.nan_to_num(cm_percent, nan=0.0)  # å¤„ç†é™¤é›¶æƒ…å†µ
        
        sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='RdYlBu_r',
                   xticklabels=class_names, yticklabels=class_names, 
                   ax=ax2, cbar_kws={'label': 'é¢„æµ‹ç™¾åˆ†æ¯” (%)'})
        
        ax2.set_title(f'{self.args.dataset_name} å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ\n'
                     f'æŒ‰è¡Œå½’ä¸€åŒ– (å¬å›ç‡è§†è§’)', 
                     fontsize=14, fontweight='bold')
        ax2.set_xlabel('é¢„æµ‹ç±»åˆ«')
        ax2.set_ylabel('çœŸå®ç±»åˆ«')
        
        # è°ƒæ•´å­å›¾é—´è·
        plt.tight_layout()
        
        # ä¿å­˜åˆ°picsç›®å½•
        pics_dir = f"/root/autodl-tmp/lnsde-contiformer/results/pics/{self.args.dataset_name}"
        os.makedirs(pics_dir, exist_ok=True)
        
        model_name_parts = []
        if self.args.use_sde:
            model_name_parts.append("SDE")
        if self.args.use_contiformer:
            model_name_parts.append("ContiFormer")
        if self.args.use_cga:
            model_name_parts.append("CGA")
        model_name = "_".join(model_name_parts) if model_name_parts else "Base"
        
        confusion_matrix_path = os.path.join(pics_dir, f"{self.args.dataset_name.lower()}_dual_confusion_matrix_{model_name.lower()}.png")
        plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {confusion_matrix_path}")
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
        report_path = os.path.join(pics_dir, f"{self.args.dataset_name.lower()}_classification_report_{model_name.lower()}.txt")
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"{self.args.dataset_name} åˆ†ç±»æŠ¥å‘Š\n")
            f.write(f"æ¨¡å‹: {model_name}\n")
            f.write(f"SDE: {'On' if self.args.use_sde else 'Off'}\n")
            f.write(f"ContiFormer: {'On' if self.args.use_contiformer else 'Off'}\n")
            f.write(f"CGA: {'On' if self.args.use_cga else 'Off'}\n\n")
            f.write(classification_report(y_true, y_pred, target_names=class_names))
            
        print(f"âœ… åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        accuracy = accuracy_score(y_true, y_pred)
        print(f"æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"åŠ æƒF1åˆ†æ•°: {report['weighted avg']['f1-score']:.4f}")
        print(f"å®å¹³å‡F1åˆ†æ•°: {report['macro avg']['f1-score']:.4f}")
        
        # è¿”å›æ··æ·†çŸ©é˜µä¾›æ—¥å¿—è®°å½•ä½¿ç”¨
        return cm
    
    def _get_class_names(self):
        """è·å–ç±»åˆ«åç§° - æ ¹æ®æ•°æ®é›†é€‚é…5-5-7åˆ†ç±»"""
        if self.args.dataset_name == "ASAS":
            # ASAS: 5ä¸ªç±»åˆ«
            return ["Beta_Persei", "Delta_Scuti", "RR_Lyrae_FM", "RR_Lyrae_FO", "W_Ursae_Maj"]
        elif self.args.dataset_name == "LINEAR": 
            # LINEAR: 5ä¸ªç±»åˆ«
            return ["Beta_Persei", "Delta_Scuti", "RR_Lyrae_FM", "RR_Lyrae_FO", "W_Ursae_Maj"]
        elif self.args.dataset_name == "MACHO":
            # MACHO: 7ä¸ªç±»åˆ«
            return ["Be", "CEPH", "EB", "LPV", "MOA", "QSO", "RRL"]
        else:
            # é»˜è®¤ç±»åˆ«åç§°
            num_classes = getattr(self.model, 'num_classes', 7)
            return [f"Class_{i}" for i in range(num_classes)]
    
    def _configure_chinese_font(self):
        """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º - è§£å†³Font 'default'é—®é¢˜"""
        import matplotlib.font_manager as fm
        
        # æ·»åŠ å­—ä½“åˆ°matplotlibç®¡ç†å™¨
        try:
            fm.fontManager.addfont("/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc")
            fm.fontManager.addfont("/usr/share/fonts/truetype/wqy/wqy-microhei.ttc")
        except:
            pass
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“ä¼˜å…ˆçº§åˆ—è¡¨
        plt.rcParams["font.sans-serif"] = ["WenQuanYi Zen Hei", "WenQuanYi Micro Hei", "SimHei", "DejaVu Sans"]
        plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        
        # æ¸…ç†matplotlibç¼“å­˜å¹¶åˆ·æ–°å­—ä½“
        try:
            # æ¸…ç†matplotlibç¼“å­˜
            import shutil
            cache_dir = fm.get_cachedir()
            if os.path.exists(cache_dir):
                shutil.rmtree(cache_dir)
            fm._rebuild()
        except:
            # å¤‡é€‰æ–¹æ³•ï¼šé‡æ–°åŠ è½½å­—ä½“ç®¡ç†å™¨
            fm.fontManager.__init__()
        
        print("âœ“ ä¸­æ–‡å­—ä½“é…ç½®æˆåŠŸ: WenQuanYi Zen Hei")
