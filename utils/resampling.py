"""
æ··åˆé‡é‡‡æ ·æ¨¡å— - æ™ºèƒ½å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
åŒ…å«å…ˆè¿›çš„æ—¶é—´åºåˆ—æ„ŸçŸ¥SMOTEï¼ˆå°‘æ•°ç±»è¿‡é‡‡æ ·ï¼‰å’ŒENNï¼ˆå¤šæ•°ç±»æ¬ é‡‡æ ·ï¼‰
ä¸“é—¨é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œé¿å…ç®€å•å¤åˆ¶ç²˜è´´
"""

import torch
import numpy as np
from sklearn.neighbors import NearestNeighbors
from collections import Counter
import pickle
import os
from typing import Tuple, Dict, Optional, List
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from scipy.interpolate import interp1d, UnivariateSpline
from scipy.signal import savgol_filter
from scipy.stats import multivariate_normal
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# é…ç½®ä¸­æ–‡å­—ä½“
def configure_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    try:
        # æ·»åŠ å­—ä½“åˆ°matplotlibç®¡ç†å™¨
        fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc')
        fm.fontManager.addfont('/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')
    except:
        pass
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“ä¼˜å…ˆçº§åˆ—è¡¨
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    return True

# åˆå§‹åŒ–æ—¶é…ç½®å­—ä½“
configure_chinese_font()
from datetime import datetime


def dtw_distance(x, y, window=None):
    """
    è®¡ç®—ä¸¤ä¸ªæ—¶é—´åºåˆ—çš„åŠ¨æ€æ—¶é—´è§„æ•´(DTW)è·ç¦»
    
    Args:
        x: ç¬¬ä¸€ä¸ªæ—¶é—´åºåˆ— (seq_len, n_features)
        y: ç¬¬äºŒä¸ªæ—¶é—´åºåˆ— (seq_len, n_features) 
        window: çº¦æŸçª—å£å¤§å°ï¼ŒNoneè¡¨ç¤ºæ— çº¦æŸ
    
    Returns:
        DTWè·ç¦»
    """
    n, m = len(x), len(y)
    
    # åˆå§‹åŒ–DTWçŸ©é˜µ
    dtw = np.full((n + 1, m + 1), np.inf)
    dtw[0, 0] = 0
    
    # è®¾ç½®çª—å£çº¦æŸ
    if window is None:
        window = max(n, m)
    
    for i in range(1, n + 1):
        start = max(1, i - window)
        end = min(m + 1, i + window + 1)
        for j in range(start, end):
            cost = np.linalg.norm(x[i-1] - y[j-1])
            dtw[i, j] = cost + min(dtw[i-1, j],      # æ’å…¥
                                   dtw[i, j-1],      # åˆ é™¤
                                   dtw[i-1, j-1])    # åŒ¹é…
    
    return dtw[n, m]


def functional_alignment(ts1, ts2, n_basis=10):
    """
    ä½¿ç”¨å‡½æ•°ä¸»æˆåˆ†åˆ†æå¯¹é½æ—¶é—´åºåˆ—
    
    Args:
        ts1, ts2: ä¸¤ä¸ªæ—¶é—´åºåˆ—
        n_basis: åŸºå‡½æ•°æ•°é‡
        
    Returns:
        å¯¹é½åçš„æ—¶é—´åºåˆ—
    """
    # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨æ ·æ¡æ’å€¼è¿›è¡Œå¯¹é½
    t1 = np.linspace(0, 1, len(ts1))
    t2 = np.linspace(0, 1, len(ts2))
    
    # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡Œæ’å€¼
    aligned_ts1 = []
    aligned_ts2 = []
    
    common_t = np.linspace(0, 1, max(len(ts1), len(ts2)))
    
    for dim in range(ts1.shape[1]):
        spline1 = UnivariateSpline(t1, ts1[:, dim], s=0.1)
        spline2 = UnivariateSpline(t2, ts2[:, dim], s=0.1)
        
        aligned_ts1.append(spline1(common_t))
        aligned_ts2.append(spline2(common_t))
    
    return np.array(aligned_ts1).T, np.array(aligned_ts2).T


class PhysicsConstrainedTimeGAN:
    """
    ç‰©ç†çº¦æŸçš„TimeGAN - ä¸“é—¨é’ˆå¯¹å…‰å˜æ›²çº¿æ•°æ®
    åœ¨TimeGANåŸºç¡€ä¸Šå¢åŠ å¤©ä½“ç‰©ç†çº¦æŸ
    """
    
    def __init__(self, 
                 seq_len=512, 
                 n_features=3,
                 n_classes=7,
                 hidden_dim=128, 
                 noise_dim=50,
                 physics_weight=0.5,
                 device='cuda',
                 random_state=535411460):
        """
        Args:
            seq_len: åºåˆ—é•¿åº¦
            n_features: ç‰¹å¾æ•° (time, mag, errmag)
            n_classes: ç±»åˆ«æ•°
            hidden_dim: éšè—å±‚ç»´åº¦
            noise_dim: å™ªå£°ç»´åº¦
            physics_weight: ç‰©ç†çº¦æŸæƒé‡
            device: è®¡ç®—è®¾å¤‡
            random_state: éšæœºç§å­
        """
        self.seq_len = seq_len
        self.n_features = n_features
        self.n_classes = n_classes
        self.hidden_dim = hidden_dim
        self.noise_dim = noise_dim
        self.physics_weight = physics_weight
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.random_state = random_state
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(random_state)
        torch.manual_seed(random_state)
        
        # ç½‘ç»œç»„ä»¶
        self.generator = None
        self.discriminator = None
        self.embedder = None
        self.recovery = None
        
        # å­˜å‚¨ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºç‰©ç†çº¦æŸï¼‰
        self.class_stats = {}
    
    def _build_embedder(self):
        """æ„å»ºåµŒå…¥ç½‘ç»œ"""
        return torch.nn.Sequential(
            torch.nn.Linear(self.n_features, self.hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(self.hidden_dim, self.hidden_dim),
            torch.nn.Sigmoid()
        ).to(self.device)
    
    def _build_recovery(self):
        """æ„å»ºæ¢å¤ç½‘ç»œ"""
        return torch.nn.Sequential(
            torch.nn.Linear(self.hidden_dim, self.hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(self.hidden_dim, self.n_features),
            torch.nn.Sigmoid()
        ).to(self.device)
    
    def _build_generator(self):
        """æ„å»ºç”Ÿæˆå™¨ - ä½¿ç”¨LSTM"""
        class Generator(torch.nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim, n_layers=2):
                super().__init__()
                self.hidden_dim = hidden_dim
                self.n_layers = n_layers
                
                # LSTMå±‚
                self.lstm = torch.nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)
                # è¾“å‡ºå±‚
                self.output_layer = torch.nn.Linear(hidden_dim, output_dim)
                self.activation = torch.nn.Sigmoid()
                
            def forward(self, x):
                # x: (batch_size, seq_len, input_dim)
                lstm_out, _ = self.lstm(x)
                output = self.activation(self.output_layer(lstm_out))
                return output
        
        return Generator(
            input_dim=self.noise_dim + self.n_classes,  # noise + class condition
            hidden_dim=self.hidden_dim,
            output_dim=self.hidden_dim,
            n_layers=2
        ).to(self.device)
    
    def _build_discriminator(self):
        """æ„å»ºåˆ¤åˆ«å™¨ - ä½¿ç”¨LSTM"""
        class Discriminator(torch.nn.Module):
            def __init__(self, input_dim, hidden_dim, n_layers=2):
                super().__init__()
                self.lstm = torch.nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)
                self.output_layer = torch.nn.Linear(hidden_dim, 1)
                self.activation = torch.nn.Sigmoid()
                
            def forward(self, x):
                # åªä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
                lstm_out, _ = self.lstm(x)
                last_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)
                output = self.activation(self.output_layer(last_output))
                return output
        
        return Discriminator(
            input_dim=self.hidden_dim,
            hidden_dim=self.hidden_dim,
            n_layers=2
        ).to(self.device)
    
    def _calculate_class_statistics(self, X, y, periods):
        """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç‰©ç†ç»Ÿè®¡ç‰¹å¾"""
        stats = {}
        
        unique_classes = torch.unique(y).cpu().numpy()
        
        for cls in unique_classes:
            cls_mask = (y == cls)
            if torch.sum(cls_mask) == 0:
                continue
                
            cls_X = X[cls_mask]  # (n_samples, seq_len, 3)
            cls_periods = periods[cls_mask]
            
            # åªä½¿ç”¨æœ‰æ•ˆæ•°æ®ç‚¹ (time >= 0)
            valid_mask = cls_X[:, :, 0] >= 0  # timeç»´åº¦
            
            cls_stats = {
                'periods': cls_periods[cls_periods > 0],
                'magnitudes': [],
                'amplitudes': [],
                'mean_errors': []
            }
            
            for i in range(len(cls_X)):
                valid_indices = valid_mask[i]
                if torch.sum(valid_indices) > 0:
                    valid_mags = cls_X[i, valid_indices, 1]  # magç»´åº¦
                    valid_errs = cls_X[i, valid_indices, 2]  # errmagç»´åº¦
                    
                    cls_stats['magnitudes'].append(torch.mean(valid_mags))
                    cls_stats['amplitudes'].append(torch.max(valid_mags) - torch.min(valid_mags))
                    cls_stats['mean_errors'].append(torch.mean(valid_errs))
            
            # è½¬æ¢ä¸ºtensorå¹¶è®¡ç®—ç»Ÿè®¡é‡
            for key in ['magnitudes', 'amplitudes', 'mean_errors']:
                if cls_stats[key]:
                    tensor_data = torch.stack(cls_stats[key])
                    cls_stats[key] = {
                        'mean': torch.mean(tensor_data),
                        'std': torch.std(tensor_data),
                        'min': torch.min(tensor_data), 
                        'max': torch.max(tensor_data)
                    }
                else:
                    cls_stats[key] = {'mean': torch.tensor(0.0), 'std': torch.tensor(1.0),
                                    'min': torch.tensor(0.0), 'max': torch.tensor(1.0)}
            
            # å¤„ç†å‘¨æœŸç»Ÿè®¡
            if len(cls_stats['periods']) > 0:
                cls_stats['periods'] = {
                    'mean': torch.mean(cls_stats['periods']),
                    'std': torch.std(cls_stats['periods']),
                    'min': torch.min(cls_stats['periods']),
                    'max': torch.max(cls_stats['periods'])
                }
            else:
                cls_stats['periods'] = {'mean': torch.tensor(1.0), 'std': torch.tensor(0.1),
                                      'min': torch.tensor(0.1), 'max': torch.tensor(10.0)}
                
            stats[int(cls)] = cls_stats
            
        return stats
    
    def _physics_constraint_loss(self, generated_X, class_labels, periods):
        """è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±"""
        if not self.class_stats:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        batch_size = generated_X.size(0)
        
        for i in range(batch_size):
            seq = generated_X[i]  # (seq_len, 3)
            cls = class_labels[i].item()
            period = periods[i].item()
            
            if cls in self.class_stats:
                # 1. å‘¨æœŸæ€§çº¦æŸ (æƒé‡: 1.0)
                periodicity_loss = self._periodicity_constraint(seq, period)
                
                # 2. æ˜Ÿç­‰èŒƒå›´çº¦æŸ (æƒé‡: 0.8)
                magnitude_loss = self._magnitude_range_constraint(seq, cls)
                
                # 3. è¯¯å·®-æ˜Ÿç­‰ç›¸å…³æ€§çº¦æŸ (æƒé‡: 0.5) 
                error_correlation_loss = self._error_magnitude_correlation(seq)
                
                # 4. å…‰å˜æ›²çº¿å¹³æ»‘æ€§çº¦æŸ (æƒé‡: 0.3)
                smoothness_loss = self._smoothness_constraint(seq)
                
                # åŠ æƒæ±‚å’Œ
                sample_loss = (1.0 * periodicity_loss + 
                             0.8 * magnitude_loss + 
                             0.5 * error_correlation_loss +
                             0.3 * smoothness_loss)
                
                total_loss += sample_loss
        
        return total_loss / batch_size
    
    def _periodicity_constraint(self, sequence, period):
        """å‘¨æœŸæ€§çº¦æŸï¼šç”Ÿæˆçš„å…‰å˜æ›²çº¿åº”è¯¥å…·æœ‰åˆç†çš„å‘¨æœŸæ€§"""
        times = sequence[:, 0]  # æ—¶é—´
        mags = sequence[:, 1]   # æ˜Ÿç­‰
        
        if period <= 0:
            return torch.tensor(0.0, device=self.device)
        
        # æ‰¾åˆ°æœ‰æ•ˆæ—¶é—´ç‚¹
        valid_mask = times >= 0
        valid_times = times[valid_mask]
        valid_mags = mags[valid_mask]
        
        if len(valid_times) < 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
            return torch.tensor(0.0, device=self.device)
        
        # è®¡ç®—ç›¸ä½
        phases = (valid_times % period) / period
        
        # æŒ‰ç›¸ä½æ’åº
        sorted_indices = torch.argsort(phases)
        sorted_phases = phases[sorted_indices]
        sorted_mags = valid_mags[sorted_indices]
        
        # è®¡ç®—ç›¸ä½æ¢¯åº¦çš„ä¸€è‡´æ€§
        phase_diffs = torch.diff(sorted_phases)
        mag_diffs = torch.diff(sorted_mags)
        
        # æœŸæœ›ï¼šç›¸é‚»ç›¸ä½ç‚¹çš„æ˜Ÿç­‰å˜åŒ–åº”è¯¥å¹³æ»‘
        smoothness_loss = torch.mean(torch.abs(mag_diffs) / (phase_diffs + 1e-6))
        
        return torch.clamp(smoothness_loss, 0, 10.0)  # é™åˆ¶æŸå¤±èŒƒå›´
    
    def _magnitude_range_constraint(self, sequence, class_label):
        """æ˜Ÿç­‰èŒƒå›´çº¦æŸï¼šé™åˆ¶æ˜Ÿç­‰å˜åŒ–å¹…åº¦åœ¨åˆç†èŒƒå›´å†…"""
        mags = sequence[:, 1]
        valid_mask = sequence[:, 0] >= 0
        valid_mags = mags[valid_mask]
        
        if len(valid_mags) == 0:
            return torch.tensor(0.0, device=self.device)
        
        amplitude = torch.max(valid_mags) - torch.min(valid_mags)
        
        if class_label in self.class_stats:
            expected_amp = self.class_stats[class_label]['amplitudes']['mean']
            amp_std = self.class_stats[class_label]['amplitudes']['std']
            
            # è®¡ç®—åç¦»ç¨‹åº¦
            deviation = torch.abs(amplitude - expected_amp)
            normalized_deviation = deviation / (amp_std + 1e-6)
            
            # å¦‚æœåç¦»è¶…è¿‡3ä¸ªæ ‡å‡†å·®ï¼Œæ–½åŠ æƒ©ç½š
            range_loss = torch.relu(normalized_deviation - 3.0)
        else:
            # é»˜è®¤çº¦æŸï¼šå˜å¹…åº”åœ¨0.01-2.0ä¹‹é—´
            range_loss = torch.relu(0.01 - amplitude) + torch.relu(amplitude - 2.0)
        
        return range_loss
    
    def _error_magnitude_correlation(self, sequence):
        """è¯¯å·®-æ˜Ÿç­‰ç›¸å…³æ€§çº¦æŸï¼šæš—æ˜Ÿé€šå¸¸æœ‰æ›´å¤§çš„æµ‹é‡è¯¯å·®"""
        mags = sequence[:, 1]
        errors = sequence[:, 2]
        valid_mask = sequence[:, 0] >= 0
        
        valid_mags = mags[valid_mask]
        valid_errors = errors[valid_mask]
        
        if len(valid_mags) < 5:
            return torch.tensor(0.0, device=self.device)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        mag_centered = valid_mags - torch.mean(valid_mags)
        err_centered = valid_errors - torch.mean(valid_errors)
        
        numerator = torch.sum(mag_centered * err_centered)
        denominator = torch.sqrt(torch.sum(mag_centered**2) * torch.sum(err_centered**2))
        
        if denominator > 1e-6:
            correlation = numerator / denominator
            # æœŸæœ›æ­£ç›¸å…³ï¼ˆæš—æ˜Ÿè¯¯å·®å¤§ï¼‰
            correlation_loss = torch.relu(0.0 - correlation)  # æƒ©ç½šè´Ÿç›¸å…³
        else:
            correlation_loss = torch.tensor(0.0, device=self.device)
        
        return correlation_loss
    
    def _smoothness_constraint(self, sequence):
        """å¹³æ»‘æ€§çº¦æŸï¼šå…‰å˜æ›²çº¿åº”è¯¥ç›¸å¯¹å¹³æ»‘ï¼Œé¿å…çªå˜"""
        mags = sequence[:, 1]
        valid_mask = sequence[:, 0] >= 0
        valid_mags = mags[valid_mask]
        
        if len(valid_mags) < 3:
            return torch.tensor(0.0, device=self.device)
        
        # è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼ˆæ›²ç‡ï¼‰
        first_diff = torch.diff(valid_mags)
        second_diff = torch.diff(first_diff)
        
        # å¹³æ»‘æ€§æŸå¤±ï¼šäºŒé˜¶å¯¼æ•°çš„å‡å€¼
        smoothness_loss = torch.mean(torch.abs(second_diff))
        
        return smoothness_loss


class AdvancedTimeSeriesSMOTE:
    """
    å…ˆè¿›çš„æ—¶é—´åºåˆ—SMOTE - çœŸæ­£çš„æ—¶é—´åºåˆ—æ„ŸçŸ¥è¿‡é‡‡æ ·
    ä½¿ç”¨DTWç›¸ä¼¼åº¦ã€å‡½æ•°æ’å€¼ã€å½¢çŠ¶ä¿æŒå’Œæ™ºèƒ½å™ªå£°æ³¨å…¥
    """
    
    def __init__(self, 
                 k_neighbors=5,
                 sampling_strategy='auto',
                 synthesis_mode='hybrid',  # 'interpolation', 'warping', 'hybrid', 'physics_timegan'
                 dtw_window=None,
                 noise_level=0.05,
                 use_functional_alignment=True,
                 physics_weight=0.5,  # ç‰©ç†çº¦æŸæƒé‡
                 random_state=535411460):
        """
        Args:
            k_neighbors: ç”¨äºSMOTEçš„é‚»å±…æ•°
            sampling_strategy: é‡‡æ ·ç­–ç•¥
            synthesis_mode: åˆæˆæ¨¡å¼
                - 'interpolation': åŸºäºå‡½æ•°æ’å€¼
                - 'warping': åŸºäºæ—¶é—´æ‰­æ›²
                - 'hybrid': æ··åˆæ¨¡å¼
                - 'physics_timegan': ç‰©ç†çº¦æŸTimeGAN
            dtw_window: DTWçª—å£å¤§å°
            noise_level: å™ªå£°æ°´å¹³
            use_functional_alignment: æ˜¯å¦ä½¿ç”¨å‡½æ•°å¯¹é½
            physics_weight: ç‰©ç†çº¦æŸæƒé‡ï¼ˆä»…å¯¹physics_timeganæœ‰æ•ˆï¼‰
            random_state: éšæœºç§å­
        """
        self.k_neighbors = k_neighbors
        self.sampling_strategy = sampling_strategy
        self.synthesis_mode = synthesis_mode
        self.dtw_window = dtw_window
        self.noise_level = noise_level
        self.use_functional_alignment = use_functional_alignment
        self.physics_weight = physics_weight
        self.random_state = random_state
        np.random.seed(random_state)
        
        # å¦‚æœä½¿ç”¨ç‰©ç†çº¦æŸTimeGANï¼Œåˆå§‹åŒ–ç›¸å…³ç»„ä»¶
        if synthesis_mode == 'physics_timegan':
            self.physics_timegan = None
    
    def _train_physics_timegan(self, X_cls_np, y_cls_np, periods_cls, epochs=50):  # å‡å°‘è®­ç»ƒè½®æ•°
        """è®­ç»ƒç‰©ç†çº¦æŸTimeGAN"""
        print(f"ğŸ§¬ è®­ç»ƒç‰©ç†çº¦æŸTimeGAN - ç±»åˆ«æ ·æœ¬: {len(X_cls_np)}")
        
        # è½¬æ¢ä¸ºtensor
        X_tensor = torch.FloatTensor(X_cls_np)
        y_tensor = torch.LongTensor(y_cls_np)
        periods_tensor = torch.FloatTensor(periods_cls)
        
        seq_len, n_features = X_tensor.shape[1], X_tensor.shape[2]
        # ä½¿ç”¨å®é™…ç±»åˆ«æ•°ï¼Œè€Œä¸æ˜¯å…¨å±€ç±»åˆ«æ•°
        unique_classes = np.unique(y_cls_np)
        n_classes = len(unique_classes)
        
        print(f"æ•°æ®ä¿¡æ¯: seq_len={seq_len}, n_features={n_features}, n_classes={n_classes}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {Counter(y_cls_np)}")
        
        # åˆå§‹åŒ–ç‰©ç†çº¦æŸTimeGAN
        self.physics_timegan = PhysicsConstrainedTimeGAN(
            seq_len=seq_len,
            n_features=n_features,
            n_classes=n_classes,
            hidden_dim=32,  # æ›´å°çš„éšè—å±‚ä»¥åŠ é€Ÿè®­ç»ƒå’Œå‡å°‘å†…å­˜
            noise_dim=16,
            physics_weight=self.physics_weight,
            random_state=self.random_state
        )
        
        # æ„å»ºç½‘ç»œç»„ä»¶
        self.physics_timegan.embedder = self.physics_timegan._build_embedder()
        self.physics_timegan.recovery = self.physics_timegan._build_recovery()
        self.physics_timegan.generator = self.physics_timegan._build_generator()
        self.physics_timegan.discriminator = self.physics_timegan._build_discriminator()
        
        # ç§»åŠ¨åˆ°GPU
        device = self.physics_timegan.device
        X_tensor = X_tensor.to(device)
        y_tensor = y_tensor.to(device)
        periods_tensor = periods_tensor.to(device)
        
        # é‡æ–°æ˜ å°„ç±»åˆ«æ ‡ç­¾åˆ°è¿ç»­çš„ç´¢å¼•
        class_mapping = {old_class: new_idx for new_idx, old_class in enumerate(unique_classes)}
        print(f"ç±»åˆ«æ˜ å°„: {class_mapping}")
        
        # é‡æ–°æ˜ å°„y_tensor
        y_remapped = torch.zeros_like(y_tensor)
        for old_class, new_idx in class_mapping.items():
            mask = (y_tensor == old_class)
            y_remapped[mask] = new_idx
        
        # è®¡ç®—ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨é‡æ–°æ˜ å°„åçš„æ ‡ç­¾ï¼‰
        self.physics_timegan.class_stats = self.physics_timegan._calculate_class_statistics(
            X_tensor, y_remapped, periods_tensor
        )
        
        print(f"è®¡ç®—ç±»åˆ«ç»Ÿè®¡å®Œæˆ: {list(self.physics_timegan.class_stats.keys())}")
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        lr = 5e-4  # é™ä½å­¦ä¹ ç‡
        embedder_optimizer = torch.optim.Adam(self.physics_timegan.embedder.parameters(), lr=lr)
        recovery_optimizer = torch.optim.Adam(self.physics_timegan.recovery.parameters(), lr=lr)
        generator_optimizer = torch.optim.Adam(self.physics_timegan.generator.parameters(), lr=lr)
        discriminator_optimizer = torch.optim.Adam(self.physics_timegan.discriminator.parameters(), lr=lr)
        
        # æŸå¤±å‡½æ•°
        mse_loss = torch.nn.MSELoss()
        bce_loss = torch.nn.BCELoss()
        
        batch_size = min(16, len(X_tensor))  # æ›´å°çš„batch size
        n_batches = max(1, (len(X_tensor) + batch_size - 1) // batch_size)
        
        print(f"å¼€å§‹è®­ç»ƒ - Batch size: {batch_size}, Epochs: {epochs}")
        
        for epoch in range(epochs):
            epoch_e_loss = 0
            epoch_g_loss = 0
            epoch_d_loss = 0
            
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = torch.randperm(len(X_tensor))
            X_shuffled = X_tensor[indices]
            y_shuffled = y_remapped[indices]
            periods_shuffled = periods_tensor[indices]
            
            for batch_idx in range(n_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, len(X_tensor))
                
                X_batch = X_shuffled[start_idx:end_idx]
                y_batch = y_shuffled[start_idx:end_idx]
                periods_batch = periods_shuffled[start_idx:end_idx]
                current_batch_size = len(X_batch)
                
                # ==================
                # 1. è®­ç»ƒEmbedderå’ŒRecovery (é‡æ„æŸå¤±)
                # ==================
                embedder_optimizer.zero_grad()
                recovery_optimizer.zero_grad()
                
                # åµŒå…¥å’Œæ¢å¤
                H = self.physics_timegan.embedder(X_batch)
                X_tilde = self.physics_timegan.recovery(H)
                
                # é‡æ„æŸå¤±
                E_loss = mse_loss(X_tilde, X_batch)
                E_loss.backward()
                embedder_optimizer.step()
                recovery_optimizer.step()
                
                epoch_e_loss += E_loss.item()
                
                # ==================
                # 2. è®­ç»ƒGenerator (ç®€åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘å¤æ‚åº¦)
                # ==================
                generator_optimizer.zero_grad()
                
                # ç”Ÿæˆå™ªå£°å’Œç±»åˆ«æ¡ä»¶
                Z = torch.randn(current_batch_size, seq_len, self.physics_timegan.noise_dim).to(device)
                
                # ç¡®ä¿ç±»åˆ«ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                y_batch_clamped = torch.clamp(y_batch, 0, n_classes - 1)
                y_one_hot = torch.eye(n_classes).to(device)[y_batch_clamped]
                y_one_hot_expanded = y_one_hot.unsqueeze(1).expand(-1, seq_len, -1)
                
                # ç”Ÿæˆå™¨è¾“å…¥ï¼šå™ªå£° + ç±»åˆ«æ¡ä»¶
                gen_input = torch.cat([Z, y_one_hot_expanded], dim=-1)
                
                # ç”ŸæˆåµŒå…¥è¡¨ç¤º
                E_hat = self.physics_timegan.generator(gen_input)
                
                # åˆ¤åˆ«å™¨åˆ¤æ–­ï¼ˆåœ¨åµŒå…¥ç©ºé—´ï¼‰
                Y_fake = self.physics_timegan.discriminator(E_hat)
                
                # æ¢å¤åˆ°åŸå§‹ç©ºé—´ç”¨äºç‰©ç†çº¦æŸ
                X_hat = self.physics_timegan.recovery(E_hat)
                
                # ç”Ÿæˆå™¨å¯¹æŠ—æŸå¤±
                G_loss_adversarial = bce_loss(Y_fake, torch.ones_like(Y_fake))
                
                # ç‰©ç†çº¦æŸæŸå¤±ï¼ˆé™ä½æƒé‡ï¼‰
                try:
                    G_loss_physics = self.physics_timegan._physics_constraint_loss(
                        X_hat, y_batch_clamped, periods_batch
                    )
                    physics_weight = 0.1  # é™ä½ç‰©ç†çº¦æŸæƒé‡
                except:
                    G_loss_physics = torch.tensor(0.0, device=device)
                    physics_weight = 0.0
                
                # æ€»ç”Ÿæˆå™¨æŸå¤±
                G_loss = G_loss_adversarial + physics_weight * G_loss_physics
                G_loss.backward()
                generator_optimizer.step()
                
                epoch_g_loss += G_loss.item()
                
                # ==================
                # 3. è®­ç»ƒDiscriminator
                # ==================
                discriminator_optimizer.zero_grad()
                
                # çœŸå®æ ·æœ¬åˆ¤åˆ«
                H_real = self.physics_timegan.embedder(X_batch).detach()
                Y_real = self.physics_timegan.discriminator(H_real)
                D_loss_real = bce_loss(Y_real, torch.ones_like(Y_real))
                
                # ç”Ÿæˆæ ·æœ¬åˆ¤åˆ«
                E_hat_detached = E_hat.detach()
                Y_fake_d = self.physics_timegan.discriminator(E_hat_detached)
                D_loss_fake = bce_loss(Y_fake_d, torch.zeros_like(Y_fake_d))
                
                # æ€»åˆ¤åˆ«å™¨æŸå¤±
                D_loss = D_loss_real + D_loss_fake
                D_loss.backward()
                discriminator_optimizer.step()
                
                epoch_d_loss += D_loss.item()
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            if (epoch + 1) % 10 == 0:
                avg_e_loss = epoch_e_loss / n_batches
                avg_g_loss = epoch_g_loss / n_batches
                avg_d_loss = epoch_d_loss / n_batches
                
                print(f"Epoch [{epoch+1}/{epochs}] "
                      f"E: {avg_e_loss:.4f}, G: {avg_g_loss:.4f}, D: {avg_d_loss:.4f}")
        
        # å­˜å‚¨ç±»åˆ«æ˜ å°„ç”¨äºç”Ÿæˆ
        self.physics_timegan.class_mapping = class_mapping
        self.physics_timegan.inverse_class_mapping = {v: k for k, v in class_mapping.items()}
        
        print("ğŸ‰ ç‰©ç†çº¦æŸTimeGANè®­ç»ƒå®Œæˆ!")
    
    def _generate_physics_timegan_samples(self, target_class, n_samples, reference_periods):
        """ä½¿ç”¨ç‰©ç†çº¦æŸTimeGANç”Ÿæˆæ ·æœ¬"""
        if self.physics_timegan is None:
            raise ValueError("ç‰©ç†çº¦æŸTimeGANæœªè®­ç»ƒ")
        
        device = self.physics_timegan.device
        seq_len = self.physics_timegan.seq_len
        noise_dim = self.physics_timegan.noise_dim
        n_classes = self.physics_timegan.n_classes
        
        # è·å–ç±»åˆ«æ˜ å°„
        class_mapping = getattr(self.physics_timegan, 'class_mapping', {})
        inverse_class_mapping = getattr(self.physics_timegan, 'inverse_class_mapping', {})
        
        # å°†ç›®æ ‡ç±»åˆ«æ˜ å°„åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„ç´¢å¼•
        if target_class in class_mapping:
            mapped_target_class = class_mapping[target_class]
        else:
            # å¦‚æœæ˜ å°„ä¸å­˜åœ¨ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
            mapped_target_class = 0  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªç±»åˆ«
        
        print(f"ç”Ÿæˆç›®æ ‡ç±»åˆ« {target_class} -> æ˜ å°„ç±»åˆ« {mapped_target_class}")
        
        self.physics_timegan.generator.eval()
        self.physics_timegan.recovery.eval()
        
        generated_samples = []
        
        with torch.no_grad():
            # åˆ†æ‰¹ç”Ÿæˆ
            batch_size = min(10, n_samples)  # æ›´å°çš„æ‰¹é‡ç”Ÿæˆ
            for i in range(0, n_samples, batch_size):
                current_batch_size = min(batch_size, n_samples - i)
                
                # ç”Ÿæˆå™ªå£°
                Z = torch.randn(current_batch_size, seq_len, noise_dim).to(device)
                
                # ç±»åˆ«æ¡ä»¶ - ä½¿ç”¨æ˜ å°„åçš„ç±»åˆ«ç´¢å¼•
                y_batch = torch.full((current_batch_size,), mapped_target_class, dtype=torch.long).to(device)
                
                # ç¡®ä¿ç±»åˆ«ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                y_batch_clamped = torch.clamp(y_batch, 0, n_classes - 1)
                
                try:
                    y_one_hot = torch.eye(n_classes).to(device)[y_batch_clamped]
                    y_one_hot_expanded = y_one_hot.unsqueeze(1).expand(-1, seq_len, -1)
                    
                    # ç”Ÿæˆå™¨è¾“å…¥
                    gen_input = torch.cat([Z, y_one_hot_expanded], dim=-1)
                    
                    # ç”ŸæˆåµŒå…¥è¡¨ç¤º
                    E_hat = self.physics_timegan.generator(gen_input)
                    
                    # æ¢å¤åˆ°åŸå§‹ç©ºé—´
                    X_hat = self.physics_timegan.recovery(E_hat)
                    
                    # è½¬æ¢ä¸ºnumpy
                    batch_samples = X_hat.cpu().numpy()
                    generated_samples.append(batch_samples)
                    
                except Exception as e:
                    print(f"ç”Ÿæˆæ‰¹æ¬¡ {i//batch_size + 1} å¤±è´¥: {str(e)}")
                    # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„åˆæˆæ ·æœ¬ä½œä¸ºåå¤‡
                    if reference_periods is not None and len(reference_periods) > 0:
                        ref_period = reference_periods[0]
                    else:
                        ref_period = 1.0
                        
                    backup_samples = []
                    for j in range(current_batch_size):
                        # åˆ›å»ºç®€å•çš„æ­£å¼¦æ³¢ä½œä¸ºåå¤‡
                        t = np.linspace(0, ref_period, seq_len)
                        mag = 15.0 + 0.5 * np.sin(2 * np.pi * t / ref_period) + np.random.normal(0, 0.1, seq_len)
                        errmag = 0.02 + 0.01 * np.abs(mag - 15.0)
                        
                        # æ·»åŠ æ— æ•ˆåŒºåŸŸ
                        valid_len = int(seq_len * 0.7)
                        t[valid_len:] = -1000
                        mag[valid_len:] = 0
                        errmag[valid_len:] = 0
                        
                        sample = np.stack([t, mag, errmag], axis=1)
                        backup_samples.append(sample)
                    
                    generated_samples.append(np.array(backup_samples))
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        if generated_samples:
            all_samples = np.concatenate(generated_samples, axis=0)
            return all_samples[:n_samples]
        else:
            # å®Œå…¨å¤±è´¥çš„æƒ…å†µï¼Œè¿”å›ç©ºæ•°ç»„
            return np.empty((0, seq_len, 3))
        
    def _synthesize_interpolation(self, ts1, ts2, lambda_val):
        """
        åŸºäºå‡½æ•°æ’å€¼çš„æ—¶é—´åºåˆ—åˆæˆ
        """
        seq_len, n_features = ts1.shape
        synthetic_ts = np.zeros_like(ts1)
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡Œæ ·æ¡æ’å€¼
        t = np.linspace(0, 1, seq_len)
        
        for dim in range(n_features):
            # åˆ›å»ºæ ·æ¡å‡½æ•°
            spline1 = UnivariateSpline(t, ts1[:, dim], s=self.noise_level)
            spline2 = UnivariateSpline(t, ts2[:, dim], s=self.noise_level)
            
            # æ’å€¼åˆæˆ
            synthetic_ts[:, dim] = lambda_val * spline1(t) + (1 - lambda_val) * spline2(t)
            
            # æ·»åŠ å½¢çŠ¶ä¿æŒå™ªå£°
            shape_noise = np.random.normal(0, self.noise_level * np.std(ts1[:, dim]), seq_len)
            synthetic_ts[:, dim] += shape_noise
        
        return synthetic_ts
    
    def _synthesize_warping(self, ts1, ts2, lambda_val):
        """
        åŸºäºæ—¶é—´æ‰­æ›²çš„æ—¶é—´åºåˆ—åˆæˆ
        """
        seq_len, n_features = ts1.shape
        
        # ç”Ÿæˆæ—¶é—´æ‰­æ›²å‡½æ•°
        t_orig = np.linspace(0, 1, seq_len)
        
        # åˆ›å»ºæ›´ç¨³å®šçš„éçº¿æ€§æ—¶é—´æ˜ å°„
        warp_strength = 0.1  # å‡å°æ‰­æ›²å¼ºåº¦
        noise1 = np.random.uniform(-1, 1, seq_len) * 0.1
        noise2 = np.random.uniform(-1, 1, seq_len) * 0.1
        
        warp1 = t_orig + warp_strength * np.sin(2 * np.pi * t_orig) * noise1[0]
        warp2 = t_orig + warp_strength * np.sin(4 * np.pi * t_orig) * noise2[0]
        
        # ç¡®ä¿æ—¶é—´æ˜ å°„å•è°ƒé€’å¢ä¸”æ— é‡å¤
        warp1 = np.clip(warp1, 0, 1)
        warp2 = np.clip(warp2, 0, 1)
        
        # æ·»åŠ å¾®å°éšæœºæ‰°åŠ¨ä»¥é¿å…é‡å¤å€¼
        eps = 1e-8
        for i in range(1, len(warp1)):
            if warp1[i] <= warp1[i-1]:
                warp1[i] = warp1[i-1] + eps
            if warp2[i] <= warp2[i-1]:
                warp2[i] = warp2[i-1] + eps
        
        # é‡æ–°å½’ä¸€åŒ–åˆ°[0,1]
        warp1 = (warp1 - warp1.min()) / (warp1.max() - warp1.min())
        warp2 = (warp2 - warp2.min()) / (warp2.max() - warp2.min())
        
        synthetic_ts = np.zeros_like(ts1)
        
        for dim in range(n_features):
            # ä½¿ç”¨çº¿æ€§æ’å€¼é¿å…cubicæ’å€¼çš„é—®é¢˜
            f1 = interp1d(warp1, ts1[:, dim], kind='linear', bounds_error=False, fill_value='extrapolate')
            f2 = interp1d(warp2, ts2[:, dim], kind='linear', bounds_error=False, fill_value='extrapolate')
            
            # åˆæˆæ–°çš„æ—¶é—´åºåˆ—
            synthetic_ts[:, dim] = lambda_val * f1(t_orig) + (1 - lambda_val) * f2(t_orig)
            
            # æ·»åŠ å°‘é‡å™ªå£°
            noise = np.random.normal(0, self.noise_level * np.std(ts1[:, dim]), seq_len)
            synthetic_ts[:, dim] += noise
        
        return synthetic_ts
    
    def _synthesize_hybrid(self, ts1, ts2, lambda_val):
        """
        æ··åˆæ¨¡å¼ï¼šç»“åˆæ’å€¼å’Œæ‰­æ›²
        """
        # éšæœºé€‰æ‹©åˆæˆç­–ç•¥
        if np.random.random() < 0.5:
            return self._synthesize_interpolation(ts1, ts2, lambda_val)
        else:
            return self._synthesize_warping(ts1, ts2, lambda_val)
    
    def _find_neighbors_dtw(self, X_cls_flat, X_cls_ts, k):
        """
        ä½¿ç”¨DTWè·ç¦»æ‰¾åˆ°æœ€è¿‘é‚»
        """
        n_samples, seq_len, n_features = X_cls_ts.shape
        
        # å¦‚æœæ ·æœ¬æ•°å¤ªå°‘ï¼Œä½¿ç”¨æ¬§æ°è·ç¦»
        if n_samples < 50:
            nn = NearestNeighbors(n_neighbors=k+1, metric='euclidean')
            nn.fit(X_cls_flat)
            return nn
        
        # è®¡ç®—DTWè·ç¦»çŸ©é˜µï¼ˆé‡‡æ ·éƒ¨åˆ†æ ·æœ¬ä»¥èŠ‚çº¦æ—¶é—´ï¼‰
        sample_size = min(100, n_samples)
        sample_indices = np.random.choice(n_samples, sample_size, replace=False)
        
        dtw_matrix = np.zeros((sample_size, sample_size))
        for i, idx1 in enumerate(sample_indices):
            for j, idx2 in enumerate(sample_indices):
                if i != j:
                    dtw_matrix[i, j] = dtw_distance(
                        X_cls_ts[idx1], X_cls_ts[idx2], 
                        window=self.dtw_window
                    )
        
        # ä½¿ç”¨DTWè·ç¦»åˆ›å»ºé‚»å±…æŸ¥æ‰¾å™¨
        class DTWNeighbors:
            def __init__(self, distance_matrix, indices):
                self.distance_matrix = distance_matrix
                self.indices = indices
                
            def kneighbors(self, query_idx, return_distance=False):
                if isinstance(query_idx, list):
                    query_idx = query_idx[0]
                
                # åœ¨é‡‡æ ·ç´¢å¼•ä¸­æ‰¾åˆ°æŸ¥è¯¢ç‚¹
                if query_idx in self.indices:
                    sample_idx = np.where(self.indices == query_idx)[0][0]
                    distances = self.distance_matrix[sample_idx]
                    neighbor_indices = np.argsort(distances)[1:k+1]
                    return [self.indices[neighbor_indices]]
                else:
                    # å¦‚æœä¸åœ¨é‡‡æ ·ä¸­ï¼Œéšæœºé€‰æ‹©é‚»å±…
                    available_indices = np.setdiff1d(np.arange(len(self.indices)), [query_idx])
                    neighbors = np.random.choice(available_indices, min(k, len(available_indices)), replace=False)
                    return [neighbors]
        
        return DTWNeighbors(dtw_matrix, sample_indices)
    
    def _synthesize_batch_gpu(self, X_cls_np, indices_pairs, lambdas):
        """
        GPUæ‰¹é‡åˆæˆæ ·æœ¬ - å¤§å¹…åŠ é€Ÿ
        
        Args:
            X_cls_np: ç±»åˆ«æ•°æ® (n_samples, seq_len, n_features)
            indices_pairs: æ ·æœ¬å¯¹ç´¢å¼• (n_synthetic, 2)
            lambdas: æ’å€¼ç³»æ•° (n_synthetic,)
        """
        import torch
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è½¬æ¢åˆ°GPU
        X_cls_tensor = torch.tensor(X_cls_np, dtype=torch.float32).to(device)
        indices_pairs = torch.tensor(indices_pairs, dtype=torch.long).to(device)
        lambdas = torch.tensor(lambdas, dtype=torch.float32).to(device)
        
        # æ‰¹é‡è·å–æ ·æœ¬å¯¹
        ts1_batch = X_cls_tensor[indices_pairs[:, 0]]  # (n_synthetic, seq_len, n_features)
        ts2_batch = X_cls_tensor[indices_pairs[:, 1]]
        lambdas = lambdas.unsqueeze(-1).unsqueeze(-1)  # (n_synthetic, 1, 1)
        
        if self.synthesis_mode == 'interpolation' or (self.synthesis_mode == 'hybrid' and torch.rand(1) < 0.5):
            # æ ·æ¡æ’å€¼ - ç®€åŒ–ä¸ºçº¿æ€§æ’å€¼ä»¥æé€Ÿ
            synthetic_batch = lambdas * ts1_batch + (1 - lambdas) * ts2_batch
            
            # æ·»åŠ å°‘é‡å™ªå£° - åŸºäºæ¯ä¸ªæ ·æœ¬çš„å˜åŒ–èŒƒå›´è€Œä¸æ˜¯æ•´ä½“æ ‡å‡†å·®
            ts1_range = torch.max(ts1_batch, dim=1, keepdim=True)[0] - torch.min(ts1_batch, dim=1, keepdim=True)[0]
            ts2_range = torch.max(ts2_batch, dim=1, keepdim=True)[0] - torch.min(ts2_batch, dim=1, keepdim=True)[0]
            avg_range = (ts1_range + ts2_range) / 2
            noise_std = self.noise_level * 0.05 * avg_range  # ä½¿ç”¨5%çš„å¹³å‡å˜åŒ–èŒƒå›´ä½œä¸ºå™ªå£°æ ‡å‡†å·®
            noise = torch.randn_like(synthetic_batch) * noise_std
            synthetic_batch += noise
            
        else:  # warping or hybrid
            # ç®€åŒ–çš„æ—¶é—´æ‰­æ›² - ä½¿ç”¨çº¿æ€§æ’å€¼é¿å…å¤æ‚è®¡ç®—
            synthetic_batch = lambdas * ts1_batch + (1 - lambdas) * ts2_batch
            
            # æ·»åŠ æ—¶åºå™ªå£° - åŸºäºæ ·æœ¬å˜åŒ–èŒƒå›´
            ts1_range = torch.max(ts1_batch, dim=1, keepdim=True)[0] - torch.min(ts1_batch, dim=1, keepdim=True)[0]
            ts2_range = torch.max(ts2_batch, dim=1, keepdim=True)[0] - torch.min(ts2_batch, dim=1, keepdim=True)[0]
            avg_range = (ts1_range + ts2_range) / 2
            time_noise_std = 0.02 * avg_range  # ä½¿ç”¨2%çš„å˜åŒ–èŒƒå›´ä½œä¸ºæ—¶åºå™ªå£°
            time_noise = torch.randn_like(synthetic_batch) * time_noise_std
            synthetic_batch += time_noise
        
        return synthetic_batch.cpu().numpy()
    
    def fit_resample(self, X, y, times=None, masks=None):
        """
        æ‰§è¡Œå…ˆè¿›çš„æ—¶é—´åºåˆ—SMOTEé‡é‡‡æ · - æ”¯æŒç‰©ç†çº¦æŸTimeGAN
        """
        from tqdm import tqdm
        import torch
        
        if self.synthesis_mode == 'physics_timegan':
            print("ğŸ§¬ å¯ç”¨ç‰©ç†çº¦æŸTimeGANè¿‡é‡‡æ ·...")
        else:
            print("ğŸš€ å¯ç”¨GPUåŠ é€Ÿæ··åˆé‡é‡‡æ ·...")
            
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        class_counts = Counter(y.tolist() if torch.is_tensor(y) else y)
        majority_class = max(class_counts, key=class_counts.get)
        majority_count = class_counts[majority_class]
        
        print(f"åŸå§‹ç±»åˆ«åˆ†å¸ƒ: {class_counts}")
        
        # ç¡®å®šæ¯ä¸ªç±»çš„ç›®æ ‡æ ·æœ¬æ•°
        if self.sampling_strategy == 'auto':
            target_counts = {cls: majority_count for cls in class_counts}
        elif isinstance(self.sampling_strategy, float):
            target_counts = {}
            for cls in class_counts:
                if cls == majority_class:
                    target_counts[cls] = class_counts[cls]
                else:
                    target_counts[cls] = int(majority_count * self.sampling_strategy)
        else:
            target_counts = self.sampling_strategy
            
        print(f"ç›®æ ‡ç±»åˆ«åˆ†å¸ƒ: {target_counts}")
        
        # è®¡ç®—æ€»çš„éœ€è¦ç”Ÿæˆçš„æ ·æœ¬æ•°
        total_synthetic_needed = sum(max(0, target_counts[cls] - class_counts[cls]) for cls in class_counts)
        print(f"ğŸ¯ éœ€è¦ç”Ÿæˆ {total_synthetic_needed} ä¸ªåˆæˆæ ·æœ¬")
        
        # å‡†å¤‡è¾“å‡ºåˆ—è¡¨
        X_list, y_list, times_list, masks_list = [], [], [], []
        
        # å¦‚æœä½¿ç”¨ç‰©ç†çº¦æŸTimeGANï¼Œéœ€è¦æå–å‘¨æœŸä¿¡æ¯
        if self.synthesis_mode == 'physics_timegan':
            # å‡è®¾å‘¨æœŸä¿¡æ¯å­˜å‚¨åœ¨æ—¶é—´åºåˆ—çš„æŸä¸ªç‰¹å¾ä¸­ï¼Œæˆ–è€…éœ€è¦ä»å¤–éƒ¨æä¾›
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šåŸºäºæ—¶é—´åºåˆ—é•¿åº¦ä¼°ç®—å‘¨æœŸ
            periods = []
            for i in range(len(X)):
                if times is not None:
                    # åŸºäºæ—¶é—´èŒƒå›´ä¼°ç®—å‘¨æœŸ
                    if torch.is_tensor(times):
                        time_seq = times[i].cpu().numpy()
                    else:
                        time_seq = times[i]
                    
                    # æ‰¾åˆ°æœ‰æ•ˆæ—¶é—´ç‚¹
                    if masks is not None:
                        if torch.is_tensor(masks):
                            mask = masks[i].cpu().numpy()
                        else:
                            mask = masks[i]
                        valid_times = time_seq[mask]
                    else:
                        valid_times = time_seq[time_seq > -1000]  # å‡è®¾-1000ä»¥ä¸‹æ˜¯å¡«å……å€¼
                    
                    if len(valid_times) > 5:
                        time_span = np.max(valid_times) - np.min(valid_times)
                        estimated_period = time_span / 3.0  # ç®€å•ä¼°ç®—
                        periods.append(max(0.1, min(100.0, estimated_period)))  # é™åˆ¶èŒƒå›´
                    else:
                        periods.append(1.0)  # é»˜è®¤å‘¨æœŸ
                else:
                    periods.append(1.0)  # é»˜è®¤å‘¨æœŸ
                    
            periods = np.array(periods)
        
        # åˆ›å»ºå…¨å±€è¿›åº¦æ¡
        total_progress = tqdm(total=total_synthetic_needed, desc="ğŸ’« ç”Ÿæˆåˆæˆæ ·æœ¬", unit="æ ·æœ¬")
        
        # å¤„ç†æ¯ä¸ªç±»
        for cls in class_counts:
            # è·å–å½“å‰ç±»çš„ç´¢å¼•
            if torch.is_tensor(y):
                cls_indices = (y == cls).nonzero(as_tuple=True)[0].cpu().numpy()
            else:
                cls_indices = np.where(y == cls)[0]
            
            # å½“å‰ç±»çš„æ•°æ®
            X_cls = X[cls_indices] if torch.is_tensor(X) else X[cls_indices]
            n_samples = len(cls_indices)
            n_synthetic = target_counts[cls] - n_samples
            
            # æ·»åŠ åŸå§‹æ ·æœ¬
            X_list.append(X_cls)
            y_list.extend([cls] * n_samples)
            
            if times is not None:
                times_cls = times[cls_indices]
                times_list.append(times_cls)
                
            if masks is not None:
                masks_cls = masks[cls_indices]
                masks_list.append(masks_cls)
            
            # å¦‚æœéœ€è¦ç”Ÿæˆåˆæˆæ ·æœ¬
            if n_synthetic > 0:
                if self.synthesis_mode == 'physics_timegan':
                    total_progress.set_description(f"ğŸ§¬ ç‰©ç†çº¦æŸTimeGANç”Ÿæˆç±»åˆ«{cls}: {n_synthetic}ä¸ªæ ·æœ¬")
                    
                    # å‡†å¤‡è®­ç»ƒæ•°æ®
                    if torch.is_tensor(X_cls):
                        X_cls_np = X_cls.cpu().numpy()
                    else:
                        X_cls_np = X_cls.copy()
                    
                    y_cls_np = np.full(n_samples, cls)
                    periods_cls = periods[cls_indices] if self.synthesis_mode == 'physics_timegan' else None
                    
                    # è®­ç»ƒç‰©ç†çº¦æŸTimeGAN
                    self._train_physics_timegan(X_cls_np, y_cls_np, periods_cls, epochs=80)
                    
                    # ç”Ÿæˆåˆæˆæ ·æœ¬
                    synthetic_samples = self._generate_physics_timegan_samples(
                        target_class=cls,
                        n_samples=n_synthetic, 
                        reference_periods=periods_cls
                    )
                    
                    X_list.append(synthetic_samples)
                    y_list.extend([cls] * n_synthetic)
                    
                    # å¤„ç†æ—¶é—´æˆ³å’Œæ©ç ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                    if times is not None:
                        # ä»åˆæˆçš„ç‰¹å¾ä¸­æå–æ—¶é—´ç»´åº¦
                        synthetic_times = synthetic_samples[:, :, 0]  # å‡è®¾æ—¶é—´æ˜¯ç¬¬0ç»´ç‰¹å¾
                        times_list.append(synthetic_times)
                    
                    if masks is not None:
                        # ç”Ÿæˆåˆç†çš„æ©ç ï¼šæ—¶é—´å¤§äºæŸä¸ªé˜ˆå€¼çš„ç‚¹è®¤ä¸ºæœ‰æ•ˆ
                        synthetic_masks = synthetic_samples[:, :, 0] > -500  # ç®€å•é˜ˆå€¼
                        masks_list.append(synthetic_masks)
                    
                    total_progress.update(n_synthetic)
                    
                else:
                    # ä½¿ç”¨åŸæœ‰çš„GPUåŠ é€Ÿæ–¹æ³•
                    total_progress.set_description(f"ğŸ’« GPUç”Ÿæˆç±»åˆ«{cls}: {n_synthetic}ä¸ªæ ·æœ¬")
                    
                    # å‡†å¤‡æ•°æ®
                    if torch.is_tensor(X_cls):
                        X_cls_np = X_cls.cpu().numpy()
                    else:
                        X_cls_np = X_cls.copy()
                    
                    # æ‰¹é‡ç”Ÿæˆæ ·æœ¬å¯¹ç´¢å¼•
                    batch_size = min(1000, n_synthetic)
                    synthetic_samples = []
                    synthetic_times = [] if times is not None else None
                    synthetic_masks = [] if masks is not None else None
                    
                    # åˆ†æ‰¹å¤„ç†
                    for batch_start in range(0, n_synthetic, batch_size):
                        batch_end = min(batch_start + batch_size, n_synthetic)
                        current_batch_size = batch_end - batch_start
                        
                        # éšæœºç”Ÿæˆæ ·æœ¬å¯¹
                        indices1 = np.random.randint(0, n_samples, current_batch_size)
                        indices2 = np.random.randint(0, n_samples, current_batch_size)
                        # ç¡®ä¿ä¸æ˜¯åŒä¸€ä¸ªæ ·æœ¬
                        mask = indices1 == indices2
                        indices2[mask] = (indices1[mask] + 1) % n_samples
                        
                        indices_pairs = np.column_stack([indices1, indices2])
                        lambdas = np.random.beta(2, 2, current_batch_size)
                        
                        # GPUæ‰¹é‡ç”Ÿæˆ
                        batch_synthetic = self._synthesize_batch_gpu(X_cls_np, indices_pairs, lambdas)
                        synthetic_samples.append(batch_synthetic)
                        
                        # å¤„ç†æ—¶é—´æˆ³å’Œæ©ç 
                        if times is not None:
                            batch_times = batch_synthetic[:, :, 0]  # æ—¶é—´æ˜¯ç¬¬0ç»´ç‰¹å¾
                            synthetic_times.append(batch_times)
                        
                        if masks is not None:
                            if torch.is_tensor(masks_cls):
                                masks_cls_np = masks_cls.cpu().numpy()
                            else:
                                masks_cls_np = masks_cls
                                
                            batch_masks = []
                            for idx1, idx2 in zip(indices1, indices2):
                                mask1 = masks_cls_np[idx1]
                                mask2 = masks_cls_np[idx2]
                                synthetic_mask = mask1 | mask2
                                batch_masks.append(synthetic_mask)
                            synthetic_masks.append(np.array(batch_masks))
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        total_progress.update(current_batch_size)
                    
                    # åˆå¹¶æ‰¹æ¬¡ç»“æœ
                    if synthetic_samples:
                        X_list.append(np.concatenate(synthetic_samples, axis=0))
                        y_list.extend([cls] * n_synthetic)
                        
                        if synthetic_times:
                            times_list.append(np.concatenate(synthetic_times, axis=0))
                        if synthetic_masks:
                            masks_list.append(np.concatenate(synthetic_masks, axis=0))
        
        # å…³é—­è¿›åº¦æ¡
        total_progress.close()
        
        print("ğŸ”— åˆå¹¶æ‰€æœ‰é‡é‡‡æ ·æ•°æ®...")
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if torch.is_tensor(X):
            X_resampled = torch.cat([torch.tensor(x) if not torch.is_tensor(x) else x 
                                    for x in X_list], dim=0)
            y_resampled = torch.tensor(y_list)
        else:
            X_resampled = np.concatenate(X_list, axis=0)
            y_resampled = np.array(y_list)
            
        times_resampled = None
        if times is not None:
            if torch.is_tensor(times):
                times_resampled = torch.cat([torch.tensor(t) if not torch.is_tensor(t) else t 
                                            for t in times_list], dim=0)
            else:
                times_resampled = np.concatenate(times_list, axis=0)
                
        masks_resampled = None
        if masks is not None:
            if torch.is_tensor(masks):
                masks_resampled = torch.cat([torch.tensor(m) if not torch.is_tensor(m) else m 
                                            for m in masks_list], dim=0)
            else:
                masks_resampled = np.concatenate(masks_list, axis=0)
        
        return X_resampled, y_resampled, times_resampled, masks_resampled
    
    def visualize_synthesis_comparison(self, X_original, y_original, n_examples=6, save_path=None):
        """
        å¯è§†åŒ–åˆæˆæ ·æœ¬ä¸åŸå§‹æ ·æœ¬çš„å¯¹æ¯”
        
        Args:
            X_original: åŸå§‹æ—¶é—´åºåˆ—æ•°æ® (n_samples, seq_len, n_features)
            y_original: åŸå§‹æ ‡ç­¾
            n_examples: æ¯ä¸ªç±»åˆ«æ˜¾ç¤ºçš„æ ·æœ¬æ•°
            save_path: ä¿å­˜è·¯å¾„
        """
        # ç»Ÿè®¡ç±»åˆ«
        class_counts = Counter(y_original.tolist() if torch.is_tensor(y_original) else y_original)
        n_classes = len(class_counts)
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(n_classes, n_examples, figsize=(n_examples*4, n_classes*3))
        if n_classes == 1:
            axes = axes.reshape(1, -1)
        if n_examples == 1:
            axes = axes.reshape(-1, 1)
            
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3']
        
        for cls_idx, cls in enumerate(class_counts.keys()):
            # è·å–å½“å‰ç±»çš„æ•°æ®
            if torch.is_tensor(y_original):
                cls_indices = (y_original == cls).nonzero(as_tuple=True)[0].cpu().numpy()
                X_cls = X_original[cls_indices].cpu().numpy()
            else:
                cls_indices = np.where(y_original == cls)[0]
                X_cls = X_original[cls_indices]
            
            # ç”Ÿæˆåˆæˆæ ·æœ¬ç”¨äºå¯¹æ¯”
            if len(cls_indices) >= 2:
                n_samples = len(cls_indices)
                synthetic_examples = []
                source_pairs = []
                
                for ex_idx in range(min(n_examples, 10)):  # æœ€å¤šç”Ÿæˆ10ä¸ªæ ·æœ¬
                    # éšæœºé€‰æ‹©ä¸¤ä¸ªæ ·æœ¬
                    idx1, idx2 = np.random.choice(n_samples, 2, replace=False)
                    ts1, ts2 = X_cls[idx1], X_cls[idx2]
                    lambda_val = np.random.beta(2, 2)
                    
                    # æ ¹æ®ä¸åŒæ¨¡å¼ç”Ÿæˆåˆæˆæ ·æœ¬
                    if self.synthesis_mode == 'interpolation':
                        synthetic_ts = self._synthesize_interpolation(ts1, ts2, lambda_val)
                    elif self.synthesis_mode == 'warping':
                        synthetic_ts = self._synthesize_warping(ts1, ts2, lambda_val)
                    else:  # hybrid
                        synthetic_ts = self._synthesize_hybrid(ts1, ts2, lambda_val)
                    
                    synthetic_examples.append(synthetic_ts)
                    source_pairs.append((ts1, ts2, lambda_val))
                
                # ç»˜åˆ¶å¯¹æ¯”å›¾
                for ex_idx in range(min(n_examples, len(synthetic_examples))):
                    ax = axes[cls_idx, ex_idx]
                    ts1, ts2, lambda_val = source_pairs[ex_idx]
                    synthetic_ts = synthetic_examples[ex_idx]
                    
                    # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªç‰¹å¾ç»´åº¦ï¼ˆå¦‚æœæœ‰å¤šä¸ªç‰¹å¾ï¼‰
                    feature_idx = 0
                    t = np.linspace(0, 1, len(ts1))
                    
                    # ç»˜åˆ¶æºæ—¶é—´åºåˆ—
                    ax.plot(t, ts1[:, feature_idx], 'o-', alpha=0.7, linewidth=1, 
                           color=colors[0], label=f'æºåºåˆ—1', markersize=3)
                    ax.plot(t, ts2[:, feature_idx], 's-', alpha=0.7, linewidth=1, 
                           color=colors[1], label=f'æºåºåˆ—2', markersize=3)
                    
                    # ç»˜åˆ¶åˆæˆæ—¶é—´åºåˆ—
                    ax.plot(t, synthetic_ts[:, feature_idx], '^-', linewidth=2, 
                           color=colors[2], label=f'åˆæˆåºåˆ—(Î»={lambda_val:.2f})', markersize=4)
                    
                    ax.set_title(f'ç±»åˆ«{cls} - æ ·æœ¬{ex_idx+1}\n{self.synthesis_mode}æ¨¡å¼', 
                               fontsize=10, fontweight='bold')
                    ax.set_xlabel('æ—¶é—´')
                    ax.set_ylabel('æ•°å€¼')
                    ax.grid(True, alpha=0.3)
                    ax.legend(fontsize=8)
                    
                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    orig_std = np.mean([np.std(ts1[:, feature_idx]), np.std(ts2[:, feature_idx])])
                    synth_std = np.std(synthetic_ts[:, feature_idx])
                    ax.text(0.02, 0.98, f'åŸå§‹æ–¹å·®: {orig_std:.3f}\nåˆæˆæ–¹å·®: {synth_std:.3f}', 
                           transform=ax.transAxes, va='top', ha='left', 
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                           fontsize=7)
            else:
                # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œæ˜¾ç¤ºæç¤º
                for ex_idx in range(n_examples):
                    ax = axes[cls_idx, ex_idx]
                    ax.text(0.5, 0.5, f'ç±»åˆ«{cls}\næ ·æœ¬ä¸è¶³', 
                           ha='center', va='center', transform=ax.transAxes,
                           fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))
                    ax.set_xlim(0, 1)
                    ax.set_ylim(0, 1)
        
        plt.suptitle(f'æ—¶é—´åºåˆ—åˆæˆæ•ˆæœå¯¹æ¯” - {self.synthesis_mode.upper()}æ¨¡å¼', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"åˆæˆå¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        return fig
    
    def visualize_synthesis_quality_metrics(self, X_original, y_original, n_synthetic=100, save_path=None):
        """
        å¯è§†åŒ–åˆæˆè´¨é‡è¯„ä¼°æŒ‡æ ‡
        """
        class_counts = Counter(y_original.tolist() if torch.is_tensor(y_original) else y_original)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()
        
        quality_metrics = {'interpolation': {}, 'warping': {}, 'hybrid': {}}
        
        for mode in quality_metrics.keys():
            # ä¸´æ—¶è®¾ç½®åˆæˆæ¨¡å¼
            original_mode = self.synthesis_mode
            self.synthesis_mode = mode
            
            for cls in class_counts.keys():
                # è·å–å½“å‰ç±»çš„æ•°æ®
                if torch.is_tensor(y_original):
                    cls_indices = (y_original == cls).nonzero(as_tuple=True)[0].cpu().numpy()
                    X_cls = X_original[cls_indices].cpu().numpy()
                else:
                    cls_indices = np.where(y_original == cls)[0]
                    X_cls = X_original[cls_indices]
                
                if len(cls_indices) >= 10:  # éœ€è¦è¶³å¤Ÿæ ·æœ¬
                    # ç”Ÿæˆåˆæˆæ ·æœ¬
                    synthetic_samples = []
                    for _ in range(min(n_synthetic, 50)):
                        idx1, idx2 = np.random.choice(len(X_cls), 2, replace=False)
                        ts1, ts2 = X_cls[idx1], X_cls[idx2]
                        lambda_val = np.random.beta(2, 2)
                        
                        if mode == 'interpolation':
                            synthetic_ts = self._synthesize_interpolation(ts1, ts2, lambda_val)
                        elif mode == 'warping':
                            synthetic_ts = self._synthesize_warping(ts1, ts2, lambda_val)
                        else:
                            synthetic_ts = self._synthesize_hybrid(ts1, ts2, lambda_val)
                        
                        synthetic_samples.append(synthetic_ts)
                    
                    if synthetic_samples:
                        synthetic_array = np.array(synthetic_samples)
                        
                        # è®¡ç®—è´¨é‡æŒ‡æ ‡
                        # 1. æ–¹å·®ä¿æŒ
                        orig_var = np.var(X_cls, axis=0).mean()
                        synth_var = np.var(synthetic_array, axis=0).mean()
                        var_ratio = synth_var / orig_var if orig_var > 0 else 1
                        
                        # 2. å‡å€¼ä¿æŒ
                        orig_mean = np.mean(X_cls, axis=0).mean()
                        synth_mean = np.mean(synthetic_array, axis=0).mean()
                        mean_diff = abs(synth_mean - orig_mean) / (abs(orig_mean) + 1e-6)
                        
                        # 3. å½¢çŠ¶ç›¸ä¼¼æ€§ï¼ˆåŸºäºç›¸å…³ç³»æ•°ï¼‰
                        correlations = []
                        for synth_sample in synthetic_samples[:10]:  # å–å‰10ä¸ªæ ·æœ¬
                            corr_with_originals = []
                            for orig_sample in X_cls[:min(20, len(X_cls))]:
                                corr = np.corrcoef(synth_sample.flatten(), orig_sample.flatten())[0,1]
                                if not np.isnan(corr):
                                    corr_with_originals.append(abs(corr))
                            if corr_with_originals:
                                correlations.append(np.mean(corr_with_originals))
                        
                        shape_similarity = np.mean(correlations) if correlations else 0
                        
                        quality_metrics[mode][cls] = {
                            'variance_ratio': var_ratio,
                            'mean_difference': mean_diff,
                            'shape_similarity': shape_similarity
                        }
            
            # æ¢å¤åŸå§‹æ¨¡å¼
            self.synthesis_mode = original_mode
        
        # ç»˜åˆ¶è´¨é‡æŒ‡æ ‡å¯¹æ¯”
        modes = list(quality_metrics.keys())
        metrics = ['variance_ratio', 'mean_difference', 'shape_similarity']
        metric_names = ['æ–¹å·®æ¯”ç‡', 'å‡å€¼å·®å¼‚', 'å½¢çŠ¶ç›¸ä¼¼æ€§']
        
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i]
            
            x_pos = np.arange(len(modes))
            values = []
            
            for mode in modes:
                mode_values = []
                for cls_metrics in quality_metrics[mode].values():
                    if metric in cls_metrics:
                        mode_values.append(cls_metrics[metric])
                values.append(np.mean(mode_values) if mode_values else 0)
            
            bars = ax.bar(x_pos, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
            ax.set_xlabel('åˆæˆæ¨¡å¼')
            ax.set_ylabel(metric_name)
            ax.set_title(f'{metric_name}å¯¹æ¯”')
            ax.set_xticks(x_pos)
            ax.set_xticklabels([mode.upper() for mode in modes])
            ax.grid(axis='y', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{value:.3f}', ha='center', va='bottom')
        
        # ç¬¬å››ä¸ªå­å›¾ï¼šç»¼åˆè´¨é‡è¯„åˆ†
        ax = axes[3]
        ç»¼åˆè¯„åˆ† = []
        for mode in modes:
            mode_score = 0
            mode_count = 0
            for cls_metrics in quality_metrics[mode].values():
                if cls_metrics:
                    # ç»¼åˆè¯„åˆ†è®¡ç®—ï¼ˆæ–¹å·®æ¯”ç‡æ¥è¿‘1æœ€å¥½ï¼Œå‡å€¼å·®å¼‚è¶Šå°è¶Šå¥½ï¼Œå½¢çŠ¶ç›¸ä¼¼æ€§è¶Šå¤§è¶Šå¥½ï¼‰
                    var_score = 1 - abs(cls_metrics.get('variance_ratio', 1) - 1)  
                    mean_score = 1 - min(cls_metrics.get('mean_difference', 1), 1)
                    shape_score = cls_metrics.get('shape_similarity', 0)
                    
                    score = (var_score + mean_score + shape_score) / 3
                    mode_score += score
                    mode_count += 1
            
            ç»¼åˆè¯„åˆ†.append(mode_score / mode_count if mode_count > 0 else 0)
        
        bars = ax.bar(x_pos, ç»¼åˆè¯„åˆ†, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax.set_xlabel('åˆæˆæ¨¡å¼')
        ax.set_ylabel('ç»¼åˆè´¨é‡è¯„åˆ†')
        ax.set_title('ç»¼åˆè´¨é‡è¯„åˆ†å¯¹æ¯”')
        ax.set_xticks(x_pos)
        ax.set_xticklabels([mode.upper() for mode in modes])
        ax.set_ylim(0, 1)
        ax.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œæ¨è
        best_mode = modes[np.argmax(ç»¼åˆè¯„åˆ†)]
        for bar, value, mode in zip(bars, ç»¼åˆè¯„åˆ†, modes):
            height = bar.get_height()
            label = f'{value:.3f}'
            if mode == best_mode:
                label += '\n(æ¨è)'
                bar.set_color('#FFD93D')
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   label, ha='center', va='bottom', fontweight='bold')
        
        plt.suptitle('æ—¶é—´åºåˆ—åˆæˆè´¨é‡è¯„ä¼°', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è´¨é‡è¯„ä¼°å›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        return fig


class EditedNearestNeighbors:
    """
    ç¼–è¾‘æœ€è¿‘é‚» (ENN) - æ¸…ç†å¤šæ•°ç±»ä¸­çš„å™ªå£°æ ·æœ¬
    æ¸©å’Œç‰ˆæœ¬ï¼šåªåˆ é™¤æ˜æ˜¾çš„å™ªå£°ç‚¹
    """
    
    def __init__(self, 
                 n_neighbors=3,
                 kind_sel='mode',
                 max_removal_ratio=0.2):  # æœ€å¤šåˆ é™¤20%çš„æ ·æœ¬
        """
        Args:
            n_neighbors: ç”¨äºåˆ¤æ–­çš„é‚»å±…æ•°
            kind_sel: é€‰æ‹©æ ‡å‡† ('mode' æˆ– 'all')
            max_removal_ratio: æœ€å¤§åˆ é™¤æ¯”ä¾‹
        """
        self.n_neighbors = n_neighbors
        self.kind_sel = kind_sel
        self.max_removal_ratio = max_removal_ratio
        
    def fit_resample(self, X, y, times=None, masks=None):
        """
        æ‰§è¡ŒENNæ¸…ç†
        
        Args:
            X: (n_samples, seq_len, n_features) æ—¶é—´åºåˆ—æ•°æ®
            y: (n_samples,) æ ‡ç­¾
            times: å¯é€‰çš„æ—¶é—´æˆ³
            masks: å¯é€‰çš„æ©ç 
            
        Returns:
            æ¸…ç†åçš„æ•°æ®
        """
        from collections import Counter  # ç§»åˆ°å‡½æ•°å¼€å§‹å¤„
        from tqdm import tqdm
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        class_counts = Counter(y.tolist() if torch.is_tensor(y) else y)
        print(f"ENNæ¸…ç†å‰ç±»åˆ«åˆ†å¸ƒ: {class_counts}")
        
        # å±•å¹³æ•°æ®ç”¨äºKNN
        n_samples = len(y)
        if torch.is_tensor(X):
            X_flat = X.reshape(n_samples, -1).cpu().numpy()
            y_np = y.cpu().numpy() if torch.is_tensor(y) else y
        else:
            X_flat = X.reshape(n_samples, -1)
            y_np = y
            
        # æ„å»ºKNN
        print("ğŸ”§ æ„å»ºKNNé‚»å±…æŸ¥æ‰¾å™¨...")
        nn = NearestNeighbors(n_neighbors=self.n_neighbors + 1)
        nn.fit(X_flat)
        
        # æ‰¾å‡ºæ¯ä¸ªæ ·æœ¬çš„é‚»å±…
        print("ğŸ” æŸ¥æ‰¾æ¯ä¸ªæ ·æœ¬çš„é‚»å±…...")
        neighbors = nn.kneighbors(X_flat, return_distance=False)[:, 1:]
        
        # å†³å®šä¿ç•™å“ªäº›æ ·æœ¬
        keep_mask = np.ones(n_samples, dtype=bool)
        removal_candidates = []
        
        print("ğŸ§¹ åˆ†æå™ªå£°æ ·æœ¬...")
        for i in tqdm(range(n_samples), desc="æ£€æŸ¥æ ·æœ¬", unit="æ ·æœ¬"):
            neighbor_labels = y_np[neighbors[i]]
            
            if self.kind_sel == 'mode':
                # å¦‚æœå¤§å¤šæ•°é‚»å±…çš„ç±»åˆ«ä¸å½“å‰æ ·æœ¬ä¸åŒï¼Œåˆ™æ ‡è®°ä¸ºå€™é€‰åˆ é™¤
                mode_label = Counter(neighbor_labels).most_common(1)[0][0]
                if mode_label != y_np[i]:
                    removal_candidates.append((i, 1))  # æƒé‡ä¸º1
            elif self.kind_sel == 'all':
                # å¦‚æœæ‰€æœ‰é‚»å±…çš„ç±»åˆ«éƒ½ä¸å½“å‰æ ·æœ¬ä¸åŒï¼Œåˆ™æ ‡è®°ä¸ºå€™é€‰åˆ é™¤
                if not np.any(neighbor_labels == y_np[i]):
                    removal_candidates.append((i, 2))  # æƒé‡ä¸º2ï¼Œä¼˜å…ˆåˆ é™¤
        
        # é™åˆ¶åˆ é™¤æ•°é‡
        max_removals = int(n_samples * self.max_removal_ratio)
        if len(removal_candidates) > max_removals:
            # æŒ‰æƒé‡æ’åºï¼Œä¼˜å…ˆåˆ é™¤æ›´æ˜æ˜¾çš„å™ªå£°
            removal_candidates.sort(key=lambda x: x[1], reverse=True)
            removal_candidates = removal_candidates[:max_removals]
        
        # åº”ç”¨åˆ é™¤
        for idx, _ in removal_candidates:
            keep_mask[idx] = False
        
        # åº”ç”¨æ©ç 
        X_cleaned = X[keep_mask]
        y_cleaned = y[keep_mask] if torch.is_tensor(y) else y[keep_mask]
        
        times_cleaned = times[keep_mask] if times is not None else None
        masks_cleaned = masks[keep_mask] if masks is not None else None
        
        # ç»Ÿè®¡æ¸…ç†åçš„åˆ†å¸ƒ
        class_counts_after = Counter(y_cleaned.tolist() if torch.is_tensor(y_cleaned) else y_cleaned)
        print(f"ENNæ¸…ç†åç±»åˆ«åˆ†å¸ƒ: {class_counts_after}")
        print(f"å…±åˆ é™¤ {n_samples - sum(keep_mask)} ä¸ªæ ·æœ¬")
        
        return X_cleaned, y_cleaned, times_cleaned, masks_cleaned


class HybridResampler:
    """
    æ··åˆé‡é‡‡æ ·å™¨ - ç»“åˆå…ˆè¿›æ—¶é—´åºåˆ—SMOTEå’ŒENNï¼Œæ”¯æŒç‰©ç†çº¦æŸTimeGAN
    """
    
    def __init__(self,
                 smote_k_neighbors=5,
                 enn_n_neighbors=3,
                 sampling_strategy='balanced',
                 synthesis_mode='hybrid',  # å¢åŠ 'physics_timegan'é€‰é¡¹
                 apply_enn=True,
                 noise_level=0.05,
                 physics_weight=0.5,  # ç‰©ç†çº¦æŸæƒé‡
                 random_state=535411460):
        """
        Args:
            smote_k_neighbors: SMOTEçš„é‚»å±…æ•°
            enn_n_neighbors: ENNçš„é‚»å±…æ•°
            sampling_strategy: é‡‡æ ·ç­–ç•¥
                - 'balanced': å®Œå…¨å¹³è¡¡ï¼ˆæ‰€æœ‰ç±»æ•°é‡ç›¸åŒï¼‰
                - 'auto': è‡ªåŠ¨å¹³è¡¡åˆ°å¤šæ•°ç±»
                - float: å°‘æ•°ç±»ç›¸å¯¹å¤šæ•°ç±»çš„æ¯”ä¾‹
            synthesis_mode: æ—¶é—´åºåˆ—åˆæˆæ¨¡å¼
                - 'interpolation': åŸºäºå‡½æ•°æ’å€¼
                - 'warping': åŸºäºæ—¶é—´æ‰­æ›²
                - 'hybrid': æ··åˆæ¨¡å¼
                - 'physics_timegan': ç‰©ç†çº¦æŸTimeGANï¼ˆæ¨èç”¨äºå…‰å˜æ›²çº¿ï¼‰
            apply_enn: æ˜¯å¦åº”ç”¨ENNæ¸…ç†
            noise_level: å™ªå£°æ°´å¹³
            physics_weight: ç‰©ç†çº¦æŸæƒé‡ï¼ˆä»…å¯¹physics_timeganæœ‰æ•ˆï¼‰
            random_state: éšæœºç§å­
        """
        self.sampling_strategy = sampling_strategy
        self.synthesis_mode = synthesis_mode
        self.apply_enn = apply_enn
        self.random_state = random_state
        self.physics_weight = physics_weight
        
        # åˆå§‹åŒ–å…ˆè¿›æ—¶é—´åºåˆ—SMOTE
        self.smote = AdvancedTimeSeriesSMOTE(
            k_neighbors=smote_k_neighbors,
            sampling_strategy='auto' if sampling_strategy == 'balanced' else sampling_strategy,
            synthesis_mode=synthesis_mode,
            noise_level=noise_level,
            physics_weight=physics_weight,  # ä¼ é€’ç‰©ç†çº¦æŸæƒé‡
            random_state=random_state
        )
        
        # åˆå§‹åŒ–ENN
        self.enn = EditedNearestNeighbors(n_neighbors=enn_n_neighbors)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats_ = {}
        
    def fit_resample(self, X, y, times=None, masks=None):
        """
        æ‰§è¡Œæ··åˆé‡é‡‡æ ·
        """
        # è®°å½•åŸå§‹åˆ†å¸ƒ
        original_counts = Counter(y.tolist() if torch.is_tensor(y) else y)
        self.stats_['original'] = dict(original_counts)
        
        print("\n" + "="*60)
        if self.synthesis_mode == 'physics_timegan':
            print("å¼€å§‹ç‰©ç†çº¦æŸTimeGANæ··åˆé‡é‡‡æ ·")
        else:
            print("å¼€å§‹ä¼ ç»Ÿæ··åˆé‡é‡‡æ ·")
        print("="*60)
        
        # Step 1: SMOTEè¿‡é‡‡æ ·
        if self.synthesis_mode == 'physics_timegan':
            print("\nStep 1: ç‰©ç†çº¦æŸTimeGANè¿‡é‡‡æ ·")
        else:
            print("\nStep 1: SMOTEè¿‡é‡‡æ ·")
            
        X_smote, y_smote, times_smote, masks_smote = self.smote.fit_resample(
            X, y, times, masks
        )
        smote_counts = Counter(y_smote.tolist() if torch.is_tensor(y_smote) else y_smote)
        self.stats_['after_smote'] = dict(smote_counts)
        
        # Step 2: ENNæ¸…ç†ï¼ˆå¯é€‰ï¼‰
        if self.apply_enn:
            print("\nStep 2: ENNæ¸…ç†")
            X_final, y_final, times_final, masks_final = self.enn.fit_resample(
                X_smote, y_smote, times_smote, masks_smote
            )
        else:
            X_final, y_final = X_smote, y_smote
            times_final, masks_final = times_smote, masks_smote
            
        final_counts = Counter(y_final.tolist() if torch.is_tensor(y_final) else y_final)
        self.stats_['final'] = dict(final_counts)
        
        print("\né‡é‡‡æ ·å®Œæˆï¼")
        print(f"åŸå§‹æ€»æ ·æœ¬æ•°: {len(y)}")
        print(f"æœ€ç»ˆæ€»æ ·æœ¬æ•°: {len(y_final)}")
        
        if self.synthesis_mode == 'physics_timegan':
            print("âœ… ç‰©ç†çº¦æŸTimeGANç¡®ä¿äº†ç”Ÿæˆæ ·æœ¬çš„å¤©ä½“ç‰©ç†ä¸€è‡´æ€§")
        
        print("="*60 + "\n")
        
        return X_final, y_final, times_final, masks_final
    
    def visualize_distribution(self, save_path=None):
        """
        å¯è§†åŒ–é‡é‡‡æ ·å‰åçš„ç±»åˆ«åˆ†å¸ƒ
        """
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        stages = ['original', 'after_smote', 'final']
        titles = ['åŸå§‹åˆ†å¸ƒ', 'SMOTEå', 'æœ€ç»ˆåˆ†å¸ƒ(ENNå)']
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        for ax, stage, title, color in zip(axes, stages, titles, colors):
            if stage in self.stats_:
                data = self.stats_[stage]
                classes = list(data.keys())
                counts = list(data.values())
                
                bars = ax.bar(classes, counts, color=color, alpha=0.7, edgecolor='black')
                ax.set_title(title, fontsize=14, fontweight='bold')
                ax.set_xlabel('ç±»åˆ«', fontsize=12)
                ax.set_ylabel('æ ·æœ¬æ•°', fontsize=12)
                ax.grid(axis='y', alpha=0.3)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, count in zip(bars, counts):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(count)}', ha='center', va='bottom', fontsize=10)
                
                # è®¡ç®—ä¸å¹³è¡¡ç‡
                max_count = max(counts)
                min_count = min(counts)
                imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
                ax.text(0.5, 0.95, f'ä¸å¹³è¡¡ç‡: {imbalance_ratio:.2f}',
                       transform=ax.transAxes, ha='center', va='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.suptitle('æ··åˆé‡é‡‡æ ·æ•ˆæœåˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
        
        return fig


def generate_compatible_resampled_data(original_data_path, output_path, sampling_strategy='balanced', 
                                       synthesis_mode='hybrid', apply_enn=True, random_state=535411460):
    """
    ç”Ÿæˆä¸åŸå§‹æ•°æ®æ ¼å¼å®Œå…¨å…¼å®¹çš„é‡é‡‡æ ·æ•°æ®
    
    Args:
        original_data_path: åŸå§‹æ•°æ®è·¯å¾„
        output_path: è¾“å‡ºé‡é‡‡æ ·æ•°æ®è·¯å¾„
        sampling_strategy: é‡‡æ ·ç­–ç•¥
        synthesis_mode: åˆæˆæ¨¡å¼
        apply_enn: æ˜¯å¦åº”ç”¨ENNæ¸…ç†
        random_state: éšæœºç§å­
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®...")
    
    # åŠ è½½åŸå§‹æ•°æ®
    with open(original_data_path, 'rb') as f:
        original_data = pickle.load(f)
    
    print(f"åŸå§‹æ•°æ®: {len(original_data)}ä¸ªæ ·æœ¬")
    
    # æå–æ•°æ®ç”¨äºé‡é‡‡æ ·
    X_list = []
    y_list = []
    times_list = []
    masks_list = []
    original_samples = []
    
    for i, sample in enumerate(original_data):
        # æå–æ—¶é—´åºåˆ—æ•°æ® (seq_len, 3) [time, mag, errmag]
        time_data = sample['time']
        mag_data = sample['mag']
        errmag_data = sample['errmag']
        mask_data = sample['mask']
        
        # æ„å»ºç‰¹å¾çŸ©é˜µ
        features = np.column_stack([time_data, mag_data, errmag_data])
        X_list.append(features)
        y_list.append(sample['label'])
        times_list.append(time_data)
        masks_list.append(mask_data)
        original_samples.append(sample)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    X = np.array(X_list)  # (n_samples, seq_len, 3)
    y = np.array(y_list)  # (n_samples,)
    times = np.array(times_list)  # (n_samples, seq_len)
    masks = np.array(masks_list)  # (n_samples, seq_len)
    
    print(f"æ•°æ®è½¬æ¢å®Œæˆ: X.shape={X.shape}, y.shape={y.shape}")
    
    # æ‰§è¡Œé‡é‡‡æ ·
    print("ğŸš€ å¼€å§‹GPUåŠ é€Ÿé‡é‡‡æ ·...")
    resampler = HybridResampler(
        smote_k_neighbors=5,
        enn_n_neighbors=3,
        sampling_strategy=sampling_strategy,
        synthesis_mode=synthesis_mode,
        apply_enn=apply_enn,
        noise_level=0.05,
        random_state=random_state
    )
    
    X_resampled, y_resampled, times_resampled, masks_resampled = resampler.fit_resample(
        X, y, times, masks
    )
    
    print(f"é‡é‡‡æ ·å®Œæˆ: {len(X_resampled)}ä¸ªæ ·æœ¬")
    
    # æ„å»ºä¸åŸå§‹æ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´çš„é‡é‡‡æ ·æ•°æ®
    print("ğŸ”— æ„å»ºå…¼å®¹æ ¼å¼çš„é‡é‡‡æ ·æ•°æ®...")
    resampled_data = []
    
    # ç»Ÿè®¡ç±»åˆ«ä¿¡æ¯
    from collections import Counter
    class_counts = Counter(y_resampled)
    unique_labels = list(class_counts.keys())
    
    # æ„å»ºç±»åˆ«åˆ°ç±»åˆ«åçš„æ˜ å°„
    label_to_class_name = {}
    for sample in original_samples:
        label_to_class_name[sample['label']] = sample['class_name']
    
    for i in range(len(X_resampled)):
        # æå–é‡é‡‡æ ·æ•°æ®
        features = X_resampled[i]  # (seq_len, 3)
        time_data = features[:, 0]
        mag_data = features[:, 1]
        errmag_data = features[:, 2]
        
        # è·å–å¯¹åº”çš„æ—¶é—´å’Œæ©ç 
        if times_resampled is not None:
            time_data = times_resampled[i]
        if masks_resampled is not None:
            mask_data = masks_resampled[i]
        else:
            # åŸºäºæ—¶é—´æ•°æ®ç”Ÿæˆæ©ç 
            mask_data = (time_data > -1000) & (time_data < 1000)
        
        # ä¿®æ­£æ•°æ®ï¼šç¡®ä¿æ²¡æœ‰å¼‚å¸¸å€¼
        # 1. ä¿®æ­£æ—¶é—´æ•°æ® - å¯¹äºå¡«å……ä½ç½®ä½¿ç”¨-1e9
        valid_mask = mask_data.astype(bool)
        time_data[~valid_mask] = -1e9
        mag_data[~valid_mask] = 0.0
        
        # 2. ä¿®æ­£errmag - ç¡®ä¿éè´Ÿä¸”åˆç†
        errmag_data = np.abs(errmag_data)  # ç¡®ä¿éè´Ÿ
        errmag_data = np.clip(errmag_data, 0.01, 2.0)  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        errmag_data[~valid_mask] = 0.0  # å¡«å……ä½ç½®è®¾ä¸º0
        
        # è®¡ç®—æœ‰æ•ˆç‚¹æ•°
        valid_points = valid_mask.sum()
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªåŒç±»åˆ«çš„åŸå§‹æ ·æœ¬ä½œä¸ºæ¨¡æ¿ï¼ˆç”¨äºperiodç­‰å‚æ•°ï¼‰
        same_class_samples = [s for s in original_samples if s['label'] == y_resampled[i]]
        if same_class_samples:
            template_sample = np.random.choice(same_class_samples)
            period = template_sample['period']
        else:
            period = np.float64(1.0)  # é»˜è®¤å‘¨æœŸ
        
        # æ„å»ºä¸åŸå§‹æ ¼å¼å®Œå…¨ä¸€è‡´çš„æ ·æœ¬
        resampled_sample = {
            # æ ¸å¿ƒæ•°æ® - ä¸¥æ ¼åŒ¹é…åŸå§‹æ•°æ®ç±»å‹
            'time': time_data.astype(np.float64),
            'mag': mag_data.astype(np.float64),
            'errmag': errmag_data.astype(np.float64),
            'mask': mask_data.astype(bool),
            'period': np.float64(period),
            'label': int(y_resampled[i]),
            
            # å…ƒæ•°æ® - åŒ¹é…åŸå§‹æ ¼å¼
            'file_id': f'resampled_{i:06d}.dat',
            'original_length': int(valid_points),
            'valid_points': np.int64(valid_points),
            'coverage': np.float64(valid_points / 512),
            'class_name': label_to_class_name.get(y_resampled[i], f'class_{y_resampled[i]}')
        }
        
        resampled_data.append(resampled_sample)
    
    # ä¿å­˜é‡é‡‡æ ·æ•°æ®
    print(f"ğŸ’¾ ä¿å­˜é‡é‡‡æ ·æ•°æ®åˆ°: {output_path}")
    with open(output_path, 'wb') as f:
        pickle.dump(resampled_data, f)
    
    # éªŒè¯ä¿å­˜çš„æ•°æ®æ ¼å¼
    print("âœ… éªŒè¯ä¿å­˜çš„æ•°æ®æ ¼å¼...")
    with open(output_path, 'rb') as f:
        saved_data = pickle.load(f)
    
    print(f"éªŒè¯: ä¿å­˜äº†{len(saved_data)}ä¸ªæ ·æœ¬")
    if saved_data:
        sample = saved_data[0]
        print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®: {list(sample.keys())}")
        for key, value in sample.items():
            if hasattr(value, 'shape'):
                print(f"  {key}: ç±»å‹={type(value)}, å½¢çŠ¶={value.shape}, dtype={value.dtype}")
            else:
                print(f"  {key}: ç±»å‹={type(value)}, å€¼={value}")
    
    # ç»Ÿè®¡é‡é‡‡æ ·åçš„ç±»åˆ«åˆ†å¸ƒ
    final_counts = Counter([s['label'] for s in saved_data])
    print(f"é‡é‡‡æ ·åç±»åˆ«åˆ†å¸ƒ: {dict(final_counts)}")
    
    return output_path


def save_resampled_data(X, y, times, masks, dataset_name, save_dir='/root/autodl-fs/lnsde-contiformer/data/resampled'):
    """
    ä¿å­˜é‡é‡‡æ ·åçš„æ•°æ®
    
    Args:
        X: é‡é‡‡æ ·åçš„ç‰¹å¾
        y: é‡é‡‡æ ·åçš„æ ‡ç­¾
        times: é‡é‡‡æ ·åçš„æ—¶é—´æˆ³
        masks: é‡é‡‡æ ·åçš„æ©ç 
        dataset_name: æ•°æ®é›†åç§°
        save_dir: ä¿å­˜ç›®å½•
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜è·¯å¾„
    save_path = os.path.join(save_dir, f'{dataset_name}_resampled_{timestamp}.pkl')
    
    # ä¿å­˜æ•°æ®
    data = {
        'X': X,
        'y': y,
        'times': times,
        'masks': masks,
        'dataset': dataset_name,
        'timestamp': timestamp,
        'distribution': dict(Counter(y.tolist() if torch.is_tensor(y) else y))
    }
    
    with open(save_path, 'wb') as f:
        pickle.dump(data, f)
    
    print(f"é‡é‡‡æ ·æ•°æ®å·²ä¿å­˜è‡³: {save_path}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = os.path.join(save_dir, f'{dataset_name}_stats_{timestamp}.txt')
    with open(stats_path, 'w') as f:
        f.write(f"æ•°æ®é›†: {dataset_name}\n")
        f.write(f"æ—¶é—´æˆ³: {timestamp}\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(y)}\n")
        f.write(f"ç±»åˆ«åˆ†å¸ƒ: {data['distribution']}\n")
        f.write(f"ç‰¹å¾å½¢çŠ¶: {X.shape}\n")
        if times is not None:
            f.write(f"æ—¶é—´æˆ³å½¢çŠ¶: {times.shape}\n")
        if masks is not None:
            f.write(f"æ©ç å½¢çŠ¶: {masks.shape}\n")
    
    print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {stats_path}")
    
    return save_path


def load_resampled_data(file_path):
    """
    åŠ è½½é‡é‡‡æ ·åçš„æ•°æ®
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«é‡é‡‡æ ·æ•°æ®çš„å­—å…¸
    """
    with open(file_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"å·²åŠ è½½é‡é‡‡æ ·æ•°æ®:")
    print(f"  æ•°æ®é›†: {data['dataset']}")
    print(f"  æ—¶é—´æˆ³: {data['timestamp']}")
    print(f"  æ€»æ ·æœ¬æ•°: {len(data['y'])}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {data['distribution']}")
    
    return data


# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•
    np.random.seed(535411460)
    
    # æ¨¡æ‹Ÿä¸å¹³è¡¡çš„æ—¶é—´åºåˆ—æ•°æ®
    n_majority = 1000
    n_minority1 = 100
    n_minority2 = 50
    seq_len = 100
    n_features = 3
    
    # ç”Ÿæˆæ•°æ®
    X_maj = np.random.randn(n_majority, seq_len, n_features)
    X_min1 = np.random.randn(n_minority1, seq_len, n_features) + 2
    X_min2 = np.random.randn(n_minority2, seq_len, n_features) - 2
    
    X = np.concatenate([X_maj, X_min1, X_min2], axis=0)
    y = np.concatenate([
        np.zeros(n_majority, dtype=int),
        np.ones(n_minority1, dtype=int),
        np.ones(n_minority2, dtype=int) * 2
    ])
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    times = np.tile(np.linspace(0, 1, seq_len), (len(y), 1))
    
    # ç”Ÿæˆæ©ç 
    masks = np.ones((len(y), seq_len), dtype=bool)
    
    print("="*60)
    print("æµ‹è¯•å…ˆè¿›æ—¶é—´åºåˆ—é‡é‡‡æ ·å™¨...")
    print("="*60)
    
    # åˆ›å»ºä¸åŒåˆæˆæ¨¡å¼çš„é‡é‡‡æ ·å™¨è¿›è¡Œå¯¹æ¯”
    modes = ['interpolation', 'warping', 'hybrid']
    results = {}
    
    for mode in modes:
        print(f"\næµ‹è¯•{mode}æ¨¡å¼:")
        print("-" * 40)
        
        # åˆ›å»ºé‡é‡‡æ ·å™¨
        resampler = HybridResampler(
            smote_k_neighbors=5,
            enn_n_neighbors=3,
            sampling_strategy='balanced',
            synthesis_mode=mode,
            noise_level=0.05,
            apply_enn=True
        )
        
        # æ‰§è¡Œé‡é‡‡æ ·
        X_resampled, y_resampled, times_resampled, masks_resampled = resampler.fit_resample(
            X, y, times, masks
        )
        
        results[mode] = {
            'X': X_resampled,
            'y': y_resampled,
            'times': times_resampled,
            'masks': masks_resampled,
            'resampler': resampler
        }
        
        # å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
        os.makedirs('/root/autodl-tmp/lnsde-contiformer/results/pics', exist_ok=True)
        save_path = f'/root/autodl-tmp/lnsde-contiformer/results/pics/resampling_{mode}_distribution.png'
        resampler.visualize_distribution(save_path=save_path)
        
        # å¯è§†åŒ–åˆæˆæ•ˆæœå¯¹æ¯”
        print(f"ç”Ÿæˆ{mode}æ¨¡å¼çš„åˆæˆæ•ˆæœå¯¹æ¯”å›¾...")
        synthesis_comparison_path = f'/root/autodl-tmp/lnsde-contiformer/results/pics/synthesis_comparison_{mode}.png'
        resampler.smote.visualize_synthesis_comparison(
            X, y, n_examples=4, save_path=synthesis_comparison_path
        )
        
        # ä¿å­˜é‡é‡‡æ ·æ•°æ®
        save_path = save_resampled_data(
            X_resampled, y_resampled, times_resampled, masks_resampled,
            dataset_name=f'test_{mode}'
        )
    
    # ç”Ÿæˆç»¼åˆè´¨é‡è¯„ä¼°ï¼ˆä½¿ç”¨hybridæ¨¡å¼çš„resamplerï¼‰
    print(f"\nç”Ÿæˆç»¼åˆè´¨é‡è¯„ä¼°...")
    quality_assessment_path = '/root/autodl-tmp/lnsde-contiformer/results/pics/synthesis_quality_assessment.png'
    results['hybrid']['resampler'].smote.visualize_synthesis_quality_metrics(
        X, y, n_synthetic=50, save_path=quality_assessment_path
    )
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "="*60)
    print("åˆæˆæ¨¡å¼å¯¹æ¯”åˆ†æ:")
    print("="*60)
    
    for mode, result in results.items():
        print(f"\n{mode.upper()}æ¨¡å¼:")
        print(f"  - æœ€ç»ˆæ ·æœ¬æ•°: {len(result['y'])}")
        print(f"  - ç±»åˆ«åˆ†å¸ƒ: {Counter(result['y'].tolist() if torch.is_tensor(result['y']) else result['y'])}")
        print(f"  - åˆæˆè´¨é‡: {'é«˜è´¨é‡æ—¶é—´åºåˆ—æ„ŸçŸ¥' if mode in ['interpolation', 'hybrid'] else 'æ—¶é—´æ‰­æ›²å˜æ¢'}")
        mode_features = {
            'interpolation': 'æ ·æ¡æ’å€¼+å½¢çŠ¶ä¿æŒå™ªå£°', 
            'warping': 'æ—¶é—´æ‰­æ›²+éçº¿æ€§æ˜ å°„', 
            'hybrid': 'éšæœºæ··åˆä¸¤ç§ç­–ç•¥'
        }
        print(f"  - ç‰¹ç‚¹: {mode_features[mode]}")
    
    print(f"\né‡è¦æ”¹è¿›:")
    print("  âœ“ ä¸å†æ˜¯ç®€å•çš„çº¿æ€§æ’å€¼å¤åˆ¶ç²˜è´´")
    print("  âœ“ ä½¿ç”¨DTWè·ç¦»è¿›è¡Œæ—¶é—´åºåˆ—ç›¸ä¼¼åº¦è®¡ç®—")
    print("  âœ“ åŸºäºæ ·æ¡æ’å€¼çš„å‡½æ•°åˆæˆ")
    print("  âœ“ æ—¶é—´æ‰­æ›²å’Œéçº¿æ€§å˜æ¢")
    print("  âœ“ å½¢çŠ¶ä¿æŒçš„æ™ºèƒ½å™ªå£°æ³¨å…¥")
    print("  âœ“ å®Œæ•´çš„è´¨é‡è¯„ä¼°å’Œå¯è§†åŒ–ç³»ç»Ÿ")
    
    print(f"\næ¨èä½¿ç”¨: HYBRIDæ¨¡å¼ - ç»“åˆäº†å‡½æ•°æ’å€¼å’Œæ—¶é—´æ‰­æ›²çš„ä¼˜åŠ¿")
    print("="*60)
    
    print("\næµ‹è¯•å®Œæˆï¼")