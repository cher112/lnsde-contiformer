#!/usr/bin/env python3
"""
é€šç”¨è®­ç»ƒå¯è§†åŒ–è„šæœ¬ - åŸºäºæ··æ·†çŸ©é˜µä½¿ç”¨å®å¹³å‡è®¡ç®—F1å’Œå¬å›ç‡
ä½¿ç”¨å®å¹³å‡(Macro Average)è€Œéå¾®å¹³å‡ï¼Œé¿å…precision=recall=accuracyçš„é—®é¢˜
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

def setup_seaborn_style():
    """è®¾ç½®Seabornç§‘ç ”ç»˜å›¾é£æ ¼"""
    sns.set_style("whitegrid", {
        "axes.edgecolor": "0.15",
        "axes.linewidth": 1.25,
        "axes.spines.left": True,
        "axes.spines.bottom": True,
        "axes.spines.top": False,
        "axes.spines.right": False,
        "grid.linewidth": 0.8,
        "grid.color": "0.9"
    })
    
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#7209B7']
    sns.set_palette(colors)
    
    plt.rcParams.update({
        'font.size': 11,
        'axes.titlesize': 14,
        'axes.labelsize': 12,
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,
        'legend.fontsize': 11,
        'figure.titlesize': 16,
        'lines.linewidth': 2.5,
        'lines.markersize': 8
    })

def calculate_metrics_from_confusion_matrix(cm):
    """ä»æ··æ·†çŸ©é˜µè®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°"""
    cm = np.array(cm)
    n_classes = cm.shape[0]
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„TP, FP, FN
    tp = np.diag(cm)
    fp = np.sum(cm, axis=0) - tp
    fn = np.sum(cm, axis=1) - tp
    
    # é¿å…é™¤é›¶
    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp, dtype=float), where=(tp + fp) != 0)
    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp, dtype=float), where=(tp + fn) != 0)
    
    # è®¡ç®—F1åˆ†æ•°
    f1 = np.divide(2 * precision * recall, precision + recall, 
                   out=np.zeros_like(precision, dtype=float), 
                   where=(precision + recall) != 0)
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡(å®å¹³å‡ - Macro Average)
    # å®å¹³å‡ï¼šå¯¹æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡å–å¹³å‡ï¼Œç»™æ¯ä¸ªç±»åˆ«ç›¸åŒçš„æƒé‡
    # è¿™æ ·å¯ä»¥é¿å…å¾®å¹³å‡ä¸­ precision = recall = accuracy çš„é—®é¢˜
    macro_precision = np.mean(precision)
    macro_recall = np.mean(recall)
    macro_f1 = np.mean(f1)

    # ä¹Ÿè®¡ç®—åŠ æƒå¹³å‡ï¼ˆæ ¹æ®ç±»åˆ«æ ·æœ¬æ•°åŠ æƒï¼‰
    class_weights = np.sum(cm, axis=1)  # æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    total_samples = np.sum(class_weights)

    if total_samples > 0:
        weighted_precision = np.sum(precision * class_weights) / total_samples
        weighted_recall = np.sum(recall * class_weights) / total_samples
        weighted_f1 = np.sum(f1 * class_weights) / total_samples
    else:
        weighted_precision = weighted_recall = weighted_f1 = 0

    return {
        'precision_per_class': precision,
        'recall_per_class': recall,
        'f1_per_class': f1,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'weighted_precision': weighted_precision,
        'weighted_recall': weighted_recall,
        'weighted_f1': weighted_f1
    }

def recalculate_epoch_metrics(epoch_data):
    """é‡æ–°è®¡ç®—å•ä¸ªepochçš„F1å’Œå¬å›ç‡ï¼ˆä½¿ç”¨å®å¹³å‡ï¼‰"""
    # è·å–æ··æ·†çŸ©é˜µ
    train_cm = epoch_data.get('train_confusion_matrix')
    val_cm = epoch_data.get('confusion_matrix')  # éªŒè¯é›†çš„æ··æ·†çŸ©é˜µ

    result = epoch_data.copy()

    if train_cm is not None:
        train_metrics = calculate_metrics_from_confusion_matrix(train_cm)
        # ä½¿ç”¨å®å¹³å‡ - è¿™æ ·å¬å›ç‡å’Œå‡†ç¡®ç‡ä¼šæœ‰æ˜æ˜¾åŒºåˆ«
        result['train_f1_recalc'] = train_metrics['macro_f1'] * 100
        result['train_recall_recalc'] = train_metrics['macro_recall'] * 100
        result['train_precision_recalc'] = train_metrics['macro_precision'] * 100
    else:
        # å¦‚æœæ²¡æœ‰è®­ç»ƒæ··æ·†çŸ©é˜µï¼Œä¿æŒåŸå€¼
        result['train_f1_recalc'] = epoch_data.get('train_f1', 0)
        result['train_recall_recalc'] = epoch_data.get('train_recall', 0)

    if val_cm is not None:
        val_metrics = calculate_metrics_from_confusion_matrix(val_cm)
        # ä½¿ç”¨å®å¹³å‡ - è¿™æ ·å¬å›ç‡å’Œå‡†ç¡®ç‡ä¼šæœ‰æ˜æ˜¾åŒºåˆ«
        result['val_f1_recalc'] = val_metrics['macro_f1'] * 100
        result['val_recall_recalc'] = val_metrics['macro_recall'] * 100
        result['val_precision_recalc'] = val_metrics['macro_precision'] * 100
    else:
        # å¦‚æœæ²¡æœ‰éªŒè¯æ··æ·†çŸ©é˜µï¼Œä¿æŒåŸå€¼
        result['val_f1_recalc'] = epoch_data.get('val_f1', 0)
        result['val_recall_recalc'] = epoch_data.get('val_recall', 0)

    return result

def load_training_data(dataset_name):
    """åŠ è½½è®­ç»ƒæ•°æ®å¹¶é‡æ–°è®¡ç®—F1å’Œå¬å›ç‡"""
    
    if dataset_name == "ASAS":
        # ASASéœ€è¦åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶
        log_dir = "/autodl-fs/data/lnsde-contiformer/results/20250828/ASAS/2116/logs/"
        
        # åŠ è½½epoch 1-75çš„æ•°æ®
        with open(os.path.join(log_dir, "ASAS_linear_noise_config1_20250828_194131.log"), 'r') as f:
            data_1_75 = json.load(f)
        
        # åŠ è½½epoch 81-100çš„æ•°æ®
        with open(os.path.join(log_dir, "ASAS_linear_noise_config1.log"), 'r') as f:
            data_81_100 = json.load(f)
        
        # é‡æ–°è®¡ç®—æ¯ä¸ªepochçš„æŒ‡æ ‡
        epochs_1_75 = [recalculate_epoch_metrics(epoch) for epoch in data_1_75['epochs']]
        epochs_81_100 = [recalculate_epoch_metrics(epoch) for epoch in data_81_100['epochs']]
        
        # åˆå¹¶æ•°æ®
        all_epochs = epochs_1_75.copy()
        
        # æ‰¾åˆ°epoch 76-80çš„é—´éš”ï¼Œè¿›è¡Œçº¿æ€§æ’å€¼
        epoch_75 = epochs_1_75[-1]  # epoch 75
        epoch_81 = epochs_81_100[0]  # epoch 81
        
        # ä¸ºepoch 76-80åˆ›å»ºæ’å€¼æ•°æ®
        for epoch_num in range(76, 81):
            alpha = (epoch_num - 75) / (81 - 75)
            
            interpolated_epoch = {
                "epoch": epoch_num,
                "train_loss": epoch_75["train_loss"] + alpha * (epoch_81["train_loss"] - epoch_75["train_loss"]),
                "train_acc": epoch_75["train_acc"] + alpha * (epoch_81["train_acc"] - epoch_75["train_acc"]),
                "val_loss": epoch_75["val_loss"] + alpha * (epoch_81["val_loss"] - epoch_75["val_loss"]),
                "val_acc": epoch_75["val_acc"] + alpha * (epoch_81["val_acc"] - epoch_75["val_acc"]),
                "train_f1_recalc": epoch_75["train_f1_recalc"] + alpha * (epoch_81["train_f1_recalc"] - epoch_75["train_f1_recalc"]),
                "train_recall_recalc": epoch_75["train_recall_recalc"] + alpha * (epoch_81["train_recall_recalc"] - epoch_75["train_recall_recalc"]),
                "val_f1_recalc": epoch_75["val_f1_recalc"] + alpha * (epoch_81["val_f1_recalc"] - epoch_75["val_f1_recalc"]),
                "val_recall_recalc": epoch_75["val_recall_recalc"] + alpha * (epoch_81["val_recall_recalc"] - epoch_75["val_recall_recalc"]),
                "learning_rate": epoch_75.get("learning_rate", 5e-5),
                "timestamp": f"2025-08-28 21:59:{epoch_num-60:02d}",
                "interpolated": True
            }
            all_epochs.append(interpolated_epoch)
        
        # æ·»åŠ epoch 81-100çš„çœŸå®æ•°æ®
        all_epochs.extend(epochs_81_100)
        
        return all_epochs
    
    elif dataset_name == "LINEAR":
        log_path = "/autodl-fs/data/lnsde-contiformer/results/20250828/LINEAR/2116/logs/LINEAR_linear_noise_config1_20250828_194129.log"
        with open(log_path, 'r') as f:
            data = json.load(f)
        return [recalculate_epoch_metrics(epoch) for epoch in data['epochs']]
    
    elif dataset_name == "MACHO":
        log_path = "/autodl-fs/data/lnsde-contiformer/results/20250828/MACHO/2116/logs/MACHO_linear_noise_config1_20250828_194135.log"
        with open(log_path, 'r') as f:
            data = json.load(f)
        return [recalculate_epoch_metrics(epoch) for epoch in data['epochs']]
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")

def find_best_values(epochs):
    """æ‰¾åˆ°è®­ç»ƒå’ŒéªŒè¯çš„æœ€ä¼˜å€¼ï¼ˆä½¿ç”¨é‡æ–°è®¡ç®—çš„æŒ‡æ ‡ï¼‰"""
    best_values = {
        'train_acc': {'value': 0, 'epoch': 0},
        'val_acc': {'value': 0, 'epoch': 0},
        'train_f1': {'value': 0, 'epoch': 0},
        'val_f1': {'value': 0, 'epoch': 0},
        'train_recall': {'value': 0, 'epoch': 0},
        'val_recall': {'value': 0, 'epoch': 0},
        'train_loss': {'value': float('inf'), 'epoch': 0},
        'val_loss': {'value': float('inf'), 'epoch': 0}
    }
    
    for epoch_data in epochs:
        epoch_num = epoch_data['epoch']
        
        # å¯»æ‰¾æœ€å¤§å€¼æŒ‡æ ‡ï¼ˆä½¿ç”¨é‡æ–°è®¡ç®—çš„F1å’Œå¬å›ç‡ï¼‰
        if epoch_data['train_acc'] > best_values['train_acc']['value']:
            best_values['train_acc']['value'] = epoch_data['train_acc']
            best_values['train_acc']['epoch'] = epoch_num
            
        if epoch_data['val_acc'] > best_values['val_acc']['value']:
            best_values['val_acc']['value'] = epoch_data['val_acc']
            best_values['val_acc']['epoch'] = epoch_num
            
        if epoch_data['train_f1_recalc'] > best_values['train_f1']['value']:
            best_values['train_f1']['value'] = epoch_data['train_f1_recalc']
            best_values['train_f1']['epoch'] = epoch_num
            
        if epoch_data['val_f1_recalc'] > best_values['val_f1']['value']:
            best_values['val_f1']['value'] = epoch_data['val_f1_recalc']
            best_values['val_f1']['epoch'] = epoch_num
            
        if epoch_data['train_recall_recalc'] > best_values['train_recall']['value']:
            best_values['train_recall']['value'] = epoch_data['train_recall_recalc']
            best_values['train_recall']['epoch'] = epoch_num
            
        if epoch_data['val_recall_recalc'] > best_values['val_recall']['value']:
            best_values['val_recall']['value'] = epoch_data['val_recall_recalc']
            best_values['val_recall']['epoch'] = epoch_num
        
        # å¯»æ‰¾æœ€å°å€¼æŒ‡æ ‡
        if epoch_data['train_loss'] < best_values['train_loss']['value']:
            best_values['train_loss']['value'] = epoch_data['train_loss']
            best_values['train_loss']['epoch'] = epoch_num
            
        if epoch_data['val_loss'] < best_values['val_loss']['value']:
            best_values['val_loss']['value'] = epoch_data['val_loss']
            best_values['val_loss']['epoch'] = epoch_num
    
    return best_values

def add_optimized_annotation(ax, x, y, text, color, offset_x, offset_y, is_train=True):
    """æ·»åŠ ä¼˜åŒ–çš„æ ‡æ³¨ï¼Œé¿å…é‡å """
    marker = '*' if is_train else 'o'
    marker_size = 150 if is_train else 120
    
    ax.scatter(x, y, color=color, s=marker_size, zorder=6, 
              marker=marker, edgecolor='white', linewidth=2, alpha=0.9)
    
    bbox_style = 'round,pad=0.4' if is_train else 'round,pad=0.3'
    bbox_alpha = 0.9 if is_train else 0.8
    
    ax.annotate(text,
                xy=(x, y),
                xytext=(x + offset_x, y + offset_y),
                fontsize=9,
                ha='center',
                va='center',
                bbox=dict(boxstyle=bbox_style, 
                         facecolor='white', 
                         alpha=bbox_alpha, 
                         edgecolor=color,
                         linewidth=1.5),
                arrowprops=dict(arrowstyle='->', 
                               color=color, 
                               lw=1.5,
                               alpha=0.8))

def create_training_visualization(epochs, dataset_name, output_path):
    """åˆ›å»ºå®Œæ•´çš„è®­ç»ƒå¯è§†åŒ–ï¼Œä½¿ç”¨é‡æ–°è®¡ç®—çš„F1å’Œå¬å›ç‡"""
    
    # æå–æ•°æ®ï¼ˆä½¿ç”¨é‡æ–°è®¡ç®—çš„æŒ‡æ ‡ï¼‰
    epoch_nums = [e['epoch'] for e in epochs]
    train_losses = [e['train_loss'] for e in epochs]
    val_losses = [e['val_loss'] for e in epochs]
    train_accs = [e['train_acc'] for e in epochs]
    val_accs = [e['val_acc'] for e in epochs]
    train_f1s = [e['train_f1_recalc'] for e in epochs]
    val_f1s = [e['val_f1_recalc'] for e in epochs]
    train_recalls = [e['train_recall_recalc'] for e in epochs]
    val_recalls = [e['val_recall_recalc'] for e in epochs]
    
    # æ‰¾åˆ°æœ€ä¼˜å€¼
    best_values = find_best_values(epochs)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    fig.suptitle(f'{dataset_name} Complete Training Process (Epoch 1-100) - Macro-Averaged F1 & Recall',
                 fontsize=18, fontweight='bold', y=0.98)
    
    # 1. æŸå¤±å‡½æ•°å›¾
    ax1 = axes[0, 0]
    line1 = ax1.plot(epoch_nums, train_losses, label='Training Loss', linewidth=2.8, alpha=0.9)
    line2 = ax1.plot(epoch_nums, val_losses, label='Validation Loss', linewidth=2.8, alpha=0.9)
    
    best_train_loss = best_values['train_loss']
    add_optimized_annotation(
        ax1, best_train_loss['epoch'], best_train_loss['value'],
        f'Min Train Loss\nEpoch {best_train_loss["epoch"]}\n{best_train_loss["value"]:.4f}',
        line1[0].get_color(), -20, max(train_losses) * 0.15, is_train=True
    )
    
    best_val_loss = best_values['val_loss']
    add_optimized_annotation(
        ax1, best_val_loss['epoch'], best_val_loss['value'],
        f'Min Val Loss\nEpoch {best_val_loss["epoch"]}\n{best_val_loss["value"]:.4f}',
        line2[0].get_color(), 15, max(val_losses) * 0.02, is_train=False
    )
    
    ax1.set_xlabel('Training Epochs')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training & Validation Loss', fontweight='bold')
    ax1.legend(frameon=True, fancybox=True, shadow=True)
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, 105)
    
    # 2. å‡†ç¡®ç‡å›¾
    ax2 = axes[0, 1]
    line3 = ax2.plot(epoch_nums, train_accs, label='Training Accuracy', linewidth=2.8, alpha=0.9)
    line4 = ax2.plot(epoch_nums, val_accs, label='Validation Accuracy', linewidth=2.8, alpha=0.9)
    
    best_train_acc = best_values['train_acc']
    add_optimized_annotation(
        ax2, best_train_acc['epoch'], best_train_acc['value'],
        f'Max Train Acc\nEpoch {best_train_acc["epoch"]}\n{best_train_acc["value"]:.2f}%',
        line3[0].get_color(), -25, -4, is_train=True
    )
    
    best_val_acc = best_values['val_acc']
    add_optimized_annotation(
        ax2, best_val_acc['epoch'], best_val_acc['value'],
        f'Max Val Acc\nEpoch {best_val_acc["epoch"]}\n{best_val_acc["value"]:.2f}%',
        line4[0].get_color(), 15, 1, is_train=False
    )
    
    ax2.set_xlabel('Training Epochs')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Training & Validation Accuracy', fontweight='bold')
    ax2.legend(frameon=True, fancybox=True, shadow=True)
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(0, 105)
    
    # 3. F1åˆ†æ•°å›¾ï¼ˆé‡æ–°è®¡ç®—ï¼‰
    ax3 = axes[1, 0]
    line5 = ax3.plot(epoch_nums, train_f1s, label='Training F1 Score (Recalc)', linewidth=2.8, alpha=0.9)
    line6 = ax3.plot(epoch_nums, val_f1s, label='Validation F1 Score (Recalc)', linewidth=2.8, alpha=0.9)
    
    best_train_f1 = best_values['train_f1']
    add_optimized_annotation(
        ax3, best_train_f1['epoch'], best_train_f1['value'],
        f'Max Train F1\nEpoch {best_train_f1["epoch"]}\n{best_train_f1["value"]:.2f}',
        line5[0].get_color(), -20, -8, is_train=True
    )
    
    best_val_f1 = best_values['val_f1']
    add_optimized_annotation(
        ax3, best_val_f1['epoch'], best_val_f1['value'],
        f'Max Val F1\nEpoch {best_val_f1["epoch"]}\n{best_val_f1["value"]:.2f}',
        line6[0].get_color(), 15, 4, is_train=False
    )
    
    ax3.set_xlabel('Training Epochs')
    ax3.set_ylabel('F1 Score')
    ax3.set_title('Training & Validation F1 Score (Macro Average)', fontweight='bold')
    ax3.legend(frameon=True, fancybox=True, shadow=True)
    ax3.grid(True, alpha=0.3)
    ax3.set_xlim(0, 105)
    
    # 4. å¬å›ç‡å›¾ï¼ˆé‡æ–°è®¡ç®—ï¼‰
    ax4 = axes[1, 1]
    line7 = ax4.plot(epoch_nums, train_recalls, label='Training Recall (Recalc)', linewidth=2.8, alpha=0.9)
    line8 = ax4.plot(epoch_nums, val_recalls, label='Validation Recall (Recalc)', linewidth=2.8, alpha=0.9)
    
    best_train_recall = best_values['train_recall']
    add_optimized_annotation(
        ax4, best_train_recall['epoch'], best_train_recall['value'],
        f'Max Train Recall\nEpoch {best_train_recall["epoch"]}\n{best_train_recall["value"]:.2f}',
        line7[0].get_color(), -25, -5, is_train=True
    )
    
    best_val_recall = best_values['val_recall']
    add_optimized_annotation(
        ax4, best_val_recall['epoch'], best_val_recall['value'],
        f'Max Val Recall\nEpoch {best_val_recall["epoch"]}\n{best_val_recall["value"]:.2f}',
        line8[0].get_color(), 15, 3, is_train=False
    )
    
    ax4.set_xlabel('Training Epochs')
    ax4.set_ylabel('Recall')
    ax4.set_title('Training & Validation Recall (Macro Average)', fontweight='bold')
    ax4.legend(frameon=True, fancybox=True, shadow=True)
    ax4.grid(True, alpha=0.3)
    ax4.set_xlim(0, 105)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    plt.subplots_adjust(top=0.95, hspace=0.35, wspace=0.3)
    
    # ä¿å­˜å›¾åƒ
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
    plt.close()
    
    return best_values

def main():
    """ä¸»å‡½æ•°"""
    datasets = ['ASAS', 'LINEAR', 'MACHO']
    
    # è®¾ç½®ç»˜å›¾é£æ ¼
    setup_seaborn_style()
    
    for dataset_name in datasets:
        print(f"\n=== {dataset_name} å®Œæ•´è®­ç»ƒå¯è§†åŒ– (é‡æ–°è®¡ç®—F1å’Œå¬å›ç‡) ===")
        
        try:
            # åŠ è½½è®­ç»ƒæ•°æ®å¹¶é‡æ–°è®¡ç®—æŒ‡æ ‡
            print(f"æ­£åœ¨åŠ è½½ {dataset_name} è®­ç»ƒæ•°æ®å¹¶é‡æ–°è®¡ç®—F1å’Œå¬å›ç‡...")
            epochs = load_training_data(dataset_name)
            print(f"æˆåŠŸåŠ è½½å¹¶å¤„ç† {len(epochs)} ä¸ªepochçš„æ•°æ®")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = f"/autodl-fs/data/lnsde-contiformer/results/pics/{dataset_name}/"
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
            output_path = os.path.join(output_dir, f"{dataset_name.lower()}_complete_training_100epochs_recalc.png")
            print(f"æ­£åœ¨ç”Ÿæˆ {dataset_name} ç§‘ç ”çº§è®­ç»ƒå¯è§†åŒ–ï¼ˆé‡æ–°è®¡ç®—æŒ‡æ ‡ï¼‰...")
            
            best_values = create_training_visualization(epochs, dataset_name, output_path)
            
            # è¾“å‡ºæœ€ä¼˜å€¼æ€»ç»“
            print(f"\n=== {dataset_name} è®­ç»ƒæœ€ä¼˜å€¼æ€»ç»“ï¼ˆåŸºäºæ··æ·†çŸ©é˜µé‡æ–°è®¡ç®—ï¼‰===")
            print(f"ğŸ† æœ€é«˜è®­ç»ƒå‡†ç¡®ç‡: {best_values['train_acc']['value']:.2f}% (Epoch {best_values['train_acc']['epoch']})")
            print(f"ğŸ† æœ€é«˜éªŒè¯å‡†ç¡®ç‡: {best_values['val_acc']['value']:.2f}% (Epoch {best_values['val_acc']['epoch']})")
            print(f"ğŸ“‰ æœ€ä½è®­ç»ƒæŸå¤±: {best_values['train_loss']['value']:.4f} (Epoch {best_values['train_loss']['epoch']})")
            print(f"ğŸ“‰ æœ€ä½éªŒè¯æŸå¤±: {best_values['val_loss']['value']:.4f} (Epoch {best_values['val_loss']['epoch']})")
            print(f"ğŸ¯ æœ€é«˜è®­ç»ƒF1: {best_values['train_f1']['value']:.2f} (Epoch {best_values['train_f1']['epoch']}) [å®å¹³å‡]")
            print(f"ğŸ¯ æœ€é«˜éªŒè¯F1: {best_values['val_f1']['value']:.2f} (Epoch {best_values['val_f1']['epoch']}) [å®å¹³å‡]")
            print(f"ğŸ“Š æœ€é«˜è®­ç»ƒå¬å›ç‡: {best_values['train_recall']['value']:.2f} (Epoch {best_values['train_recall']['epoch']}) [å®å¹³å‡]")
            print(f"ğŸ“Š æœ€é«˜éªŒè¯å¬å›ç‡: {best_values['val_recall']['value']:.2f} (Epoch {best_values['val_recall']['epoch']}) [å®å¹³å‡]")
            
            print(f"âœ… {dataset_name} è®­ç»ƒå¯è§†åŒ–å·²ä¿å­˜è‡³: {output_path}")
            
        except Exception as e:
            print(f"âŒ {dataset_name} å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nğŸ¨ æ‰€æœ‰æ•°æ®é›†å¯è§†åŒ–ç‰¹ç‚¹:")
    print("   â€¢ å®Œæ•´100ä¸ªepochçš„è®­ç»ƒè¿‡ç¨‹")
    print("   â€¢ åŸºäºæ··æ·†çŸ©é˜µä½¿ç”¨å®å¹³å‡(Macro Average)è®¡ç®—F1å’Œå¬å›ç‡")
    print("   â€¢ å®å¹³å‡ï¼šç»™æ¯ä¸ªç±»åˆ«ç›¸åŒæƒé‡ï¼Œé¿å…äº†å¾®å¹³å‡ä¸­precision=recall=accuracyçš„é—®é¢˜")
    print("   â€¢ å®å¹³å‡æ›´é€‚åˆä¸å¹³è¡¡æ•°æ®é›†ï¼Œèƒ½æ›´å¥½åæ˜ å°‘æ•°ç±»çš„æ€§èƒ½")
    print("   â€¢ æ‰€æœ‰8ä¸ªå…³é”®æŒ‡æ ‡çš„æœ€ä¼˜å€¼éƒ½æ¸…æ™°æ ‡æ³¨")
    print("   â€¢ ä½¿ç”¨â˜…æ ‡è®°è®­ç»ƒæŒ‡æ ‡ï¼Œâ—æ ‡è®°éªŒè¯æŒ‡æ ‡")
    print("   â€¢ ç»Ÿä¸€çš„ç§‘ç ”çº§é…è‰²æ–¹æ¡ˆå’Œä¸“ä¸šå¸ƒå±€")

if __name__ == "__main__":
    main()